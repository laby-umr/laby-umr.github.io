---
sidebar_position: 1
title: Apache Kafkaè¯¦è§£
description: æ·±å…¥ç†è§£Kafkaæ¶æ„åŸç†ã€æ ¸å¿ƒç»„ä»¶ã€æ€§èƒ½ä¼˜åŒ–ä¸å®æˆ˜åº”ç”¨ï¼ŒåŒ…å«è¯¦ç»†çš„æŠ€æœ¯è§£æå’Œæœ€ä½³å®è·µ
authors: [Laby]
tags: [Kafka, æ¶ˆæ¯é˜Ÿåˆ—, åˆ†å¸ƒå¼ç³»ç»Ÿ, æµå¤„ç†, é«˜å¹¶å‘, å¤§æ•°æ®, äº‹ä»¶é©±åŠ¨]
last_update:
  date: 2025-08-12
  author: Laby
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

# Apache Kafkaåˆ†å¸ƒå¼æµå¤„ç†å¹³å°è¯¦è§£

Apache Kafkaæ˜¯ä¸€ä¸ªå¼€æºçš„åˆ†å¸ƒå¼äº‹ä»¶æµå¤„ç†å¹³å°ï¼Œç”±LinkedInå¼€å‘å¹¶è´¡çŒ®ç»™Apacheè½¯ä»¶åŸºé‡‘ä¼šã€‚Kafkaä»¥å…¶é«˜ååé‡ã€ä½å»¶è¿Ÿã€é«˜å¯é æ€§å’Œæ°´å¹³æ‰©å±•èƒ½åŠ›è‘—ç§°ï¼Œå·²æˆä¸ºç°ä»£æ•°æ®æ¶æ„ä¸­ä¸å¯æˆ–ç¼ºçš„æ ¸å¿ƒç»„ä»¶ï¼Œå¹¿æ³›åº”ç”¨äºå®æ—¶æ•°æ®ç®¡é“ã€æµå¤„ç†åº”ç”¨ã€äº‹ä»¶é©±åŠ¨æ¶æ„ç­‰åœºæ™¯ã€‚

:::tip æ ¸å¿ƒä»·å€¼
**Kafka = é«˜ååé‡æµå¤„ç† + åˆ†å¸ƒå¼æŒä¹…åŒ– + å®æ—¶æ•°æ®ç®¡é“ + äº‹ä»¶é©±åŠ¨æ¶æ„**
- ğŸš€ **æè‡´æ€§èƒ½**ï¼šå•æœºç™¾ä¸‡çº§TPSï¼Œé›†ç¾¤å¯è¾¾åƒä¸‡çº§æ¶ˆæ¯å¤„ç†èƒ½åŠ›
- ğŸ“Š **æµå¼æ¶æ„**ï¼šç»Ÿä¸€çš„æµå¤„ç†å’Œæ‰¹å¤„ç†å¹³å°ï¼Œæ”¯æŒå®æ—¶å’Œå†å²æ•°æ®åˆ†æ
- ï¿½ **æŒæŒä¹…åŒ–å­˜å‚¨**ï¼šæ¶ˆæ¯æŒä¹…åŒ–åˆ°ç£ç›˜ï¼Œæ”¯æŒæ•°æ®å›æº¯å’Œé‡æ”¾
- ğŸŒ **åˆ†å¸ƒå¼è®¾è®¡**ï¼šå¤©ç„¶æ”¯æŒæ°´å¹³æ‰©å±•å’Œé«˜å¯ç”¨æ¶æ„
- ï¿½ **ç”Ÿæ€å®Œæ•´***ï¼šä¸°å¯Œçš„è¿æ¥å™¨ã€æµå¤„ç†æ¡†æ¶å’Œç›‘æ§å·¥å…·
:::

## 1. KafkaåŸºç¡€æ¶æ„ä¸è®¾è®¡ç†å¿µ

### 1.1 Kafkaæ ¸å¿ƒè®¾è®¡å“²å­¦

Kafkaçš„è®¾è®¡éµå¾ªä»¥ä¸‹æ ¸å¿ƒç†å¿µï¼Œè¿™äº›ç†å¿µå†³å®šäº†å…¶åœ¨å¤§æ•°æ®å’Œå®æ—¶å¤„ç†é¢†åŸŸçš„ç‹¬ç‰¹åœ°ä½ï¼š

```mermaid
graph TB
    A[Kafkaè®¾è®¡ç†å¿µ] --> B[é«˜ååé‡ä¼˜å…ˆ]
    A --> C[åˆ†å¸ƒå¼æ¶æ„]
    A --> D[æŒä¹…åŒ–å­˜å‚¨]
    A --> E[æµå¼å¤„ç†]
    A --> F[å®¹é”™æœºåˆ¶]
    
    B --> B1[é¡ºåºI/O]
    B --> B2[é›¶æ‹·è´æŠ€æœ¯]
    B --> B3[æ‰¹é‡å¤„ç†]
    B --> B4[å‹ç¼©ä¼˜åŒ–]
    
    C --> C1[æ°´å¹³æ‰©å±•]
    C --> C2[åˆ†åŒºæœºåˆ¶]
    C --> C3[å‰¯æœ¬ç­–ç•¥]
    C --> C4[è´Ÿè½½å‡è¡¡]
    
    D --> D1[æ—¥å¿—ç»“æ„]
    D --> D2[æ®µæ–‡ä»¶ç®¡ç†]
    D --> D3[ç´¢å¼•æœºåˆ¶]
    D --> D4[æ¸…ç†ç­–ç•¥]
    
    E --> E1[äº‹ä»¶æº¯æº]
    E --> E2[æµè¡¨å¯¹å¶]
    E --> E3[æ—¶é—´çª—å£]
    E --> E4[çŠ¶æ€ç®¡ç†]
    
    F --> F1[å‰¯æœ¬æœºåˆ¶]
    F --> F2[Leaderé€‰ä¸¾]
    F --> F3[æ•…éšœæ£€æµ‹]
    F --> F4[è‡ªåŠ¨æ¢å¤]
```

#### è®¾è®¡åŸåˆ™è¯¦è§£

**1. é«˜æ€§èƒ½ä¼˜å…ˆåŸåˆ™**
- **é¡ºåºI/O**ï¼šåˆ©ç”¨ç£ç›˜é¡ºåºè¯»å†™çš„é«˜æ€§èƒ½ç‰¹æ€§
- **é›¶æ‹·è´**ï¼šå‡å°‘æ•°æ®åœ¨å†…æ ¸æ€å’Œç”¨æˆ·æ€ä¹‹é—´çš„æ‹·è´
- **æ‰¹é‡æ“ä½œ**ï¼šé€šè¿‡æ‰¹å¤„ç†æé«˜ååé‡
- **é¡µç¼“å­˜**ï¼šå……åˆ†åˆ©ç”¨æ“ä½œç³»ç»Ÿçš„é¡µç¼“å­˜æœºåˆ¶

**2. åˆ†å¸ƒå¼æ¶æ„åŸåˆ™**
- **æ— çŠ¶æ€è®¾è®¡**ï¼šBrokerèŠ‚ç‚¹æ— çŠ¶æ€ï¼Œä¾¿äºæ‰©å±•
- **åˆ†åŒºå¹¶è¡Œ**ï¼šé€šè¿‡åˆ†åŒºå®ç°å¹¶è¡Œå¤„ç†
- **å‰¯æœ¬å†—ä½™**ï¼šå¤šå‰¯æœ¬ä¿è¯æ•°æ®å®‰å…¨
- **å¼¹æ€§æ‰©å±•**ï¼šæ”¯æŒåŠ¨æ€æ·»åŠ èŠ‚ç‚¹

**3. æŒä¹…åŒ–å­˜å‚¨åŸåˆ™**
- **æ—¥å¿—ç»“æ„**ï¼šé‡‡ç”¨åªè¿½åŠ çš„æ—¥å¿—ç»“æ„
- **åˆ†æ®µå­˜å‚¨**ï¼šå°†æ—¥å¿—åˆ†æ®µå­˜å‚¨ï¼Œä¾¿äºç®¡ç†
- **å‹ç¼©å­˜å‚¨**ï¼šæ”¯æŒå¤šç§å‹ç¼©ç®—æ³•
- **æ¸…ç†ç­–ç•¥**ï¼šæ”¯æŒåŸºäºæ—¶é—´å’Œå¤§å°çš„æ¸…ç†

### 1.2 Kafkaåº”ç”¨åœºæ™¯å…¨æ™¯å›¾

| åº”ç”¨åœºæ™¯ | ä¼ ç»Ÿè§£å†³æ–¹æ¡ˆ | Kafkaè§£å†³æ–¹æ¡ˆ | æ ¸å¿ƒä¼˜åŠ¿ | é€‚ç”¨è§„æ¨¡ |
|---------|-------------|---------------|----------|---------|
| **å®æ—¶æ•°æ®ç®¡é“** | ETLå·¥å…· + æ•°æ®åº“ | Kafka Connect + Streams | å®æ—¶æ€§ã€å¯æ‰©å±•æ€§ | å¤§è§„æ¨¡æ•°æ®æµ |
| **äº‹ä»¶é©±åŠ¨æ¶æ„** | æ¶ˆæ¯é˜Ÿåˆ— + äº‹ä»¶æ€»çº¿ | Kafka + Schema Registry | è§£è€¦ã€å¯è¿½æº¯ | å¾®æœåŠ¡æ¶æ„ |
| **æ—¥å¿—èšåˆ** | æ–‡ä»¶ç³»ç»Ÿ + æ—¥å¿—æ”¶é›†å™¨ | Kafka + ELK Stack | ç»Ÿä¸€æ”¶é›†ã€å®æ—¶åˆ†æ | åˆ†å¸ƒå¼ç³»ç»Ÿ |
| **æµå¤„ç†** | Storm/Spark Streaming | Kafka Streams | ç®€åŒ–æ¶æ„ã€ä½å»¶è¿Ÿ | å®æ—¶è®¡ç®—åœºæ™¯ |
| **æŒ‡æ ‡ç›‘æ§** | æ—¶åºæ•°æ®åº“ | Kafka + InfluxDB | é«˜ååã€å®æ—¶å‘Šè­¦ | ç›‘æ§ç³»ç»Ÿ |
| **CQRS/äº‹ä»¶æº¯æº** | æ•°æ®åº“ + äº‹ä»¶è¡¨ | Kafka + å¿«ç…§å­˜å‚¨ | é«˜æ€§èƒ½ã€æ˜“æ‰©å±• | å¤æ‚ä¸šåŠ¡ç³»ç»Ÿ |
## 
2. Kafkaæ ¸å¿ƒç»„ä»¶æ·±åº¦è§£æ

### 2.1 Brokerï¼ˆä»£ç†æœåŠ¡å™¨ï¼‰- é›†ç¾¤çš„åŸºçŸ³

Brokeræ˜¯Kafkaé›†ç¾¤ä¸­çš„æ ¸å¿ƒæœåŠ¡èŠ‚ç‚¹ï¼Œæ¯ä¸ªBrokeréƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„KafkaæœåŠ¡å™¨å®ä¾‹ï¼Œè´Ÿè´£å­˜å‚¨æ•°æ®ã€å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚å’Œå‚ä¸é›†ç¾¤åè°ƒã€‚

<Tabs>
<TabItem value="architecture" label="Brokeræ¶æ„">

```mermaid
graph TB
    subgraph "Kafka Brokerå†…éƒ¨æ¶æ„"
        A[ç½‘ç»œå±‚] --> B[APIå±‚]
        B --> C[æ—¥å¿—ç®¡ç†å™¨]
        B --> D[å‰¯æœ¬ç®¡ç†å™¨]
        B --> E[æ§åˆ¶å™¨]
        B --> F[ç»„åè°ƒå™¨]
        
        C --> C1[æ—¥å¿—æ®µ]
        C --> C2[ç´¢å¼•æ–‡ä»¶]
        C --> C3[æ—¶é—´ç´¢å¼•]
        
        D --> D1[Leaderå‰¯æœ¬]
        D --> D2[Followerå‰¯æœ¬]
        D --> D3[ISRç®¡ç†]
        
        E --> E1[åˆ†åŒºåˆ†é…]
        E --> E2[Leaderé€‰ä¸¾]
        E --> E3[å…ƒæ•°æ®ç®¡ç†]
        
        F --> F1[æ¶ˆè´¹è€…ç»„ç®¡ç†]
        F --> F2[ä½ç§»ç®¡ç†]
        F --> F3[é‡å¹³è¡¡åè°ƒ]
    end
```

</TabItem>
<TabItem value="config" label="Brokeré…ç½®è¯¦è§£">

```bash title="server.properties - Brokeræ ¸å¿ƒé…ç½®"
############################# åŸºç¡€é…ç½® #############################
# Brokeråœ¨é›†ç¾¤ä¸­çš„å”¯ä¸€æ ‡è¯†ï¼Œå¿…é¡»ä¸ºæ­£æ•´æ•°ä¸”é›†ç¾¤å†…å”¯ä¸€
broker.id=0

# Brokerç›‘å¬çš„ç½‘ç»œæ¥å£å’Œç«¯å£é…ç½®
# PLAINTEXT: æ˜æ–‡ä¼ è¾“åè®®
# SSL: åŠ å¯†ä¼ è¾“åè®®  
# SASL_PLAINTEXT: SASLè®¤è¯ + æ˜æ–‡ä¼ è¾“
# SASL_SSL: SASLè®¤è¯ + åŠ å¯†ä¼ è¾“
listeners=PLAINTEXT://localhost:9092

# å¤–éƒ¨å®¢æˆ·ç«¯è¿æ¥çš„åœ°å€ï¼Œç”¨äºå®¢æˆ·ç«¯å‘ç°
advertised.listeners=PLAINTEXT://localhost:9092

# ç½‘ç»œçº¿ç¨‹æ•°ï¼Œå¤„ç†ç½‘ç»œè¯·æ±‚
# å»ºè®®è®¾ç½®ä¸ºCPUæ ¸æ•°ï¼Œé«˜å¹¶å‘åœºæ™¯å¯é€‚å½“å¢åŠ 
num.network.threads=8

# I/Oçº¿ç¨‹æ•°ï¼Œå¤„ç†ç£ç›˜è¯»å†™
# å»ºè®®è®¾ç½®ä¸ºç£ç›˜æ•°é‡çš„2-3å€
num.io.threads=16

# Socketå‘é€ç¼“å†²åŒºå¤§å°ï¼ˆå­—èŠ‚ï¼‰
socket.send.buffer.bytes=102400

# Socketæ¥æ”¶ç¼“å†²åŒºå¤§å°ï¼ˆå­—èŠ‚ï¼‰
socket.receive.buffer.bytes=102400

# å•ä¸ªè¯·æ±‚çš„æœ€å¤§å¤§å°ï¼ˆå­—èŠ‚ï¼‰
socket.request.max.bytes=104857600

############################# æ—¥å¿—é…ç½® #############################
# æ—¥å¿—æ–‡ä»¶å­˜å‚¨ç›®å½•ï¼Œæ”¯æŒå¤šä¸ªç›®å½•ä»¥é€—å·åˆ†éš”
# å¤šç›®å½•å¯ä»¥åˆ†æ•£I/Oè´Ÿè½½ï¼Œæé«˜æ€§èƒ½
log.dirs=/var/kafka-logs-1,/var/kafka-logs-2,/var/kafka-logs-3

# æ¯ä¸ªTopicçš„é»˜è®¤åˆ†åŒºæ•°
num.partitions=3

# æ—¥å¿—ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤7å¤©
log.retention.hours=168

# æ—¥å¿—æ®µæ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤1GB
log.segment.bytes=1073741824

# æ—¥å¿—æ®µæ»šåŠ¨æ—¶é—´é—´éš”ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤7å¤©
log.roll.hours=168

# æ—¥å¿—æ¸…ç†æ£€æŸ¥é—´éš”ï¼ˆæ¯«ç§’ï¼‰
log.retention.check.interval.ms=300000

############################# å‰¯æœ¬é…ç½® #############################
# é»˜è®¤å‰¯æœ¬å› å­ï¼Œå»ºè®®è®¾ç½®ä¸º3
default.replication.factor=3

# æœ€å°åŒæ­¥å‰¯æœ¬æ•°ï¼Œå»ºè®®è®¾ç½®ä¸ºå‰¯æœ¬å› å­-1
min.insync.replicas=2

# å‰¯æœ¬æ‹‰å–ç­‰å¾…æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
replica.fetch.wait.max.ms=500

# å‰¯æœ¬æ‹‰å–æœ€å¤§å­—èŠ‚æ•°
replica.fetch.max.bytes=1048576

############################# ZooKeeperé…ç½® #############################
# ZooKeeperè¿æ¥å­—ç¬¦ä¸²
zookeeper.connect=localhost:2181

# ZooKeeperè¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
zookeeper.connection.timeout.ms=18000

# ZooKeeperä¼šè¯è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
zookeeper.session.timeout.ms=18000

############################# æ€§èƒ½ä¼˜åŒ–é…ç½® #############################
# æ¶ˆæ¯æœ€å¤§å¤§å°ï¼ˆå­—èŠ‚ï¼‰
message.max.bytes=1000000

# å‰¯æœ¬æ‹‰å–æ¶ˆæ¯æœ€å¤§å¤§å°ï¼ˆå­—èŠ‚ï¼‰
replica.fetch.max.bytes=1048576

# ç”Ÿäº§è€…è¯·æ±‚é˜Ÿåˆ—æœ€å¤§å¤§å°
queued.max.requests=500

# è‡ªåŠ¨åˆ›å»ºTopicå¼€å…³ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­
auto.create.topics.enable=false

# åˆ é™¤Topicå¼€å…³
delete.topic.enable=true

# å‹ç¼©ç±»å‹ï¼šnone, gzip, snappy, lz4, zstd
compression.type=lz4

# æ—¥å¿—åˆ·ç›˜ç­–ç•¥
log.flush.interval.messages=10000
log.flush.interval.ms=1000

# åå°çº¿ç¨‹æ•°
background.threads=10
```

</TabItem>
<TabItem value="monitoring" label="Brokerç›‘æ§">

```java title="BrokerçŠ¶æ€ç›‘æ§å®ç°"
import org.apache.kafka.clients.admin.*;
import org.apache.kafka.common.Node;
import java.util.*;
import java.util.concurrent.ExecutionException;

/**
 * Kafka Brokerç›‘æ§å·¥å…·ç±»
 * æä¾›é›†ç¾¤çŠ¶æ€ã€èŠ‚ç‚¹ä¿¡æ¯ã€æ€§èƒ½æŒ‡æ ‡ç­‰ç›‘æ§åŠŸèƒ½
 */
public class KafkaBrokerMonitor {
    
    private final AdminClient adminClient;
    
    public KafkaBrokerMonitor(String bootstrapServers) {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
        props.put(AdminClientConfig.DEFAULT_API_TIMEOUT_MS_CONFIG, 60000);
        this.adminClient = AdminClient.create(props);
    }
    
    /**
     * è·å–é›†ç¾¤åŸºæœ¬ä¿¡æ¯
     */
    public ClusterInfo getClusterInfo() {
        try {
            DescribeClusterResult clusterResult = adminClient.describeCluster();
            
            // è·å–é›†ç¾¤ID
            String clusterId = clusterResult.clusterId().get();
            
            // è·å–ControllerèŠ‚ç‚¹
            Node controller = clusterResult.controller().get();
            
            // è·å–æ‰€æœ‰èŠ‚ç‚¹
            Collection<Node> nodes = clusterResult.nodes().get();
            
            return new ClusterInfo(clusterId, controller, nodes);
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("è·å–é›†ç¾¤ä¿¡æ¯å¤±è´¥", e);
        }
    }
    
    /**
     * æ£€æŸ¥Brokerå¥åº·çŠ¶æ€
     */
    public Map<Integer, BrokerHealth> checkBrokerHealth() {
        Map<Integer, BrokerHealth> healthMap = new HashMap<>();
        
        try {
            ClusterInfo clusterInfo = getClusterInfo();
            
            for (Node node : clusterInfo.getNodes()) {
                BrokerHealth health = new BrokerHealth();
                health.setBrokerId(node.id());
                health.setHost(node.host());
                health.setPort(node.port());
                health.setIsController(node.id() == clusterInfo.getController().id());
                
                // æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨çº¿
                health.setOnline(isNodeOnline(node));
                
                // è·å–èŠ‚ç‚¹è´Ÿè½½ä¿¡æ¯
                health.setPartitionCount(getPartitionCount(node.id()));
                health.setLeaderCount(getLeaderCount(node.id()));
                
                healthMap.put(node.id(), health);
            }
            
        } catch (Exception e) {
            System.err.println("æ£€æŸ¥Brokerå¥åº·çŠ¶æ€å¤±è´¥: " + e.getMessage());
        }
        
        return healthMap;
    }
    
    /**
     * æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨çº¿
     */
    private boolean isNodeOnline(Node node) {
        try {
            // é€šè¿‡æè¿°é›†ç¾¤æ¥æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å“åº”
            DescribeClusterOptions options = new DescribeClusterOptions()
                .timeoutMs(5000);
            adminClient.describeCluster(options).nodes().get();
            return true;
        } catch (Exception e) {
            return false;
        }
    }
    
    /**
     * è·å–æŒ‡å®šBrokerçš„åˆ†åŒºæ•°é‡
     */
    private int getPartitionCount(int brokerId) {
        try {
            ListTopicsResult topicsResult = adminClient.listTopics();
            Set<String> topicNames = topicsResult.names().get();
            
            DescribeTopicsResult describeResult = adminClient.describeTopics(topicNames);
            Map<String, TopicDescription> topicDescriptions = describeResult.all().get();
            
            int partitionCount = 0;
            for (TopicDescription description : topicDescriptions.values()) {
                for (TopicPartitionInfo partition : description.partitions()) {
                    // æ£€æŸ¥è¯¥åˆ†åŒºçš„å‰¯æœ¬æ˜¯å¦åœ¨æŒ‡å®šBrokerä¸Š
                    for (Node replica : partition.replicas()) {
                        if (replica.id() == brokerId) {
                            partitionCount++;
                            break;
                        }
                    }
                }
            }
            
            return partitionCount;
        } catch (Exception e) {
            return -1;
        }
    }
    
    /**
     * è·å–æŒ‡å®šBrokerä½œä¸ºLeaderçš„åˆ†åŒºæ•°é‡
     */
    private int getLeaderCount(int brokerId) {
        try {
            ListTopicsResult topicsResult = adminClient.listTopics();
            Set<String> topicNames = topicsResult.names().get();
            
            DescribeTopicsResult describeResult = adminClient.describeTopics(topicNames);
            Map<String, TopicDescription> topicDescriptions = describeResult.all().get();
            
            int leaderCount = 0;
            for (TopicDescription description : topicDescriptions.values()) {
                for (TopicPartitionInfo partition : description.partitions()) {
                    if (partition.leader() != null && partition.leader().id() == brokerId) {
                        leaderCount++;
                    }
                }
            }
            
            return leaderCount;
        } catch (Exception e) {
            return -1;
        }
    }
    
    /**
     * æ‰“å°é›†ç¾¤çŠ¶æ€æŠ¥å‘Š
     */
    public void printClusterReport() {
        System.out.println("=== Kafkaé›†ç¾¤çŠ¶æ€æŠ¥å‘Š ===");
        
        ClusterInfo clusterInfo = getClusterInfo();
        System.out.println("é›†ç¾¤ID: " + clusterInfo.getClusterId());
        System.out.println("Controller: Broker-" + clusterInfo.getController().id() + 
                         " (" + clusterInfo.getController().host() + ":" + 
                         clusterInfo.getController().port() + ")");
        System.out.println("èŠ‚ç‚¹æ€»æ•°: " + clusterInfo.getNodes().size());
        
        System.out.println("\n=== Brokerå¥åº·çŠ¶æ€ ===");
        Map<Integer, BrokerHealth> healthMap = checkBrokerHealth();
        
        for (BrokerHealth health : healthMap.values()) {
            System.out.printf("Broker-%d [%s:%d] - %s%s - åˆ†åŒºæ•°: %d, Leaderæ•°: %d%n",
                health.getBrokerId(),
                health.getHost(),
                health.getPort(),
                health.isOnline() ? "åœ¨çº¿" : "ç¦»çº¿",
                health.isController() ? " (Controller)" : "",
                health.getPartitionCount(),
                health.getLeaderCount()
            );
        }
    }
    
    public void close() {
        adminClient.close();
    }
    
    // å†…éƒ¨ç±»å®šä¹‰
    public static class ClusterInfo {
        private String clusterId;
        private Node controller;
        private Collection<Node> nodes;
        
        public ClusterInfo(String clusterId, Node controller, Collection<Node> nodes) {
            this.clusterId = clusterId;
            this.controller = controller;
            this.nodes = nodes;
        }
        
        // Getters
        public String getClusterId() { return clusterId; }
        public Node getController() { return controller; }
        public Collection<Node> getNodes() { return nodes; }
    }
    
    public static class BrokerHealth {
        private int brokerId;
        private String host;
        private int port;
        private boolean online;
        private boolean isController;
        private int partitionCount;
        private int leaderCount;
        
        // Getters and Setters
        public int getBrokerId() { return brokerId; }
        public void setBrokerId(int brokerId) { this.brokerId = brokerId; }
        
        public String getHost() { return host; }
        public void setHost(String host) { this.host = host; }
        
        public int getPort() { return port; }
        public void setPort(int port) { this.port = port; }
        
        public boolean isOnline() { return online; }
        public void setOnline(boolean online) { this.online = online; }
        
        public boolean isController() { return isController; }
        public void setIsController(boolean isController) { this.isController = isController; }
        
        public int getPartitionCount() { return partitionCount; }
        public void setPartitionCount(int partitionCount) { this.partitionCount = partitionCount; }
        
        public int getLeaderCount() { return leaderCount; }
        public void setLeaderCount(int leaderCount) { this.leaderCount = leaderCount; }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
public class BrokerMonitorExample {
    public static void main(String[] args) {
        KafkaBrokerMonitor monitor = new KafkaBrokerMonitor("localhost:9092");
        
        try {
            // æ‰“å°é›†ç¾¤çŠ¶æ€æŠ¥å‘Š
            monitor.printClusterReport();
            
            // å®šæœŸç›‘æ§ï¼ˆæ¯30ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
            Timer timer = new Timer();
            timer.scheduleAtFixedRate(new TimerTask() {
                @Override
                public void run() {
                    System.out.println("\n" + new Date() + " - é›†ç¾¤å¥åº·æ£€æŸ¥");
                    monitor.printClusterReport();
                }
            }, 0, 30000);
            
        } finally {
            // ç¨‹åºé€€å‡ºæ—¶å…³é—­ç›‘æ§
            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                monitor.close();
            }));
        }
    }
}
```

</TabItem>
</Tabs>

**Brokeræ ¸å¿ƒèŒè´£**ï¼š
- **æ•°æ®å­˜å‚¨**ï¼šç®¡ç†Topicåˆ†åŒºçš„æ—¥å¿—æ–‡ä»¶å’Œç´¢å¼•
- **è¯·æ±‚å¤„ç†**ï¼šå¤„ç†ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…çš„è¯»å†™è¯·æ±‚
- **å‰¯æœ¬ç®¡ç†**ï¼šç»´æŠ¤åˆ†åŒºå‰¯æœ¬çš„åŒæ­¥çŠ¶æ€
- **é›†ç¾¤åè°ƒ**ï¼šå‚ä¸Leaderé€‰ä¸¾å’Œå…ƒæ•°æ®åŒæ­¥
- **å®¢æˆ·ç«¯æœåŠ¡**ï¼šæä¾›å…ƒæ•°æ®ä¿¡æ¯å’Œè·¯ç”±æœåŠ¡#
## 2.2 Topicï¼ˆä¸»é¢˜ï¼‰- æ¶ˆæ¯åˆ†ç±»çš„é€»è¾‘å®¹å™¨

Topicæ˜¯Kafkaä¸­æ¶ˆæ¯çš„é€»è¾‘åˆ†ç±»å•å…ƒï¼Œç±»ä¼¼äºæ•°æ®åº“ä¸­çš„è¡¨æˆ–æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡ä»¶å¤¹ã€‚æ¯ä¸ªTopicå¯ä»¥æœ‰å¤šä¸ªç”Ÿäº§è€…å†™å…¥æ•°æ®ï¼Œå¤šä¸ªæ¶ˆè´¹è€…è¯»å–æ•°æ®ï¼Œæ˜¯Kafkaæ¶ˆæ¯ç³»ç»Ÿçš„æ ¸å¿ƒæŠ½è±¡ã€‚

<Tabs>
<TabItem value="concept" label="Topicæ¦‚å¿µæ¨¡å‹">

```mermaid
graph TB
    subgraph "Topicé€»è¾‘ç»“æ„"
        T[Topic: user-events] --> P0[Partition 0]
        T --> P1[Partition 1] 
        T --> P2[Partition 2]
        
        P0 --> P0R0[Replica 0 - Leader]
        P0 --> P0R1[Replica 1 - Follower]
        P0 --> P0R2[Replica 2 - Follower]
        
        P1 --> P1R0[Replica 0 - Follower]
        P1 --> P1R1[Replica 1 - Leader]
        P1 --> P1R2[Replica 2 - Follower]
        
        P2 --> P2R0[Replica 0 - Follower]
        P2 --> P2R1[Replica 1 - Follower]
        P2 --> P2R2[Replica 2 - Leader]
    end
    
    subgraph "æ¶ˆæ¯æµå‘"
        Producer1[Producer 1] --> T
        Producer2[Producer 2] --> T
        Producer3[Producer 3] --> T
        
        T --> Consumer1[Consumer Group A]
        T --> Consumer2[Consumer Group B]
    end
```

</TabItem>
<TabItem value="management" label="Topicç®¡ç†">

```java title="Topicç®¡ç†å®Œæ•´å®ç°"
import org.apache.kafka.clients.admin.*;
import org.apache.kafka.common.config.ConfigResource;
import java.util.*;
import java.util.concurrent.ExecutionException;

/**
 * Kafka Topicç®¡ç†å·¥å…·ç±»
 * æä¾›Topicçš„åˆ›å»ºã€åˆ é™¤ã€é…ç½®ã€ç›‘æ§ç­‰å®Œæ•´åŠŸèƒ½
 */
public class KafkaTopicManager {
    
    private final AdminClient adminClient;
    
    public KafkaTopicManager(String bootstrapServers) {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
        this.adminClient = AdminClient.create(props);
    }
    
    /**
     * åˆ›å»ºTopicï¼ˆå¸¦å®Œæ•´é…ç½®ï¼‰
     */
    public void createTopic(String topicName, int partitions, short replicationFactor, 
                           Map<String, String> configs) {
        try {
            // æ£€æŸ¥Topicæ˜¯å¦å·²å­˜åœ¨
            if (topicExists(topicName)) {
                System.out.println("Topic '" + topicName + "' å·²å­˜åœ¨");
                return;
            }
            
            // åˆ›å»ºNewTopicå¯¹è±¡
            NewTopic newTopic = new NewTopic(topicName, partitions, replicationFactor);
            
            // è®¾ç½®Topicé…ç½®
            if (configs != null && !configs.isEmpty()) {
                newTopic.configs(configs);
            }
            
            // æ‰§è¡Œåˆ›å»ºæ“ä½œ
            CreateTopicsResult result = adminClient.createTopics(Arrays.asList(newTopic));
            result.all().get(); // ç­‰å¾…åˆ›å»ºå®Œæˆ
            
            System.out.println("Topic '" + topicName + "' åˆ›å»ºæˆåŠŸ");
            System.out.println("  åˆ†åŒºæ•°: " + partitions);
            System.out.println("  å‰¯æœ¬å› å­: " + replicationFactor);
            if (configs != null) {
                System.out.println("  é…ç½®é¡¹: " + configs);
            }
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("åˆ›å»ºTopicå¤±è´¥: " + topicName, e);
        }
    }
    
    /**
     * åˆ›å»ºä¸šåŠ¡Topicçš„ä¾¿æ·æ–¹æ³•
     */
    public void createBusinessTopic(String topicName, TopicType type) {
        Map<String, String> configs = new HashMap<>();
        int partitions;
        short replicationFactor = 3;
        
        switch (type) {
            case HIGH_THROUGHPUT:
                // é«˜ååé‡Topicé…ç½®
                partitions = 12;
                configs.put("compression.type", "lz4");
                configs.put("min.insync.replicas", "2");
                configs.put("retention.ms", "604800000"); // 7å¤©
                configs.put("segment.ms", "86400000");    // 1å¤©
                break;
                
            case LOW_LATENCY:
                // ä½å»¶è¿ŸTopicé…ç½®
                partitions = 6;
                configs.put("compression.type", "none");
                configs.put("min.insync.replicas", "2");
                configs.put("retention.ms", "259200000"); // 3å¤©
                configs.put("segment.ms", "3600000");     // 1å°æ—¶
                break;
                
            case LONG_RETENTION:
                // é•¿æœŸä¿ç•™Topicé…ç½®
                partitions = 6;
                configs.put("compression.type", "gzip");
                configs.put("min.insync.replicas", "2");
                configs.put("retention.ms", "2592000000"); // 30å¤©
                configs.put("segment.ms", "604800000");     // 7å¤©
                configs.put("cleanup.policy", "delete");
                break;
                
            case COMPACTED:
                // å‹ç¼©Topicé…ç½®ï¼ˆé€‚ç”¨äºçŠ¶æ€å­˜å‚¨ï¼‰
                partitions = 6;
                configs.put("cleanup.policy", "compact");
                configs.put("compression.type", "snappy");
                configs.put("min.cleanable.dirty.ratio", "0.1");
                configs.put("segment.ms", "86400000"); // 1å¤©
                break;
                
            default:
                // é»˜è®¤é…ç½®
                partitions = 3;
                configs.put("retention.ms", "604800000"); // 7å¤©
        }
        
        createTopic(topicName, partitions, replicationFactor, configs);
    }
    
    /**
     * æ£€æŸ¥Topicæ˜¯å¦å­˜åœ¨
     */
    public boolean topicExists(String topicName) {
        try {
            ListTopicsResult result = adminClient.listTopics();
            Set<String> topicNames = result.names().get();
            return topicNames.contains(topicName);
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("æ£€æŸ¥Topicå­˜åœ¨æ€§å¤±è´¥", e);
        }
    }
    
    /**
     * è·å–Topicè¯¦ç»†ä¿¡æ¯
     */
    public TopicInfo getTopicInfo(String topicName) {
        try {
            // è·å–Topicæè¿°ä¿¡æ¯
            DescribeTopicsResult describeResult = adminClient.describeTopics(Arrays.asList(topicName));
            TopicDescription description = describeResult.all().get().get(topicName);
            
            // è·å–Topicé…ç½®ä¿¡æ¯
            ConfigResource configResource = new ConfigResource(ConfigResource.Type.TOPIC, topicName);
            DescribeConfigsResult configResult = adminClient.describeConfigs(Arrays.asList(configResource));
            Config config = configResult.all().get().get(configResource);
            
            return new TopicInfo(description, config);
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("è·å–Topicä¿¡æ¯å¤±è´¥: " + topicName, e);
        }
    }
    
    /**
     * åˆ—å‡ºæ‰€æœ‰Topic
     */
    public List<String> listTopics() {
        try {
            ListTopicsResult result = adminClient.listTopics();
            return new ArrayList<>(result.names().get());
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("åˆ—å‡ºTopicå¤±è´¥", e);
        }
    }
    
    /**
     * åˆ é™¤Topic
     */
    public void deleteTopic(String topicName) {
        try {
            if (!topicExists(topicName)) {
                System.out.println("Topic '" + topicName + "' ä¸å­˜åœ¨");
                return;
            }
            
            DeleteTopicsResult result = adminClient.deleteTopics(Arrays.asList(topicName));
            result.all().get(); // ç­‰å¾…åˆ é™¤å®Œæˆ
            
            System.out.println("Topic '" + topicName + "' åˆ é™¤æˆåŠŸ");
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("åˆ é™¤Topicå¤±è´¥: " + topicName, e);
        }
    }
    
    /**
     * ä¿®æ”¹Topicé…ç½®
     */
    public void updateTopicConfig(String topicName, Map<String, String> configUpdates) {
        try {
            ConfigResource resource = new ConfigResource(ConfigResource.Type.TOPIC, topicName);
            
            // æ„å»ºé…ç½®æ›´æ–°æ“ä½œ
            Map<ConfigResource, Collection<AlterConfigOp>> configs = new HashMap<>();
            Collection<AlterConfigOp> ops = new ArrayList<>();
            
            for (Map.Entry<String, String> entry : configUpdates.entrySet()) {
                ops.add(new AlterConfigOp(
                    new ConfigEntry(entry.getKey(), entry.getValue()),
                    AlterConfigOp.OpType.SET
                ));
            }
            
            configs.put(resource, ops);
            
            // æ‰§è¡Œé…ç½®æ›´æ–°
            AlterConfigsResult result = adminClient.incrementalAlterConfigs(configs);
            result.all().get(); // ç­‰å¾…æ›´æ–°å®Œæˆ
            
            System.out.println("Topic '" + topicName + "' é…ç½®æ›´æ–°æˆåŠŸ: " + configUpdates);
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("æ›´æ–°Topicé…ç½®å¤±è´¥: " + topicName, e);
        }
    }
    
    /**
     * å¢åŠ Topicåˆ†åŒºæ•°
     */
    public void increasePartitions(String topicName, int newPartitionCount) {
        try {
            // è·å–å½“å‰åˆ†åŒºæ•°
            TopicInfo topicInfo = getTopicInfo(topicName);
            int currentPartitions = topicInfo.getPartitionCount();
            
            if (newPartitionCount <= currentPartitions) {
                System.out.println("æ–°åˆ†åŒºæ•°å¿…é¡»å¤§äºå½“å‰åˆ†åŒºæ•° " + currentPartitions);
                return;
            }
            
            // åˆ›å»ºåˆ†åŒºå¢åŠ è¯·æ±‚
            Map<String, NewPartitions> partitionUpdates = new HashMap<>();
            partitionUpdates.put(topicName, NewPartitions.increaseTo(newPartitionCount));
            
            // æ‰§è¡Œåˆ†åŒºå¢åŠ æ“ä½œ
            CreatePartitionsResult result = adminClient.createPartitions(partitionUpdates);
            result.all().get(); // ç­‰å¾…æ“ä½œå®Œæˆ
            
            System.out.println("Topic '" + topicName + "' åˆ†åŒºæ•°ä» " + currentPartitions + 
                             " å¢åŠ åˆ° " + newPartitionCount);
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("å¢åŠ Topicåˆ†åŒºå¤±è´¥: " + topicName, e);
        }
    }
    
    /**
     * æ‰“å°Topicè¯¦ç»†æŠ¥å‘Š
     */
    public void printTopicReport(String topicName) {
        TopicInfo info = getTopicInfo(topicName);
        
        System.out.println("=== Topicè¯¦ç»†ä¿¡æ¯: " + topicName + " ===");
        System.out.println("åˆ†åŒºæ•°: " + info.getPartitionCount());
        System.out.println("å‰¯æœ¬å› å­: " + info.getReplicationFactor());
        System.out.println("æ˜¯å¦ä¸ºå†…éƒ¨Topic: " + info.isInternal());
        
        System.out.println("\n--- åˆ†åŒºåˆ†å¸ƒ ---");
        for (TopicPartitionInfo partition : info.getPartitions()) {
            System.out.printf("åˆ†åŒº %d: Leader=%d, ISR=%s, Replicas=%s%n",
                partition.partition(),
                partition.leader().id(),
                partition.isr().stream().map(n -> String.valueOf(n.id())).collect(Collectors.joining(",")),
                partition.replicas().stream().map(n -> String.valueOf(n.id())).collect(Collectors.joining(","))
            );
        }
        
        System.out.println("\n--- é…ç½®ä¿¡æ¯ ---");
        Map<String, String> configs = info.getConfigs();
        configs.entrySet().stream()
            .filter(entry -> !entry.getValue().isEmpty())
            .forEach(entry -> System.out.println(entry.getKey() + " = " + entry.getValue()));
    }
    
    public void close() {
        adminClient.close();
    }
    
    // Topicç±»å‹æšä¸¾
    public enum TopicType {
        HIGH_THROUGHPUT,    // é«˜ååé‡
        LOW_LATENCY,        // ä½å»¶è¿Ÿ
        LONG_RETENTION,     // é•¿æœŸä¿ç•™
        COMPACTED          // å‹ç¼©å­˜å‚¨
    }
    
    // Topicä¿¡æ¯å°è£…ç±»
    public static class TopicInfo {
        private final TopicDescription description;
        private final Config config;
        
        public TopicInfo(TopicDescription description, Config config) {
            this.description = description;
            this.config = config;
        }
        
        public String getName() { return description.name(); }
        public int getPartitionCount() { return description.partitions().size(); }
        public boolean isInternal() { return description.isInternal(); }
        public List<TopicPartitionInfo> getPartitions() { return description.partitions(); }
        
        public short getReplicationFactor() {
            return description.partitions().isEmpty() ? 0 : 
                   (short) description.partitions().get(0).replicas().size();
        }
        
        public Map<String, String> getConfigs() {
            Map<String, String> configMap = new HashMap<>();
            for (ConfigEntry entry : config.entries()) {
                configMap.put(entry.name(), entry.value());
            }
            return configMap;
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
public class TopicManagerExample {
    public static void main(String[] args) {
        KafkaTopicManager manager = new KafkaTopicManager("localhost:9092");
        
        try {
            // åˆ›å»ºä¸åŒç±»å‹çš„ä¸šåŠ¡Topic
            manager.createBusinessTopic("user-events", KafkaTopicManager.TopicType.HIGH_THROUGHPUT);
            manager.createBusinessTopic("user-profiles", KafkaTopicManager.TopicType.COMPACTED);
            manager.createBusinessTopic("audit-logs", KafkaTopicManager.TopicType.LONG_RETENTION);
            
            // åˆ—å‡ºæ‰€æœ‰Topic
            System.out.println("å½“å‰Topicåˆ—è¡¨: " + manager.listTopics());
            
            // æŸ¥çœ‹Topicè¯¦ç»†ä¿¡æ¯
            manager.printTopicReport("user-events");
            
            // å¢åŠ åˆ†åŒºæ•°
            manager.increasePartitions("user-events", 18);
            
            // æ›´æ–°Topicé…ç½®
            Map<String, String> configUpdates = new HashMap<>();
            configUpdates.put("retention.ms", "1209600000"); // 14å¤©
            manager.updateTopicConfig("user-events", configUpdates);
            
        } finally {
            manager.close();
        }
    }
}
```

</TabItem>
<TabItem value="naming" label="å‘½åè§„èŒƒ">

**Topicå‘½åæœ€ä½³å®è·µ**ï¼š

```bash title="Topicå‘½åè§„èŒƒç¤ºä¾‹"
# 1. åŸºç¡€å‘½åè§„èŒƒ
# æ ¼å¼ï¼š<ä¸šåŠ¡åŸŸ>.<æ•°æ®ç±»å‹>.<ç‰ˆæœ¬>
user.events.v1              # ç”¨æˆ·äº‹ä»¶æµ
order.transactions.v2       # è®¢å•äº¤æ˜“æµ
inventory.updates.v1        # åº“å­˜æ›´æ–°æµ

# 2. æŒ‰ç¯å¢ƒåŒºåˆ†
# æ ¼å¼ï¼š<ç¯å¢ƒ>.<ä¸šåŠ¡åŸŸ>.<æ•°æ®ç±»å‹>
dev.user.events            # å¼€å‘ç¯å¢ƒç”¨æˆ·äº‹ä»¶
staging.order.transactions # æµ‹è¯•ç¯å¢ƒè®¢å•äº¤æ˜“
prod.inventory.updates     # ç”Ÿäº§ç¯å¢ƒåº“å­˜æ›´æ–°

# 3. æŒ‰æ•°æ®æµå‘åŒºåˆ†
# æ ¼å¼ï¼š<æºç³»ç»Ÿ>-to-<ç›®æ ‡ç³»ç»Ÿ>.<æ•°æ®ç±»å‹>
mysql-to-elasticsearch.users    # MySQLåˆ°ESçš„ç”¨æˆ·æ•°æ®
app-to-analytics.clickstream    # åº”ç”¨åˆ°åˆ†æç³»ç»Ÿçš„ç‚¹å‡»æµ
crm-to-warehouse.customers      # CRMåˆ°æ•°ä»“çš„å®¢æˆ·æ•°æ®

# 4. æŒ‰ä¸šåŠ¡åŠŸèƒ½åŒºåˆ†
user-registration-events        # ç”¨æˆ·æ³¨å†Œäº‹ä»¶
payment-processing-commands     # æ”¯ä»˜å¤„ç†å‘½ä»¤
notification-delivery-status    # é€šçŸ¥æŠ•é€’çŠ¶æ€

# 5. å†…éƒ¨ç³»ç»ŸTopic
_consumer_offsets              # Kafkaå†…éƒ¨ä½ç§»Topic
_transaction_state             # äº‹åŠ¡çŠ¶æ€Topic
__schema_registry_schemas      # Schema Registryå†…éƒ¨Topic
```

**å‘½åè§„èŒƒè¦ç‚¹**ï¼š
- ä½¿ç”¨å°å†™å­—æ¯å’Œè¿å­—ç¬¦
- é¿å…ä½¿ç”¨ä¸‹åˆ’çº¿å¼€å¤´ï¼ˆç³»ç»Ÿä¿ç•™ï¼‰
- åç§°è¦æœ‰ä¸šåŠ¡å«ä¹‰ï¼Œä¾¿äºç†è§£
- è€ƒè™‘ç‰ˆæœ¬ç®¡ç†å’Œç¯å¢ƒéš”ç¦»
- é•¿åº¦é€‚ä¸­ï¼Œé¿å…è¿‡é•¿æˆ–è¿‡çŸ­

</TabItem>
</Tabs>

### 2.3 Partitionï¼ˆåˆ†åŒºï¼‰- å¹¶è¡Œå¤„ç†çš„åŸºç¡€

åˆ†åŒºæ˜¯Topicçš„ç‰©ç†åˆ†å‰²å•å…ƒï¼Œæ¯ä¸ªåˆ†åŒºæ˜¯ä¸€ä¸ªæœ‰åºã€ä¸å¯å˜çš„æ¶ˆæ¯åºåˆ—ã€‚åˆ†åŒºæœºåˆ¶æ˜¯Kafkaå®ç°é«˜å¹¶å‘ã€é«˜ååé‡çš„æ ¸å¿ƒè®¾è®¡ã€‚

<Tabs>
<TabItem value="structure" label="åˆ†åŒºç»“æ„">

```mermaid
graph TB
    subgraph "åˆ†åŒºå†…éƒ¨ç»“æ„"
        P[Partition 0] --> S1[Segment 0<br/>0-999]
        P --> S2[Segment 1<br/>1000-1999]
        P --> S3[Segment 2<br/>2000-2999]
        P --> S4[Active Segment<br/>3000-...]
        
        S1 --> L1[Log File<br/>00000000000000000000.log]
        S1 --> I1[Index File<br/>00000000000000000000.index]
        S1 --> T1[Time Index<br/>00000000000000000000.timeindex]
        
        S2 --> L2[Log File<br/>00000000000000001000.log]
        S2 --> I2[Index File<br/>00000000000000001000.index]
        S2 --> T2[Time Index<br/>00000000000000001000.timeindex]
    end
    
    subgraph "æ¶ˆæ¯å­˜å‚¨æ ¼å¼"
        M[Message] --> MO[Offset]
        M --> MT[Timestamp]
        M --> MK[Key]
        M --> MV[Value]
        M --> MH[Headers]
    end
```

</TabItem>
<TabItem value="partitioner" label="åˆ†åŒºç­–ç•¥">

```java title="è‡ªå®šä¹‰åˆ†åŒºå™¨å®ç°"
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.utils.Utils;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ThreadLocalRandom;

/**
 * è‡ªå®šä¹‰åˆ†åŒºå™¨å®ç°
 * æ”¯æŒå¤šç§åˆ†åŒºç­–ç•¥ï¼Œå¯æ ¹æ®ä¸šåŠ¡éœ€æ±‚çµæ´»é€‰æ‹©
 */
public class CustomPartitioner implements Partitioner {
    
    private final Map<String, Integer> stickyPartitionCache = new ConcurrentHashMap<>();
    
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, 
                        Object value, byte[] valueBytes, Cluster cluster) {
        
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        
        if (keyBytes == null) {
            // æ— keyæ—¶ä½¿ç”¨ç²˜æ€§åˆ†åŒºç­–ç•¥
            return getStickyPartition(topic, cluster);
        }
        
        // æœ‰keyæ—¶æ ¹æ®keyç±»å‹é€‰æ‹©åˆ†åŒºç­–ç•¥
        if (key instanceof String) {
            return partitionByString((String) key, numPartitions);
        } else if (key instanceof Integer) {
            return partitionByInteger((Integer) key, numPartitions);
        } else if (key instanceof Long) {
            return partitionByLong((Long) key, numPartitions);
        } else {
            // é»˜è®¤ä½¿ç”¨hashåˆ†åŒº
            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
        }
    }
    
    /**
     * å­—ç¬¦ä¸²keyåˆ†åŒºç­–ç•¥
     * æ”¯æŒä¸šåŠ¡è§„åˆ™åˆ†åŒº
     */
    private int partitionByString(String key, int numPartitions) {
        // ç”¨æˆ·IDåˆ†åŒºï¼šuser:123 -> æ ¹æ®ç”¨æˆ·IDåˆ†åŒº
        if (key.startsWith("user:")) {
            String userId = key.substring(5);
            return Math.abs(userId.hashCode()) % numPartitions;
        }
        
        // åœ°åŒºåˆ†åŒºï¼šregion:beijing -> æ ¹æ®åœ°åŒºåˆ†åŒº
        if (key.startsWith("region:")) {
            String region = key.substring(7);
            return getRegionPartition(region, numPartitions);
        }
        
        // æ—¶é—´åˆ†åŒºï¼šdate:2025-01-01 -> æ ¹æ®æ—¥æœŸåˆ†åŒº
        if (key.startsWith("date:")) {
            String date = key.substring(5);
            return Math.abs(date.hashCode()) % numPartitions;
        }
        
        // é»˜è®¤hashåˆ†åŒº
        return Math.abs(key.hashCode()) % numPartitions;
    }
    
    /**
     * æ•´æ•°keyåˆ†åŒºç­–ç•¥
     */
    private int partitionByInteger(Integer key, int numPartitions) {
        // å¶æ•°åˆ†åŒºç­–ç•¥ï¼šå¶æ•°keyåˆ†é…åˆ°å‰åŠéƒ¨åˆ†åˆ†åŒº
        if (key % 2 == 0) {
            return key % (numPartitions / 2);
        } else {
            return (numPartitions / 2) + (key % (numPartitions - numPartitions / 2));
        }
    }
    
    /**
     * é•¿æ•´æ•°keyåˆ†åŒºç­–ç•¥
     */
    private int partitionByLong(Long key, int numPartitions) {
        // æ—¶é—´æˆ³åˆ†åŒºï¼šæ ¹æ®æ—¶é—´æˆ³çš„å°æ—¶éƒ¨åˆ†åˆ†åŒº
        if (key > 1000000000000L) { // å‡è®¾æ˜¯æ—¶é—´æˆ³
            long hour = (key / 1000 / 3600) % 24;
            return (int) (hour % numPartitions);
        }
        
        return (int) (Math.abs(key) % numPartitions);
    }
    
    /**
     * åœ°åŒºåˆ†åŒºæ˜ å°„
     */
    private int getRegionPartition(String region, int numPartitions) {
        Map<String, Integer> regionMap = new HashMap<>();
        regionMap.put("beijing", 0);
        regionMap.put("shanghai", 1);
        regionMap.put("guangzhou", 2);
        regionMap.put("shenzhen", 3);
        
        return regionMap.getOrDefault(region.toLowerCase(), 
                                    Math.abs(region.hashCode()) % numPartitions);
    }
    
    /**
     * ç²˜æ€§åˆ†åŒºç­–ç•¥ï¼ˆæ— keyæ—¶ä½¿ç”¨ï¼‰
     * å‡å°‘åˆ†åŒºåˆ‡æ¢ï¼Œæé«˜æ‰¹å¤„ç†æ•ˆç‡
     */
    private int getStickyPartition(String topic, Cluster cluster) {
        Integer partition = stickyPartitionCache.get(topic);
        if (partition == null) {
            List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
            partition = ThreadLocalRandom.current().nextInt(partitions.size());
            stickyPartitionCache.put(topic, partition);
        }
        return partition;
    }
    
    @Override
    public void close() {
        stickyPartitionCache.clear();
    }
    
    @Override
    public void configure(Map<String, ?> configs) {
        // å¯ä»¥ä»é…ç½®ä¸­è¯»å–åˆ†åŒºç­–ç•¥å‚æ•°
    }
}

/**
 * åˆ†åŒºç­–ç•¥æµ‹è¯•å’Œåˆ†æå·¥å…·
 */
public class PartitionAnalyzer {
    
    /**
     * åˆ†æåˆ†åŒºåˆ†å¸ƒå‡åŒ€æ€§
     */
    public static void analyzePartitionDistribution(String topic, List<String> keys, 
                                                   Partitioner partitioner, int numPartitions) {
        Map<Integer, Integer> partitionCounts = new HashMap<>();
        
        // æ¨¡æ‹Ÿé›†ç¾¤ä¿¡æ¯
        Cluster cluster = createMockCluster(topic, numPartitions);
        
        // ç»Ÿè®¡æ¯ä¸ªåˆ†åŒºçš„æ¶ˆæ¯æ•°é‡
        for (String key : keys) {
            byte[] keyBytes = key != null ? key.getBytes() : null;
            int partition = partitioner.partition(topic, key, keyBytes, null, null, cluster);
            partitionCounts.put(partition, partitionCounts.getOrDefault(partition, 0) + 1);
        }
        
        // æ‰“å°åˆ†å¸ƒç»Ÿè®¡
        System.out.println("=== åˆ†åŒºåˆ†å¸ƒåˆ†æ ===");
        System.out.println("æ€»æ¶ˆæ¯æ•°: " + keys.size());
        System.out.println("åˆ†åŒºæ•°: " + numPartitions);
        System.out.println("å¹³å‡æ¯åˆ†åŒº: " + (keys.size() / numPartitions));
        
        for (int i = 0; i < numPartitions; i++) {
            int count = partitionCounts.getOrDefault(i, 0);
            double percentage = (double) count / keys.size() * 100;
            System.out.printf("åˆ†åŒº %d: %d æ¡æ¶ˆæ¯ (%.2f%%)%n", i, count, percentage);
        }
        
        // è®¡ç®—åˆ†å¸ƒå‡åŒ€æ€§
        double variance = calculateVariance(partitionCounts, numPartitions, keys.size());
        System.out.printf("åˆ†å¸ƒæ–¹å·®: %.2f (è¶Šå°è¶Šå‡åŒ€)%n", variance);
    }
    
    private static double calculateVariance(Map<Integer, Integer> counts, int numPartitions, int totalMessages) {
        double mean = (double) totalMessages / numPartitions;
        double sumSquaredDiff = 0;
        
        for (int i = 0; i < numPartitions; i++) {
            int count = counts.getOrDefault(i, 0);
            sumSquaredDiff += Math.pow(count - mean, 2);
        }
        
        return sumSquaredDiff / numPartitions;
    }
    
    private static Cluster createMockCluster(String topic, int numPartitions) {
        List<PartitionInfo> partitions = new ArrayList<>();
        for (int i = 0; i < numPartitions; i++) {
            partitions.add(new PartitionInfo(topic, i, null, null, null));
        }
        return new Cluster("test-cluster", Collections.emptyList(), partitions, 
                          Collections.emptySet(), Collections.emptySet());
    }
    
    public static void main(String[] args) {
        // ç”Ÿæˆæµ‹è¯•æ•°æ®
        List<String> testKeys = new ArrayList<>();
        
        // ç”¨æˆ·IDæµ‹è¯•æ•°æ®
        for (int i = 1; i <= 1000; i++) {
            testKeys.add("user:" + i);
        }
        
        // åœ°åŒºæµ‹è¯•æ•°æ®
        String[] regions = {"beijing", "shanghai", "guangzhou", "shenzhen", "hangzhou"};
        for (int i = 0; i < 500; i++) {
            testKeys.add("region:" + regions[i % regions.length]);
        }
        
        // æ—¥æœŸæµ‹è¯•æ•°æ®
        for (int i = 1; i <= 31; i++) {
            testKeys.add("date:2025-01-" + String.format("%02d", i));
        }
        
        // åˆ†æè‡ªå®šä¹‰åˆ†åŒºå™¨çš„åˆ†å¸ƒ
        CustomPartitioner partitioner = new CustomPartitioner();
        analyzePartitionDistribution("test-topic", testKeys, partitioner, 6);
    }
}
```

</TabItem>
<TabItem value="optimization" label="åˆ†åŒºä¼˜åŒ–">

**åˆ†åŒºæ•°é‡é€‰æ‹©æŒ‡å—**ï¼š

| è€ƒè™‘å› ç´  | å»ºè®® | è¯´æ˜ |
|---------|------|------|
| **ååé‡éœ€æ±‚** | å•åˆ†åŒº10-30MB/s | æ ¹æ®ä¸šåŠ¡å³°å€¼æµé‡è®¡ç®—æ‰€éœ€åˆ†åŒºæ•° |
| **æ¶ˆè´¹è€…æ•°é‡** | åˆ†åŒºæ•° â‰¥ æ¶ˆè´¹è€…æ•° | ä¿è¯æ¯ä¸ªæ¶ˆè´¹è€…éƒ½æœ‰åˆ†åŒºå¯æ¶ˆè´¹ |
| **å­˜å‚¨å®¹é‡** | è€ƒè™‘å•åˆ†åŒºå¤§å° | é¿å…å•åˆ†åŒºè¿‡å¤§å½±å“æ€§èƒ½ |
| **ç½‘ç»œå¼€é”€** | é¿å…åˆ†åŒºè¿‡å¤š | åˆ†åŒºè¿‡å¤šä¼šå¢åŠ ç½‘ç»œå’Œå†…å­˜å¼€é”€ |
| **æ•…éšœæ¢å¤** | å¹³è¡¡å¯ç”¨æ€§å’Œæ€§èƒ½ | åˆ†åŒºå¤šæ¢å¤å¿«ï¼Œä½†èµ„æºæ¶ˆè€—å¤§ |

```java title="åˆ†åŒºæ•°é‡è®¡ç®—å·¥å…·"
/**
 * åˆ†åŒºæ•°é‡è®¡ç®—å’Œä¼˜åŒ–å»ºè®®å·¥å…·
 */
public class PartitionCalculator {
    
    /**
     * æ ¹æ®ååé‡éœ€æ±‚è®¡ç®—åˆ†åŒºæ•°
     */
    public static int calculatePartitionsByThroughput(
            long targetThroughputMBps,      // ç›®æ ‡ååé‡ MB/s
            long singlePartitionThroughput, // å•åˆ†åŒºååé‡ MB/s
            double safetyFactor             // å®‰å…¨ç³»æ•°
    ) {
        int basePartitions = (int) Math.ceil((double) targetThroughputMBps / singlePartitionThroughput);
        return (int) Math.ceil(basePartitions * safetyFactor);
    }
    
    /**
     * æ ¹æ®æ¶ˆè´¹è€…æ•°é‡è®¡ç®—åˆ†åŒºæ•°
     */
    public static int calculatePartitionsByConsumers(
            int maxConsumers,               // æœ€å¤§æ¶ˆè´¹è€…æ•°é‡
            double parallelismFactor        // å¹¶è¡Œåº¦ç³»æ•°
    ) {
        return (int) Math.ceil(maxConsumers * parallelismFactor);
    }
    
    /**
     * æ ¹æ®æ•°æ®é‡è®¡ç®—åˆ†åŒºæ•°
     */
    public static int calculatePartitionsByDataSize(
            long dailyDataSizeGB,           // æ¯æ—¥æ•°æ®é‡ GB
            int retentionDays,              // ä¿ç•™å¤©æ•°
            long maxPartitionSizeGB         // å•åˆ†åŒºæœ€å¤§å¤§å° GB
    ) {
        long totalDataSize = dailyDataSizeGB * retentionDays;
        return (int) Math.ceil((double) totalDataSize / maxPartitionSizeGB);
    }
    
    /**
     * ç»¼åˆè®¡ç®—æ¨èåˆ†åŒºæ•°
     */
    public static PartitionRecommendation recommendPartitions(
            long targetThroughputMBps,
            int maxConsumers,
            long dailyDataSizeGB,
            int retentionDays
    ) {
        // åŸºäºä¸åŒå› ç´ è®¡ç®—åˆ†åŒºæ•°
        int throughputPartitions = calculatePartitionsByThroughput(targetThroughputMBps, 20, 1.5);
        int consumerPartitions = calculatePartitionsByConsumers(maxConsumers, 1.2);
        int dataSizePartitions = calculatePartitionsByDataSize(dailyDataSizeGB, retentionDays, 50);
        
        // å–æœ€å¤§å€¼ä½œä¸ºæ¨èå€¼
        int recommendedPartitions = Math.max(Math.max(throughputPartitions, consumerPartitions), dataSizePartitions);
        
        // è°ƒæ•´ä¸º2çš„å¹‚æ¬¡æ–¹ï¼ˆå¯é€‰ï¼Œä¾¿äºè´Ÿè½½å‡è¡¡ï¼‰
        int powerOfTwoPartitions = nextPowerOfTwo(recommendedPartitions);
        
        return new PartitionRecommendation(
            recommendedPartitions,
            powerOfTwoPartitions,
            throughputPartitions,
            consumerPartitions,
            dataSizePartitions
        );
    }
    
    private static int nextPowerOfTwo(int n) {
        if (n <= 1) return 1;
        return Integer.highestOneBit(n - 1) << 1;
    }
    
    /**
     * åˆ†åŒºæ¨èç»“æœ
     */
    public static class PartitionRecommendation {
        private final int recommended;
        private final int powerOfTwo;
        private final int byThroughput;
        private final int byConsumers;
        private final int byDataSize;
        
        public PartitionRecommendation(int recommended, int powerOfTwo, 
                                     int byThroughput, int byConsumers, int byDataSize) {
            this.recommended = recommended;
            this.powerOfTwo = powerOfTwo;
            this.byThroughput = byThroughput;
            this.byConsumers = byConsumers;
            this.byDataSize = byDataSize;
        }
        
        public void printReport() {
            System.out.println("=== åˆ†åŒºæ•°é‡æ¨èæŠ¥å‘Š ===");
            System.out.println("åŸºäºååé‡: " + byThroughput + " ä¸ªåˆ†åŒº");
            System.out.println("åŸºäºæ¶ˆè´¹è€…æ•°é‡: " + byConsumers + " ä¸ªåˆ†åŒº");
            System.out.println("åŸºäºæ•°æ®å¤§å°: " + byDataSize + " ä¸ªåˆ†åŒº");
            System.out.println("æ¨èåˆ†åŒºæ•°: " + recommended + " ä¸ªåˆ†åŒº");
            System.out.println("2çš„å¹‚æ¬¡æ–¹: " + powerOfTwo + " ä¸ªåˆ†åŒº");
            System.out.println("\nå»ºè®®ä½¿ç”¨ " + Math.max(recommended, powerOfTwo) + " ä¸ªåˆ†åŒº");
        }
        
        // Getters
        public int getRecommended() { return recommended; }
        public int getPowerOfTwo() { return powerOfTwo; }
        public int getByThroughput() { return byThroughput; }
        public int getByConsumers() { return byConsumers; }
        public int getByDataSize() { return byDataSize; }
    }
    
    public static void main(String[] args) {
        // ç¤ºä¾‹ï¼šè®¡ç®—ç”µå•†è®¢å•Topicçš„åˆ†åŒºæ•°
        PartitionRecommendation recommendation = recommendPartitions(
            100,    // ç›®æ ‡ååé‡ 100MB/s
            20,     // æœ€å¤§20ä¸ªæ¶ˆè´¹è€…
            10,     // æ¯æ—¥10GBæ•°æ®
            30      // ä¿ç•™30å¤©
        );
        
        recommendation.printReport();
    }
}
```

</TabItem>
</Tabs>

# 3. Kafkaç”Ÿäº§è€…ä¸æ¶ˆè´¹è€…æ·±åº¦å®æˆ˜

### 3.1 Producerï¼ˆç”Ÿäº§è€…ï¼‰- é«˜æ€§èƒ½æ¶ˆæ¯å‘é€

ç”Ÿäº§è€…æ˜¯å‘Kafkaé›†ç¾¤å‘é€æ¶ˆæ¯çš„å®¢æˆ·ç«¯åº”ç”¨ç¨‹åºã€‚Kafkaç”Ÿäº§è€…å…·æœ‰é«˜åº¦çš„å¯é…ç½®æ€§ï¼Œæ”¯æŒåŒæ­¥/å¼‚æ­¥å‘é€ã€æ‰¹å¤„ç†ã€å‹ç¼©ã€äº‹åŠ¡ç­‰å¤šç§ç‰¹æ€§ã€‚

<Tabs>
<TabItem value="basic" label="åŸºç¡€ç”Ÿäº§è€…">

```java title="Kafkaç”Ÿäº§è€…åŸºç¡€å®ç°"
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import java.util.Properties;
import java.util.concurrent.Future;

/**
 * Kafkaç”Ÿäº§è€…åŸºç¡€å®ç°
 * å±•ç¤ºåŒæ­¥å’Œå¼‚æ­¥å‘é€çš„å®Œæ•´ç”¨æ³•
 */
public class KafkaProducerBasic {
    
    private final KafkaProducer<String, String> producer;
    private final String topicName;
    
    public KafkaProducerBasic(String bootstrapServers, String topicName) {
        this.topicName = topicName;
        this.producer = createProducer(bootstrapServers);
    }
    
    /**
     * åˆ›å»ºç”Ÿäº§è€…å®ä¾‹
     */
    private KafkaProducer<String, String> createProducer(String bootstrapServers) {
        Properties props = new Properties();
        
        // åŸºç¡€è¿æ¥é…ç½®
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        // å¯é æ€§é…ç½®
        props.put(ProducerConfig.ACKS_CONFIG, "all");                    // ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
        props.put(ProducerConfig.RETRIES_CONFIG, 3);                     // é‡è¯•3æ¬¡
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 1000);         // é‡è¯•é—´éš”1ç§’
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);       // å¯ç”¨å¹‚ç­‰æ€§
        
        // æ€§èƒ½ä¼˜åŒ–é…ç½®
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);              // æ‰¹æ¬¡å¤§å°16KB
        props.put(ProducerConfig.LINGER_MS_CONFIG, 10);                  // ç­‰å¾…10msæ”¶é›†æ›´å¤šæ¶ˆæ¯
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);        // ç¼“å†²åŒº32MB
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");        // LZ4å‹ç¼©
        
        // ç½‘ç»œé…ç½®
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);      // è¯·æ±‚è¶…æ—¶30ç§’
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 120000);    // æŠ•é€’è¶…æ—¶2åˆ†é’Ÿ
        
        return new KafkaProducer<>(props);
    }
    
    /**
     * åŒæ­¥å‘é€æ¶ˆæ¯
     */
    public RecordMetadata sendSync(String key, String value) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value);
        
        try {
            // åŒæ­¥å‘é€ï¼Œä¼šé˜»å¡ç›´åˆ°æ”¶åˆ°å“åº”
            RecordMetadata metadata = producer.send(record).get();
            
            System.out.printf("åŒæ­¥å‘é€æˆåŠŸ: topic=%s, partition=%d, offset=%d, timestamp=%d%n",
                metadata.topic(), metadata.partition(), metadata.offset(), metadata.timestamp());
            
            return metadata;
            
        } catch (Exception e) {
            System.err.println("åŒæ­¥å‘é€å¤±è´¥: " + e.getMessage());
            throw new RuntimeException("å‘é€æ¶ˆæ¯å¤±è´¥", e);
        }
    }
    
    /**
     * å¼‚æ­¥å‘é€æ¶ˆæ¯
     */
    public Future<RecordMetadata> sendAsync(String key, String value) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value);
        
        // å¼‚æ­¥å‘é€ï¼Œç«‹å³è¿”å›Future
        return producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception == null) {
                    System.out.printf("å¼‚æ­¥å‘é€æˆåŠŸ: topic=%s, partition=%d, offset=%d%n",
                        metadata.topic(), metadata.partition(), metadata.offset());
                } else {
                    System.err.println("å¼‚æ­¥å‘é€å¤±è´¥: " + exception.getMessage());
                }
            }
        });
    }
    
    /**
     * å‘é€å¸¦å¤´éƒ¨ä¿¡æ¯çš„æ¶ˆæ¯
     */
    public void sendWithHeaders(String key, String value, Map<String, String> headers) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value);
        
        // æ·»åŠ å¤´éƒ¨ä¿¡æ¯
        if (headers != null) {
            for (Map.Entry<String, String> entry : headers.entrySet()) {
                record.headers().add(entry.getKey(), entry.getValue().getBytes());
            }
        }
        
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                System.out.println("å¸¦å¤´éƒ¨æ¶ˆæ¯å‘é€æˆåŠŸ: " + metadata.offset());
            } else {
                System.err.println("å¸¦å¤´éƒ¨æ¶ˆæ¯å‘é€å¤±è´¥: " + exception.getMessage());
            }
        });
    }
    
    /**
     * æ‰¹é‡å‘é€æ¶ˆæ¯
     */
    public void sendBatch(List<MessageData> messages) {
        List<Future<RecordMetadata>> futures = new ArrayList<>();
        
        // æ‰¹é‡æäº¤æ¶ˆæ¯
        for (MessageData msg : messages) {
            ProducerRecord<String, String> record = 
                new ProducerRecord<>(topicName, msg.getKey(), msg.getValue());
            
            Future<RecordMetadata> future = producer.send(record);
            futures.add(future);
        }
        
        // ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
        for (int i = 0; i < futures.size(); i++) {
            try {
                RecordMetadata metadata = futures.get(i).get();
                System.out.println("æ‰¹é‡æ¶ˆæ¯ " + i + " å‘é€æˆåŠŸ: " + metadata.offset());
            } catch (Exception e) {
                System.err.println("æ‰¹é‡æ¶ˆæ¯ " + i + " å‘é€å¤±è´¥: " + e.getMessage());
            }
        }
    }
    
    /**
     * åˆ·æ–°ç¼“å†²åŒºï¼Œç¡®ä¿æ‰€æœ‰æ¶ˆæ¯éƒ½å‘é€
     */
    public void flush() {
        producer.flush();
        System.out.println("ç”Ÿäº§è€…ç¼“å†²åŒºå·²åˆ·æ–°");
    }
    
    /**
     * å…³é—­ç”Ÿäº§è€…
     */
    public void close() {
        producer.close();
        System.out.println("ç”Ÿäº§è€…å·²å…³é—­");
    }
    
    // æ¶ˆæ¯æ•°æ®å°è£…ç±»
    public static class MessageData {
        private String key;
        private String value;
        
        public MessageData(String key, String value) {
            this.key = key;
            this.value = value;
        }
        
        public String getKey() { return key; }
        public String getValue() { return value; }
    }
}

// ç”Ÿäº§è€…ä½¿ç”¨ç¤ºä¾‹
public class ProducerExample {
    public static void main(String[] args) {
        KafkaProducerBasic producer = new KafkaProducerBasic("localhost:9092", "user-events");
        
        try {
            // åŒæ­¥å‘é€
            producer.sendSync("user-123", "login");
            
            // å¼‚æ­¥å‘é€
            producer.sendAsync("user-456", "logout");
            
            // å¸¦å¤´éƒ¨ä¿¡æ¯å‘é€
            Map<String, String> headers = new HashMap<>();
            headers.put("source", "mobile-app");
            headers.put("version", "1.0");
            producer.sendWithHeaders("user-789", "purchase", headers);
            
            // æ‰¹é‡å‘é€
            List<KafkaProducerBasic.MessageData> batch = Arrays.asList(
                new KafkaProducerBasic.MessageData("user-001", "view_product"),
                new KafkaProducerBasic.MessageData("user-002", "add_to_cart"),
                new KafkaProducerBasic.MessageData("user-003", "checkout")
            );
            producer.sendBatch(batch);
            
            // ç¡®ä¿æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
            producer.flush();
            
        } finally {
            producer.close();
        }
    }
}
```

</TabItem>
<TabItem value="advanced" label="é«˜çº§ç”Ÿäº§è€…">

```java title="é«˜çº§ç”Ÿäº§è€…å®ç° - æ”¯æŒäº‹åŠ¡å’Œç›‘æ§"
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.errors.ProducerFencedException;
import org.apache.kafka.common.errors.OutOfOrderSequenceException;
import org.apache.kafka.common.errors.AuthorizationException;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

/**
 * é«˜çº§Kafkaç”Ÿäº§è€…å®ç°
 * æ”¯æŒäº‹åŠ¡ã€ç›‘æ§ã€é‡è¯•ç­–ç•¥ã€æ•…éšœå¤„ç†ç­‰é«˜çº§ç‰¹æ€§
 */
public class KafkaProducerAdvanced {
    
    private final KafkaProducer<String, String> producer;
    private final String topicName;
    private final boolean transactionalEnabled;
    private final ProducerMetrics metrics;
    private final ScheduledExecutorService scheduler;
    
    // æ€§èƒ½æŒ‡æ ‡
    private final AtomicLong totalSent = new AtomicLong(0);
    private final AtomicLong totalFailed = new AtomicLong(0);
    private final AtomicLong totalBytes = new AtomicLong(0);
    
    public KafkaProducerAdvanced(String bootstrapServers, String topicName, boolean enableTransactions) {
        this.topicName = topicName;
        this.transactionalEnabled = enableTransactions;
        this.producer = createAdvancedProducer(bootstrapServers, enableTransactions);
        this.metrics = new ProducerMetrics();
        this.scheduler = Executors.newScheduledThreadPool(1);
        
        // å¯åŠ¨äº‹åŠ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (transactionalEnabled) {
            producer.initTransactions();
        }
        
        // å¯åŠ¨æŒ‡æ ‡ç›‘æ§
        startMetricsReporting();
    }
    
    /**
     * åˆ›å»ºé«˜çº§ç”Ÿäº§è€…é…ç½®
     */
    private KafkaProducer<String, String> createAdvancedProducer(String bootstrapServers, boolean enableTransactions) {
        Properties props = new Properties();
        
        // åŸºç¡€é…ç½®
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        // é«˜å¯é æ€§é…ç½®
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);     // æ— é™é‡è¯•
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 1000);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // ä¿è¯é¡ºåº
        
        // äº‹åŠ¡é…ç½®
        if (enableTransactions) {
            props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "producer-" + System.currentTimeMillis());
        }
        
        // é«˜æ€§èƒ½é…ç½®
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768);              // 32KBæ‰¹æ¬¡
        props.put(ProducerConfig.LINGER_MS_CONFIG, 20);                  // ç­‰å¾…20ms
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 67108864);        // 64MBç¼“å†²åŒº
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");
        
        // è¶…æ—¶é…ç½®
        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 300000);    // 5åˆ†é’ŸæŠ•é€’è¶…æ—¶
        
        // ç›‘æ§é…ç½®
        props.put(ProducerConfig.METRIC_REPORTERS_CONFIG, "");
        props.put(ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG, 30000);
        props.put(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG, 2);
        
        return new KafkaProducer<>(props);
    }
    
    /**
     * äº‹åŠ¡æ€§å‘é€æ¶ˆæ¯
     */
    public void sendTransactional(List<MessageData> messages) {
        if (!transactionalEnabled) {
            throw new IllegalStateException("äº‹åŠ¡æœªå¯ç”¨");
        }
        
        try {
            // å¼€å§‹äº‹åŠ¡
            producer.beginTransaction();
            
            // å‘é€æ¶ˆæ¯
            for (MessageData msg : messages) {
                ProducerRecord<String, String> record = 
                    new ProducerRecord<>(topicName, msg.getKey(), msg.getValue());
                producer.send(record);
            }
            
            // æäº¤äº‹åŠ¡
            producer.commitTransaction();
            
            totalSent.addAndGet(messages.size());
            System.out.println("äº‹åŠ¡æäº¤æˆåŠŸï¼Œå‘é€ " + messages.size() + " æ¡æ¶ˆæ¯");
            
        } catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) {
            // ä¸å¯æ¢å¤çš„å¼‚å¸¸ï¼Œå…³é—­ç”Ÿäº§è€…
            System.err.println("ä¸å¯æ¢å¤çš„äº‹åŠ¡å¼‚å¸¸: " + e.getMessage());
            close();
            throw e;
        } catch (Exception e) {
            // å¯æ¢å¤çš„å¼‚å¸¸ï¼Œä¸­æ­¢äº‹åŠ¡
            System.err.println("äº‹åŠ¡å¼‚å¸¸ï¼Œæ­£åœ¨ä¸­æ­¢: " + e.getMessage());
            producer.abortTransaction();
            totalFailed.addAndGet(messages.size());
            throw e;
        }
    }
    
    /**
     * å¸¦é‡è¯•ç­–ç•¥çš„å‘é€
     */
    public void sendWithRetry(String key, String value, int maxRetries) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value);
        
        sendWithRetry(record, maxRetries, 0);
    }
    
    private void sendWithRetry(ProducerRecord<String, String> record, int maxRetries, int attempt) {
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                // å‘é€æˆåŠŸ
                totalSent.incrementAndGet();
                totalBytes.addAndGet(record.value().getBytes().length);
                metrics.recordSuccess(System.currentTimeMillis() - record.timestamp());
                
                System.out.printf("æ¶ˆæ¯å‘é€æˆåŠŸ (å°è¯• %d): offset=%d%n", attempt + 1, metadata.offset());
                
            } else {
                // å‘é€å¤±è´¥
                totalFailed.incrementAndGet();
                metrics.recordFailure();
                
                if (attempt < maxRetries) {
                    // é‡è¯•
                    System.out.printf("æ¶ˆæ¯å‘é€å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• (%d/%d): %s%n", 
                        attempt + 1, maxRetries, exception.getMessage());
                    
                    // å»¶è¿Ÿé‡è¯•
                    scheduler.schedule(() -> {
                        sendWithRetry(record, maxRetries, attempt + 1);
                    }, (attempt + 1) * 1000, TimeUnit.MILLISECONDS);
                    
                } else {
                    // é‡è¯•æ¬¡æ•°ç”¨å°½
                    System.err.printf("æ¶ˆæ¯å‘é€æœ€ç»ˆå¤±è´¥ (%d æ¬¡å°è¯•): %s%n", 
                        maxRetries + 1, exception.getMessage());
                }
            }
        });
    }
    
    /**
     * å¼‚æ­¥å‘é€å¸¦å›è°ƒ
     */
    public void sendAsyncWithCallback(String key, String value, MessageCallback callback) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topicName, key, value);
        
        producer.send(record, (metadata, exception) -> {
            if (exception == null) {
                totalSent.incrementAndGet();
                totalBytes.addAndGet(record.value().getBytes().length);
                
                if (callback != null) {
                    callback.onSuccess(metadata);
                }
            } else {
                totalFailed.incrementAndGet();
                
                if (callback != null) {
                    callback.onFailure(exception);
                }
            }
        });
    }
    
    /**
     * è·å–ç”Ÿäº§è€…æŒ‡æ ‡
     */
    public ProducerStats getStats() {
        return new ProducerStats(
            totalSent.get(),
            totalFailed.get(),
            totalBytes.get(),
            metrics.getAverageLatency(),
            metrics.getSuccessRate()
        );
    }
    
    /**
     * å¯åŠ¨æŒ‡æ ‡ç›‘æ§
     */
    private void startMetricsReporting() {
        scheduler.scheduleAtFixedRate(() -> {
            ProducerStats stats = getStats();
            System.out.printf("ç”Ÿäº§è€…æŒ‡æ ‡ - æˆåŠŸ: %d, å¤±è´¥: %d, å­—èŠ‚æ•°: %d, å¹³å‡å»¶è¿Ÿ: %.2fms, æˆåŠŸç‡: %.2f%%%n",
                stats.getTotalSent(), stats.getTotalFailed(), stats.getTotalBytes(),
                stats.getAverageLatency(), stats.getSuccessRate() * 100);
        }, 30, 30, TimeUnit.SECONDS);
    }
    
    /**
     * ä¼˜é›…å…³é—­
     */
    public void close() {
        try {
            // åˆ·æ–°ç¼“å†²åŒº
            producer.flush();
            
            // å…³é—­ç”Ÿäº§è€…
            producer.close(Duration.ofSeconds(10));
            
            // å…³é—­è°ƒåº¦å™¨
            scheduler.shutdown();
            
            System.out.println("ç”Ÿäº§è€…å·²ä¼˜é›…å…³é—­");
            
        } catch (Exception e) {
            System.err.println("å…³é—­ç”Ÿäº§è€…æ—¶å‘ç”Ÿå¼‚å¸¸: " + e.getMessage());
        }
    }
    
    // æ¶ˆæ¯å›è°ƒæ¥å£
    public interface MessageCallback {
        void onSuccess(RecordMetadata metadata);
        void onFailure(Exception exception);
    }
    
    // ç”Ÿäº§è€…æŒ‡æ ‡ç±»
    private static class ProducerMetrics {
        private final AtomicLong totalLatency = new AtomicLong(0);
        private final AtomicLong successCount = new AtomicLong(0);
        private final AtomicLong failureCount = new AtomicLong(0);
        
        public void recordSuccess(long latency) {
            totalLatency.addAndGet(latency);
            successCount.incrementAndGet();
        }
        
        public void recordFailure() {
            failureCount.incrementAndGet();
        }
        
        public double getAverageLatency() {
            long count = successCount.get();
            return count > 0 ? (double) totalLatency.get() / count : 0.0;
        }
        
        public double getSuccessRate() {
            long total = successCount.get() + failureCount.get();
            return total > 0 ? (double) successCount.get() / total : 0.0;
        }
    }
    
    // ç”Ÿäº§è€…ç»Ÿè®¡ä¿¡æ¯
    public static class ProducerStats {
        private final long totalSent;
        private final long totalFailed;
        private final long totalBytes;
        private final double averageLatency;
        private final double successRate;
        
        public ProducerStats(long totalSent, long totalFailed, long totalBytes, 
                           double averageLatency, double successRate) {
            this.totalSent = totalSent;
            this.totalFailed = totalFailed;
            this.totalBytes = totalBytes;
            this.averageLatency = averageLatency;
            this.successRate = successRate;
        }
        
        // Getters
        public long getTotalSent() { return totalSent; }
        public long getTotalFailed() { return totalFailed; }
        public long getTotalBytes() { return totalBytes; }
        public double getAverageLatency() { return averageLatency; }
        public double getSuccessRate() { return successRate; }
    }
    
    // æ¶ˆæ¯æ•°æ®ç±»
    public static class MessageData {
        private String key;
        private String value;
        
        public MessageData(String key, String value) {
            this.key = key;
            this.value = value;
        }
        
        public String getKey() { return key; }
        public String getValue() { return value; }
    }
}

// é«˜çº§ç”Ÿäº§è€…ä½¿ç”¨ç¤ºä¾‹
public class AdvancedProducerExample {
    public static void main(String[] args) {
        KafkaProducerAdvanced producer = new KafkaProducerAdvanced(
            "localhost:9092", "user-events", true);
        
        try {
            // äº‹åŠ¡æ€§å‘é€
            List<KafkaProducerAdvanced.MessageData> transactionalMessages = Arrays.asList(
                new KafkaProducerAdvanced.MessageData("user-001", "login"),
                new KafkaProducerAdvanced.MessageData("user-001", "view_product"),
                new KafkaProducerAdvanced.MessageData("user-001", "purchase")
            );
            producer.sendTransactional(transactionalMessages);
            
            // å¸¦é‡è¯•çš„å‘é€
            producer.sendWithRetry("user-002", "important_event", 3);
            
            // å¼‚æ­¥å‘é€å¸¦å›è°ƒ
            producer.sendAsyncWithCallback("user-003", "callback_event", 
                new KafkaProducerAdvanced.MessageCallback() {
                    @Override
                    public void onSuccess(RecordMetadata metadata) {
                        System.out.println("å›è°ƒæˆåŠŸ: " + metadata.offset());
                    }
                    
                    @Override
                    public void onFailure(Exception exception) {
                        System.err.println("å›è°ƒå¤±è´¥: " + exception.getMessage());
                    }
                });
            
            // ç­‰å¾…ä¸€æ®µæ—¶é—´æŸ¥çœ‹æŒ‡æ ‡
            Thread.sleep(60000);
            
            // æ‰“å°æœ€ç»ˆç»Ÿè®¡
            KafkaProducerAdvanced.ProducerStats stats = producer.getStats();
            System.out.println("æœ€ç»ˆç»Ÿè®¡: " + stats.getTotalSent() + " æˆåŠŸ, " + 
                             stats.getTotalFailed() + " å¤±è´¥");
            
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            producer.close();
        }
    }
}
```

</TabItem>
<TabItem value="performance" label="æ€§èƒ½ä¼˜åŒ–">

**ç”Ÿäº§è€…æ€§èƒ½ä¼˜åŒ–é…ç½®å¯¹æ¯”**ï¼š

| é…ç½®é¡¹ | é«˜ååé‡é…ç½® | ä½å»¶è¿Ÿé…ç½® | é«˜å¯é æ€§é…ç½® | è¯´æ˜ |
|--------|-------------|-----------|-------------|------|
| **acks** | 1 | 1 | all | ç¡®è®¤çº§åˆ« |
| **batch.size** | 65536 (64KB) | 0 | 16384 (16KB) | æ‰¹æ¬¡å¤§å° |
| **linger.ms** | 100 | 0 | 10 | ç­‰å¾…æ—¶é—´ |
| **compression.type** | lz4 | none | gzip | å‹ç¼©ç®—æ³• |
| **buffer.memory** | 134217728 (128MB) | 33554432 (32MB) | 67108864 (64MB) | ç¼“å†²åŒºå¤§å° |
| **retries** | 5 | 0 | Integer.MAX_VALUE | é‡è¯•æ¬¡æ•° |
| **max.in.flight.requests** | 5 | 1 | 1 | å¹¶å‘è¯·æ±‚æ•° |

```java title="ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•å·¥å…·"
/**
 * Kafkaç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•å·¥å…·
 * ç”¨äºæµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„ç”Ÿäº§è€…æ€§èƒ½
 */
public class ProducerPerformanceTest {
    
    /**
     * æ€§èƒ½æµ‹è¯•é…ç½®
     */
    public enum PerformanceProfile {
        HIGH_THROUGHPUT,    // é«˜ååé‡
        LOW_LATENCY,        // ä½å»¶è¿Ÿ
        HIGH_RELIABILITY    // é«˜å¯é æ€§
    }
    
    /**
     * åˆ›å»ºæ€§èƒ½æµ‹è¯•ç”Ÿäº§è€…
     */
    public static KafkaProducer<String, String> createPerformanceProducer(
            String bootstrapServers, PerformanceProfile profile) {
        
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        switch (profile) {
            case HIGH_THROUGHPUT:
                // é«˜ååé‡é…ç½®
                props.put(ProducerConfig.ACKS_CONFIG, "1");
                props.put(ProducerConfig.BATCH_SIZE_CONFIG, 65536);
                props.put(ProducerConfig.LINGER_MS_CONFIG, 100);
                props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");
                props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 134217728);
                props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
                break;
                
            case LOW_LATENCY:
                // ä½å»¶è¿Ÿé…ç½®
                props.put(ProducerConfig.ACKS_CONFIG, "1");
                props.put(ProducerConfig.BATCH_SIZE_CONFIG, 0);
                props.put(ProducerConfig.LINGER_MS_CONFIG, 0);
                props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "none");
                props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 33554432);
                props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
                break;
                
            case HIGH_RELIABILITY:
                // é«˜å¯é æ€§é…ç½®
                props.put(ProducerConfig.ACKS_CONFIG, "all");
                props.put(ProducerConfig.BATCH_SIZE_CONFIG, 16384);
                props.put(ProducerConfig.LINGER_MS_CONFIG, 10);
                props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "gzip");
                props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 67108864);
                props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
                props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
                props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
                break;
        }
        
        return new KafkaProducer<>(props);
    }
    
    /**
     * æ‰§è¡Œæ€§èƒ½æµ‹è¯•
     */
    public static TestResult runPerformanceTest(String bootstrapServers, String topic,
                                              PerformanceProfile profile, int messageCount, int messageSize) {
        
        KafkaProducer<String, String> producer = createPerformanceProducer(bootstrapServers, profile);
        
        // ç”Ÿæˆæµ‹è¯•æ¶ˆæ¯
        String messageValue = generateMessage(messageSize);
        
        long startTime = System.currentTimeMillis();
        AtomicLong successCount = new AtomicLong(0);
        AtomicLong failureCount = new AtomicLong(0);
        List<Long> latencies = Collections.synchronizedList(new ArrayList<>());
        
        CountDownLatch latch = new CountDownLatch(messageCount);
        
        // å‘é€æ¶ˆæ¯
        for (int i = 0; i < messageCount; i++) {
            String key = "key-" + i;
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, messageValue);
            
            long sendTime = System.nanoTime();
            producer.send(record, (metadata, exception) -> {
                long latency = (System.nanoTime() - sendTime) / 1_000_000; // è½¬æ¢ä¸ºæ¯«ç§’
                
                if (exception == null) {
                    successCount.incrementAndGet();
                    latencies.add(latency);
                } else {
                    failureCount.incrementAndGet();
                }
                
                latch.countDown();
            });
        }
        
        try {
            // ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
            latch.await(5, TimeUnit.MINUTES);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        long endTime = System.currentTimeMillis();
        producer.close();
        
        // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        long totalTime = endTime - startTime;
        double throughput = (double) successCount.get() / totalTime * 1000; // æ¶ˆæ¯/ç§’
        double avgLatency = latencies.stream().mapToLong(Long::longValue).average().orElse(0.0);
        double p95Latency = calculatePercentile(latencies, 0.95);
        double p99Latency = calculatePercentile(latencies, 0.99);
        
        return new TestResult(profile, messageCount, messageSize, totalTime,
                            successCount.get(), failureCount.get(), throughput,
                            avgLatency, p95Latency, p99Latency);
    }
    
    private static String generateMessage(int size) {
        StringBuilder sb = new StringBuilder(size);
        for (int i = 0; i < size; i++) {
            sb.append('a');
        }
        return sb.toString();
    }
    
    private static double calculatePercentile(List<Long> values, double percentile) {
        if (values.isEmpty()) return 0.0;
        
        List<Long> sorted = new ArrayList<>(values);
        Collections.sort(sorted);
        
        int index = (int) Math.ceil(percentile * sorted.size()) - 1;
        return sorted.get(Math.max(0, index));
    }
    
    // æµ‹è¯•ç»“æœç±»
    public static class TestResult {
        private final PerformanceProfile profile;
        private final int messageCount;
        private final int messageSize;
        private final long totalTime;
        private final long successCount;
        private final long failureCount;
        private final double throughput;
        private final double avgLatency;
        private final double p95Latency;
        private final double p99Latency;
        
        public TestResult(PerformanceProfile profile, int messageCount, int messageSize,
                         long totalTime, long successCount, long failureCount,
                         double throughput, double avgLatency, double p95Latency, double p99Latency) {
            this.profile = profile;
            this.messageCount = messageCount;
            this.messageSize = messageSize;
            this.totalTime = totalTime;
            this.successCount = successCount;
            this.failureCount = failureCount;
            this.throughput = throughput;
            this.avgLatency = avgLatency;
            this.p95Latency = p95Latency;
            this.p99Latency = p99Latency;
        }
        
        public void printReport() {
            System.out.println("=== æ€§èƒ½æµ‹è¯•æŠ¥å‘Š ===");
            System.out.println("é…ç½®ç±»å‹: " + profile);
            System.out.println("æ¶ˆæ¯æ•°é‡: " + messageCount);
            System.out.println("æ¶ˆæ¯å¤§å°: " + messageSize + " å­—èŠ‚");
            System.out.println("æ€»è€—æ—¶: " + totalTime + " ms");
            System.out.println("æˆåŠŸæ•°é‡: " + successCount);
            System.out.println("å¤±è´¥æ•°é‡: " + failureCount);
            System.out.printf("ååé‡: %.2f æ¶ˆæ¯/ç§’%n", throughput);
            System.out.printf("å¹³å‡å»¶è¿Ÿ: %.2f ms%n", avgLatency);
            System.out.printf("P95å»¶è¿Ÿ: %.2f ms%n", p95Latency);
            System.out.printf("P99å»¶è¿Ÿ: %.2f ms%n", p99Latency);
            System.out.println();
        }
        
        // Getters
        public PerformanceProfile getProfile() { return profile; }
        public double getThroughput() { return throughput; }
        public double getAvgLatency() { return avgLatency; }
        public double getP95Latency() { return p95Latency; }
        public double getP99Latency() { return p99Latency; }
    }
    
    public static void main(String[] args) {
        String bootstrapServers = "localhost:9092";
        String topic = "performance-test";
        int messageCount = 10000;
        int messageSize = 1024; // 1KB
        
        // æµ‹è¯•ä¸åŒé…ç½®çš„æ€§èƒ½
        for (PerformanceProfile profile : PerformanceProfile.values()) {
            System.out.println("å¼€å§‹æµ‹è¯•é…ç½®: " + profile);
            TestResult result = runPerformanceTest(bootstrapServers, topic, profile, messageCount, messageSize);
            result.printReport();
        }
    }
}
```

</TabItem>
</Tabs>

### 3.2 Consumerï¼ˆæ¶ˆè´¹è€…ï¼‰- é«˜æ•ˆæ¶ˆæ¯æ¶ˆè´¹

Kafkaæ¶ˆè´¹è€…è´Ÿè´£ä»Topicä¸­è¯»å–æ¶ˆæ¯ã€‚æ¶ˆè´¹è€…å¯ä»¥å•ç‹¬å·¥ä½œï¼Œä¹Ÿå¯ä»¥ä½œä¸ºæ¶ˆè´¹è€…ç»„çš„ä¸€éƒ¨åˆ†ååŒå·¥ä½œï¼Œå®ç°è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»ã€‚

<Tabs>
<TabItem value="basic" label="åŸºç¡€æ¶ˆè´¹è€…">

```java title="Kafkaæ¶ˆè´¹è€…åŸºç¡€å®ç°"
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.time.Duration;
import java.util.*;

/**
 * Kafkaæ¶ˆè´¹è€…åŸºç¡€å®ç°
 * å±•ç¤ºè‡ªåŠ¨æäº¤ã€æ‰‹åŠ¨æäº¤ã€æŒ‡å®šåˆ†åŒºæ¶ˆè´¹ç­‰åŠŸèƒ½
 */
public class KafkaConsumerBasic {
    
    private final KafkaConsumer<String, String> consumer;
    private final List<String> topics;
    private volatile boolean running = false;
    
    public KafkaConsumerBasic(String bootstrapServers, String groupId, List<String> topics) {
        this.topics = topics;
        this.consumer = createConsumer(bootstrapServers, groupId);
    }
    
    /**
     * åˆ›å»ºæ¶ˆè´¹è€…å®ä¾‹
     */
    private KafkaConsumer<String, String> createConsumer(String bootstrapServers, String groupId) {
        Properties props = new Properties();
        
        // åŸºç¡€è¿æ¥é…ç½®
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        
        // ä½ç§»ç®¡ç†é…ç½®
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);      // ç¦ç”¨è‡ªåŠ¨æäº¤
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");  // ä»æœ€æ—©ä½ç§»å¼€å§‹
        
        // æ‹‰å–é…ç½®
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);          // æœ€å°æ‹‰å–1KB
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);         // æœ€å¤§ç­‰å¾…500ms
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);          // æ¯æ¬¡æœ€å¤šæ‹‰å–500æ¡
        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 1048576); // æ¯åˆ†åŒºæœ€å¤§1MB
        
        // ä¼šè¯é…ç½®
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);      // ä¼šè¯è¶…æ—¶30ç§’
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);   // å¿ƒè·³é—´éš”10ç§’
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000);   // æœ€å¤§pollé—´éš”5åˆ†é’Ÿ
        
        return new KafkaConsumer<>(props);
    }
    
    /**
     * è‡ªåŠ¨æäº¤æ¨¡å¼æ¶ˆè´¹
     */
    public void consumeWithAutoCommit() {
        // é‡æ–°é…ç½®ä¸ºè‡ªåŠ¨æäº¤
        consumer.close();
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "auto-commit-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);       // å¯ç”¨è‡ªåŠ¨æäº¤
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 5000);  // 5ç§’è‡ªåŠ¨æäº¤ä¸€æ¬¡
        
        KafkaConsumer<String, String> autoCommitConsumer = new KafkaConsumer<>(props);
        autoCommitConsumer.subscribe(topics);
        
        System.out.println("å¼€å§‹è‡ªåŠ¨æäº¤æ¨¡å¼æ¶ˆè´¹...");
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = autoCommitConsumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("è‡ªåŠ¨æäº¤æ¶ˆè´¹: topic=%s, partition=%d, offset=%d, key=%s, value=%s%n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    
                    // å¤„ç†æ¶ˆæ¯
                    processMessage(record);
                }
                
                // è‡ªåŠ¨æäº¤ä¼šåœ¨åå°å®šæœŸæ‰§è¡Œ
            }
        } finally {
            autoCommitConsumer.close();
        }
    }
    
    /**
     * æ‰‹åŠ¨åŒæ­¥æäº¤æ¨¡å¼æ¶ˆè´¹
     */
    public void consumeWithSyncCommit() {
        consumer.subscribe(topics);
        running = true;
        
        System.out.println("å¼€å§‹æ‰‹åŠ¨åŒæ­¥æäº¤æ¨¡å¼æ¶ˆè´¹...");
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("åŒæ­¥æäº¤æ¶ˆè´¹: topic=%s, partition=%d, offset=%d, key=%s, value=%s%n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    
                    // å¤„ç†æ¶ˆæ¯
                    processMessage(record);
                }
                
                if (!records.isEmpty()) {
                    try {
                        // åŒæ­¥æäº¤ä½ç§»ï¼Œä¼šé˜»å¡ç›´åˆ°æäº¤æˆåŠŸæˆ–å¤±è´¥
                        consumer.commitSync();
                        System.out.println("ä½ç§»åŒæ­¥æäº¤æˆåŠŸ");
                    } catch (CommitFailedException e) {
                        System.err.println("ä½ç§»æäº¤å¤±è´¥: " + e.getMessage());
                    }
                }
            }
        } finally {
            consumer.close();
        }
    }
    
    /**
     * æ‰‹åŠ¨å¼‚æ­¥æäº¤æ¨¡å¼æ¶ˆè´¹
     */
    public void consumeWithAsyncCommit() {
        consumer.subscribe(topics);
        running = true;
        
        System.out.println("å¼€å§‹æ‰‹åŠ¨å¼‚æ­¥æäº¤æ¨¡å¼æ¶ˆè´¹...");
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("å¼‚æ­¥æäº¤æ¶ˆè´¹: topic=%s, partition=%d, offset=%d, key=%s, value=%s%n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    
                    // å¤„ç†æ¶ˆæ¯
                    processMessage(record);
                }
                
                if (!records.isEmpty()) {
                    // å¼‚æ­¥æäº¤ä½ç§»ï¼Œä¸ä¼šé˜»å¡
                    consumer.commitAsync(new OffsetCommitCallback() {
                        @Override
                        public void onComplete(Map<TopicPartition, OffsetAndMetadata> offsets, Exception exception) {
                            if (exception == null) {
                                System.out.println("ä½ç§»å¼‚æ­¥æäº¤æˆåŠŸ: " + offsets);
                            } else {
                                System.err.println("ä½ç§»å¼‚æ­¥æäº¤å¤±è´¥: " + exception.getMessage());
                            }
                        }
                    });
                }
            }
        } finally {
            // å…³é—­å‰è¿›è¡Œæœ€åä¸€æ¬¡åŒæ­¥æäº¤
            try {
                consumer.commitSync();
            } catch (Exception e) {
                System.err.println("æœ€ç»ˆåŒæ­¥æäº¤å¤±è´¥: " + e.getMessage());
            }
            consumer.close();
        }
    }
    
    /**
     * æŒ‡å®šåˆ†åŒºæ¶ˆè´¹
     */
    public void consumeSpecificPartitions(String topic, List<Integer> partitions) {
        // æ„å»ºTopicPartitionåˆ—è¡¨
        List<TopicPartition> topicPartitions = new ArrayList<>();
        for (Integer partition : partitions) {
            topicPartitions.add(new TopicPartition(topic, partition));
        }
        
        // åˆ†é…æŒ‡å®šåˆ†åŒºï¼ˆä¸ä½¿ç”¨subscribeï¼‰
        consumer.assign(topicPartitions);
        
        // å¯é€‰ï¼šæŒ‡å®šèµ·å§‹ä½ç§»
        consumer.seekToBeginning(topicPartitions); // ä»å¤´å¼€å§‹
        // consumer.seekToEnd(topicPartitions);    // ä»æœ«å°¾å¼€å§‹
        
        running = true;
        System.out.println("å¼€å§‹æŒ‡å®šåˆ†åŒºæ¶ˆè´¹: " + topicPartitions);
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("æŒ‡å®šåˆ†åŒºæ¶ˆè´¹: topic=%s, partition=%d, offset=%d, key=%s, value=%s%n",
                        record.topic(), record.partition(), record.offset(), record.key(), record.value());
                    
                    // å¤„ç†æ¶ˆæ¯
                    processMessage(record);
                }
                
                // æ‰‹åŠ¨æäº¤ä½ç§»
                if (!records.isEmpty()) {
                    consumer.commitSync();
                }
            }
        } finally {
            consumer.close();
        }
    }
    
    /**
     * æŒ‰æ—¶é—´æˆ³æ¶ˆè´¹
     */
    public void consumeFromTimestamp(String topic, long timestamp) {
        // è·å–Topicçš„æ‰€æœ‰åˆ†åŒº
        List<PartitionInfo> partitionInfos = consumer.partitionsFor(topic);
        List<TopicPartition> topicPartitions = new ArrayList<>();
        
        for (PartitionInfo partitionInfo : partitionInfos) {
            topicPartitions.add(new TopicPartition(topic, partitionInfo.partition()));
        }
        
        // åˆ†é…åˆ†åŒº
        consumer.assign(topicPartitions);
        
        // æ„å»ºæ—¶é—´æˆ³æŸ¥è¯¢æ˜ å°„
        Map<TopicPartition, Long> timestampsToSearch = new HashMap<>();
        for (TopicPartition tp : topicPartitions) {
            timestampsToSearch.put(tp, timestamp);
        }
        
        // æ ¹æ®æ—¶é—´æˆ³æŸ¥æ‰¾ä½ç§»
        Map<TopicPartition, OffsetAndTimestamp> offsetsForTimes = consumer.offsetsForTimes(timestampsToSearch);
        
        // è®¾ç½®èµ·å§‹ä½ç§»
        for (TopicPartition tp : topicPartitions) {
            OffsetAndTimestamp offsetAndTimestamp = offsetsForTimes.get(tp);
            if (offsetAndTimestamp != null) {
                consumer.seek(tp, offsetAndTimestamp.offset());
                System.out.println("åˆ†åŒº " + tp.partition() + " ä»ä½ç§» " + offsetAndTimestamp.offset() + " å¼€å§‹æ¶ˆè´¹");
            } else {
                // å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”æ—¶é—´æˆ³çš„ä½ç§»ï¼Œä»æœ«å°¾å¼€å§‹
                consumer.seekToEnd(Arrays.asList(tp));
                System.out.println("åˆ†åŒº " + tp.partition() + " æ²¡æœ‰æ‰¾åˆ°å¯¹åº”æ—¶é—´æˆ³çš„ä½ç§»ï¼Œä»æœ«å°¾å¼€å§‹");
            }
        }
        
        running = true;
        System.out.println("å¼€å§‹ä»æ—¶é—´æˆ³ " + new Date(timestamp) + " æ¶ˆè´¹");
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("æ—¶é—´æˆ³æ¶ˆè´¹: topic=%s, partition=%d, offset=%d, timestamp=%s, key=%s, value=%s%n",
                        record.topic(), record.partition(), record.offset(), 
                        new Date(record.timestamp()), record.key(), record.value());
                    
                    // å¤„ç†æ¶ˆæ¯
                    processMessage(record);
                }
                
                if (!records.isEmpty()) {
                    consumer.commitSync();
                }
            }
        } finally {
            consumer.close();
        }
    }
    
    /**
     * æ‰¹é‡å¤„ç†æ¶ˆæ¯
     */
    public void consumeInBatches(int batchSize) {
        consumer.subscribe(topics);
        running = true;
        
        List<ConsumerRecord<String, String>> batch = new ArrayList<>();
        
        System.out.println("å¼€å§‹æ‰¹é‡æ¶ˆè´¹ï¼Œæ‰¹æ¬¡å¤§å°: " + batchSize);
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    batch.add(record);
                    
                    // å½“æ‰¹æ¬¡è¾¾åˆ°æŒ‡å®šå¤§å°æ—¶å¤„ç†
                    if (batch.size() >= batchSize) {
                        processBatch(batch);
                        batch.clear();
                        
                        // æäº¤ä½ç§»
                        consumer.commitSync();
                    }
                }
            }
            
            // å¤„ç†å‰©ä½™çš„æ¶ˆæ¯
            if (!batch.isEmpty()) {
                processBatch(batch);
                consumer.commitSync();
            }
            
        } finally {
            consumer.close();
        }
    }
    
    /**
     * å¤„ç†å•æ¡æ¶ˆæ¯
     */
    private void processMessage(ConsumerRecord<String, String> record) {
        try {
            // æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†
            Thread.sleep(10);
            
            // è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“çš„ä¸šåŠ¡é€»è¾‘
            // ä¾‹å¦‚ï¼šä¿å­˜åˆ°æ•°æ®åº“ã€å‘é€åˆ°å…¶ä»–ç³»ç»Ÿç­‰
            
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            System.err.println("æ¶ˆæ¯å¤„ç†è¢«ä¸­æ–­");
        } catch (Exception e) {
            System.err.println("å¤„ç†æ¶ˆæ¯å¤±è´¥: " + e.getMessage());
            // å¯ä»¥é€‰æ‹©é‡è¯•ã€è®°å½•é”™è¯¯æ—¥å¿—æˆ–å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
        }
    }
    
    /**
     * æ‰¹é‡å¤„ç†æ¶ˆæ¯
     */
    private void processBatch(List<ConsumerRecord<String, String>> batch) {
        System.out.println("å¤„ç†æ‰¹æ¬¡ï¼ŒåŒ…å« " + batch.size() + " æ¡æ¶ˆæ¯");
        
        try {
            // æ‰¹é‡å¤„ç†é€»è¾‘
            for (ConsumerRecord<String, String> record : batch) {
                // å¤„ç†å•æ¡æ¶ˆæ¯
                processMessage(record);
            }
            
            System.out.println("æ‰¹æ¬¡å¤„ç†å®Œæˆ");
            
        } catch (Exception e) {
            System.err.println("æ‰¹æ¬¡å¤„ç†å¤±è´¥: " + e.getMessage());
            // å¯ä»¥é€‰æ‹©é‡è¯•æ•´ä¸ªæ‰¹æ¬¡æˆ–å•ç‹¬å¤„ç†å¤±è´¥çš„æ¶ˆæ¯
        }
    }
    
    /**
     * åœæ­¢æ¶ˆè´¹
     */
    public void stop() {
        running = false;
        System.out.println("æ­£åœ¨åœæ­¢æ¶ˆè´¹è€…...");
    }
    
    /**
     * è·å–æ¶ˆè´¹è€…æŒ‡æ ‡
     */
    public void printConsumerMetrics() {
        Map<MetricName, ? extends Metric> metrics = consumer.metrics();
        
        System.out.println("=== æ¶ˆè´¹è€…æŒ‡æ ‡ ===");
        for (Map.Entry<MetricName, ? extends Metric> entry : metrics.entrySet()) {
            MetricName name = entry.getKey();
            Metric metric = entry.getValue();
            
            // åªæ‰“å°é‡è¦æŒ‡æ ‡
            if (name.name().contains("records-consumed") || 
                name.name().contains("bytes-consumed") ||
                name.name().contains("fetch-latency")) {
                System.out.printf("%s.%s: %.2f%n", name.group(), name.name(), metric.metricValue());
            }
        }
    }
}

// æ¶ˆè´¹è€…ä½¿ç”¨ç¤ºä¾‹
public class ConsumerExample {
    public static void main(String[] args) {
        List<String> topics = Arrays.asList("user-events", "order-events");
        KafkaConsumerBasic consumer = new KafkaConsumerBasic("localhost:9092", "example-group", topics);
        
        // åˆ›å»ºæ¶ˆè´¹çº¿ç¨‹
        Thread consumerThread = new Thread(() -> {
            // é€‰æ‹©ä¸åŒçš„æ¶ˆè´¹æ¨¡å¼
            // consumer.consumeWithAutoCommit();
            // consumer.consumeWithSyncCommit();
            consumer.consumeWithAsyncCommit();
            // consumer.consumeSpecificPartitions("user-events", Arrays.asList(0, 1));
            // consumer.consumeFromTimestamp("user-events", System.currentTimeMillis() - 3600000); // 1å°æ—¶å‰
            // consumer.consumeInBatches(10);
        });
        
        consumerThread.start();
        
        // è¿è¡Œä¸€æ®µæ—¶é—´ååœæ­¢
        try {
            Thread.sleep(60000); // è¿è¡Œ1åˆ†é’Ÿ
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        consumer.stop();
        
        try {
            consumerThread.join();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        // æ‰“å°æ¶ˆè´¹è€…æŒ‡æ ‡
        consumer.printConsumerMetrics();
    }
}
```

</TabItem>
<TabItem value="consumer-group" label="æ¶ˆè´¹è€…ç»„">

```java title="æ¶ˆè´¹è€…ç»„ç®¡ç†å’Œç›‘æ§"
import org.apache.kafka.clients.admin.*;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import java.util.*;
import java.util.concurrent.ExecutionException;

/**
 * Kafkaæ¶ˆè´¹è€…ç»„ç®¡ç†å·¥å…·
 * æä¾›æ¶ˆè´¹è€…ç»„çš„åˆ›å»ºã€ç›‘æ§ã€é‡å¹³è¡¡ç­‰åŠŸèƒ½
 */
public class ConsumerGroupManager {
    
    private final AdminClient adminClient;
    
    public ConsumerGroupManager(String bootstrapServers) {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        this.adminClient = AdminClient.create(props);
    }
    
    /**
     * åˆ—å‡ºæ‰€æœ‰æ¶ˆè´¹è€…ç»„
     */
    public List<String> listConsumerGroups() {
        try {
            ListConsumerGroupsResult result = adminClient.listConsumerGroups();
            return result.all().get().stream()
                .map(ConsumerGroupListing::groupId)
                .collect(Collectors.toList());
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("åˆ—å‡ºæ¶ˆè´¹è€…ç»„å¤±è´¥", e);
        }
    }
    
    /**
     * è·å–æ¶ˆè´¹è€…ç»„è¯¦ç»†ä¿¡æ¯
     */
    public ConsumerGroupInfo getConsumerGroupInfo(String groupId) {
        try {
            // è·å–æ¶ˆè´¹è€…ç»„æè¿°
            DescribeConsumerGroupsResult describeResult = 
                adminClient.describeConsumerGroups(Arrays.asList(groupId));
            ConsumerGroupDescription description = describeResult.all().get().get(groupId);
            
            // è·å–æ¶ˆè´¹è€…ç»„ä½ç§»ä¿¡æ¯
            ListConsumerGroupOffsetsResult offsetsResult = 
                adminClient.listConsumerGroupOffsets(groupId);
            Map<TopicPartition, OffsetAndMetadata> offsets = offsetsResult.partitionsToOffsetAndMetadata().get();
            
            return new ConsumerGroupInfo(description, offsets);
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("è·å–æ¶ˆè´¹è€…ç»„ä¿¡æ¯å¤±è´¥: " + groupId, e);
        }
    }
    
    /**
     * é‡ç½®æ¶ˆè´¹è€…ç»„ä½ç§»
     */
    public void resetConsumerGroupOffsets(String groupId, String topic, OffsetResetStrategy strategy) {
        try {
            // é¦–å…ˆæ£€æŸ¥æ¶ˆè´¹è€…ç»„çŠ¶æ€
            ConsumerGroupInfo groupInfo = getConsumerGroupInfo(groupId);
            if (groupInfo.getState() != ConsumerGroupState.Empty) {
                throw new IllegalStateException("æ¶ˆè´¹è€…ç»„å¿…é¡»ä¸ºç©ºçŠ¶æ€æ‰èƒ½é‡ç½®ä½ç§»ï¼Œå½“å‰çŠ¶æ€: " + groupInfo.getState());
            }
            
            // è·å–Topicåˆ†åŒºä¿¡æ¯
            DescribeTopicsResult topicsResult = adminClient.describeTopics(Arrays.asList(topic));
            TopicDescription topicDescription = topicsResult.all().get().get(topic);
            
            Map<TopicPartition, OffsetAndMetadata> offsetsToReset = new HashMap<>();
            
            for (TopicPartitionInfo partitionInfo : topicDescription.partitions()) {
                TopicPartition tp = new TopicPartition(topic, partitionInfo.partition());
                
                long newOffset;
                switch (strategy) {
                    case EARLIEST:
                        // é‡ç½®åˆ°æœ€æ—©ä½ç§»
                        newOffset = getEarliestOffset(tp);
                        break;
                    case LATEST:
                        // é‡ç½®åˆ°æœ€æ–°ä½ç§»
                        newOffset = getLatestOffset(tp);
                        break;
                    default:
                        throw new IllegalArgumentException("ä¸æ”¯æŒçš„é‡ç½®ç­–ç•¥: " + strategy);
                }
                
                offsetsToReset.put(tp, new OffsetAndMetadata(newOffset));
            }
            
            // æ‰§è¡Œä½ç§»é‡ç½®
            AlterConsumerGroupOffsetsResult alterResult = 
                adminClient.alterConsumerGroupOffsets(groupId, offsetsToReset);
            alterResult.all().get();
            
            System.out.println("æ¶ˆè´¹è€…ç»„ " + groupId + " çš„ä½ç§»é‡ç½®æˆåŠŸï¼Œç­–ç•¥: " + strategy);
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("é‡ç½®æ¶ˆè´¹è€…ç»„ä½ç§»å¤±è´¥: " + groupId, e);
        }
    }
    
    /**
     * åˆ é™¤æ¶ˆè´¹è€…ç»„
     */
    public void deleteConsumerGroup(String groupId) {
        try {
            // æ£€æŸ¥æ¶ˆè´¹è€…ç»„çŠ¶æ€
            ConsumerGroupInfo groupInfo = getConsumerGroupInfo(groupId);
            if (groupInfo.getState() != ConsumerGroupState.Empty) {
                throw new IllegalStateException("åªèƒ½åˆ é™¤ç©ºçš„æ¶ˆè´¹è€…ç»„ï¼Œå½“å‰çŠ¶æ€: " + groupInfo.getState());
            }
            
            DeleteConsumerGroupsResult result = adminClient.deleteConsumerGroups(Arrays.asList(groupId));
            result.all().get();
            
            System.out.println("æ¶ˆè´¹è€…ç»„ " + groupId + " åˆ é™¤æˆåŠŸ");
            
        } catch (InterruptedException | ExecutionException e) {
            throw new RuntimeException("åˆ é™¤æ¶ˆè´¹è€…ç»„å¤±è´¥: " + groupId, e);
        }
    }
    
    /**
     * ç›‘æ§æ¶ˆè´¹è€…ç»„å»¶è¿Ÿ
     */
    public ConsumerGroupLag getConsumerGroupLag(String groupId) {
        try {
            ConsumerGroupInfo groupInfo = getConsumerGroupInfo(groupId);
            Map<TopicPartition, OffsetAndMetadata> currentOffsets = groupInfo.getOffsets();
            
            // è·å–æ¯ä¸ªåˆ†åŒºçš„æœ€æ–°ä½ç§»
            Map<TopicPartition, Long> endOffsets = getEndOffsets(currentOffsets.keySet());
            
            Map<TopicPartition, Long> lagMap = new HashMap<>();
            long totalLag = 0;
            
            for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : currentOffsets.entrySet()) {
                TopicPartition tp = entry.getKey();
                long currentOffset = entry.getValue().offset();
                long endOffset = endOffsets.getOrDefault(tp, currentOffset);
                
                long lag = Math.max(0, endOffset - currentOffset);
                lagMap.put(tp, lag);
                totalLag += lag;
            }
            
            return new ConsumerGroupLag(groupId, lagMap, totalLag);
            
        } catch (Exception e) {
            throw new RuntimeException("è·å–æ¶ˆè´¹è€…ç»„å»¶è¿Ÿå¤±è´¥: " + groupId, e);
        }
    }
    
    /**
     * æ‰“å°æ¶ˆè´¹è€…ç»„è¯¦ç»†æŠ¥å‘Š
     */
    public void printConsumerGroupReport(String groupId) {
        ConsumerGroupInfo info = getConsumerGroupInfo(groupId);
        ConsumerGroupLag lag = getConsumerGroupLag(groupId);
        
        System.out.println("=== æ¶ˆè´¹è€…ç»„è¯¦ç»†æŠ¥å‘Š: " + groupId + " ===");
        System.out.println("çŠ¶æ€: " + info.getState());
        System.out.println("åè°ƒå™¨: " + info.getCoordinator().host() + ":" + info.getCoordinator().port());
        System.out.println("åˆ†åŒºåˆ†é…ç­–ç•¥: " + info.getPartitionAssignor());
        System.out.println("æˆå‘˜æ•°é‡: " + info.getMembers().size());
        System.out.println("æ€»å»¶è¿Ÿ: " + lag.getTotalLag() + " æ¡æ¶ˆæ¯");
        
        System.out.println("\n--- æˆå‘˜ä¿¡æ¯ ---");
        for (MemberDescription member : info.getMembers()) {
            System.out.printf("æˆå‘˜ID: %s, å®¢æˆ·ç«¯ID: %s, ä¸»æœº: %s%n",
                member.consumerId(), member.clientId(), member.host());
            System.out.println("  åˆ†é…çš„åˆ†åŒº: " + member.assignment().topicPartitions());
        }
        
        System.out.println("\n--- åˆ†åŒºä½ç§»å’Œå»¶è¿Ÿ ---");
        for (Map.Entry<TopicPartition, Long> entry : lag.getPartitionLags().entrySet()) {
            TopicPartition tp = entry.getKey();
            long partitionLag = entry.getValue();
            long currentOffset = info.getOffsets().get(tp).offset();
            
            System.out.printf("%s-%d: å½“å‰ä½ç§»=%d, å»¶è¿Ÿ=%d%n",
                tp.topic(), tp.partition(), currentOffset, partitionLag);
        }
    }
    
    private long getEarliestOffset(TopicPartition tp) {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨Consumer.beginningOffsets()
        return 0L;
    }
    
    private long getLatestOffset(TopicPartition tp) {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨Consumer.endOffsets()
        return Long.MAX_VALUE;
    }
    
    private Map<TopicPartition, Long> getEndOffsets(Set<TopicPartition> partitions) {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä½¿ç”¨Consumer.endOffsets()
        Map<TopicPartition, Long> endOffsets = new HashMap<>();
        for (TopicPartition tp : partitions) {
            endOffsets.put(tp, 1000L); // æ¨¡æ‹Ÿå€¼
        }
        return endOffsets;
    }
    
    public void close() {
        adminClient.close();
    }
    
    // ä½ç§»é‡ç½®ç­–ç•¥æšä¸¾
    public enum OffsetResetStrategy {
        EARLIEST,   // é‡ç½®åˆ°æœ€æ—©
        LATEST      // é‡ç½®åˆ°æœ€æ–°
    }
    
    // æ¶ˆè´¹è€…ç»„ä¿¡æ¯å°è£…ç±»
    public static class ConsumerGroupInfo {
        private final ConsumerGroupDescription description;
        private final Map<TopicPartition, OffsetAndMetadata> offsets;
        
        public ConsumerGroupInfo(ConsumerGroupDescription description, 
                               Map<TopicPartition, OffsetAndMetadata> offsets) {
            this.description = description;
            this.offsets = offsets;
        }
        
        public String getGroupId() { return description.groupId(); }
        public ConsumerGroupState getState() { return description.state(); }
        public Node getCoordinator() { return description.coordinator(); }
        public String getPartitionAssignor() { return description.partitionAssignor(); }
        public Collection<MemberDescription> getMembers() { return description.members(); }
        public Map<TopicPartition, OffsetAndMetadata> getOffsets() { return offsets; }
    }
    
    // æ¶ˆè´¹è€…ç»„å»¶è¿Ÿä¿¡æ¯
    public static class ConsumerGroupLag {
        private final String groupId;
        private final Map<TopicPartition, Long> partitionLags;
        private final long totalLag;
        
        public ConsumerGroupLag(String groupId, Map<TopicPartition, Long> partitionLags, long totalLag) {
            this.groupId = groupId;
            this.partitionLags = partitionLags;
            this.totalLag = totalLag;
        }
        
        public String getGroupId() { return groupId; }
        public Map<TopicPartition, Long> getPartitionLags() { return partitionLags; }
        public long getTotalLag() { return totalLag; }
    }
}

/**
 * å¤šçº¿ç¨‹æ¶ˆè´¹è€…ç»„å®ç°
 * æ¼”ç¤ºå¦‚ä½•åœ¨ä¸€ä¸ªåº”ç”¨ä¸­å¯åŠ¨å¤šä¸ªæ¶ˆè´¹è€…å®ä¾‹
 */
public class MultiThreadConsumerGroup {
    
    private final String bootstrapServers;
    private final String groupId;
    private final List<String> topics;
    private final int consumerCount;
    private final List<Thread> consumerThreads;
    private volatile boolean running = false;
    
    public MultiThreadConsumerGroup(String bootstrapServers, String groupId, 
                                  List<String> topics, int consumerCount) {
        this.bootstrapServers = bootstrapServers;
        this.groupId = groupId;
        this.topics = topics;
        this.consumerCount = consumerCount;
        this.consumerThreads = new ArrayList<>();
    }
    
    /**
     * å¯åŠ¨å¤šä¸ªæ¶ˆè´¹è€…çº¿ç¨‹
     */
    public void start() {
        running = true;
        
        for (int i = 0; i < consumerCount; i++) {
            final int consumerId = i;
            Thread consumerThread = new Thread(() -> {
                runConsumer(consumerId);
            }, "Consumer-" + i);
            
            consumerThreads.add(consumerThread);
            consumerThread.start();
        }
        
        System.out.println("å¯åŠ¨äº† " + consumerCount + " ä¸ªæ¶ˆè´¹è€…çº¿ç¨‹");
    }
    
    /**
     * è¿è¡Œå•ä¸ªæ¶ˆè´¹è€…
     */
    private void runConsumer(int consumerId) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        
        // ä¸ºæ¯ä¸ªæ¶ˆè´¹è€…è®¾ç½®å”¯ä¸€çš„å®¢æˆ·ç«¯ID
        props.put(ConsumerConfig.CLIENT_ID_CONFIG, "consumer-" + consumerId);
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        
        try {
            consumer.subscribe(topics, new ConsumerRebalanceListener() {
                @Override
                public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
                    System.out.println("Consumer-" + consumerId + " åˆ†åŒºè¢«æ’¤é”€: " + partitions);
                    // åœ¨åˆ†åŒºè¢«æ’¤é”€å‰æäº¤ä½ç§»
                    consumer.commitSync();
                }
                
                @Override
                public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
                    System.out.println("Consumer-" + consumerId + " åˆ†åŒºè¢«åˆ†é…: " + partitions);
                }
            });
            
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Consumer-%d å¤„ç†æ¶ˆæ¯: topic=%s, partition=%d, offset=%d, key=%s%n",
                        consumerId, record.topic(), record.partition(), record.offset(), record.key());
                    
                    // æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†
                    try {
                        Thread.sleep(100);
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                }
                
                if (!records.isEmpty()) {
                    consumer.commitSync();
                }
            }
            
        } catch (Exception e) {
            System.err.println("Consumer-" + consumerId + " å‘ç”Ÿå¼‚å¸¸: " + e.getMessage());
        } finally {
            consumer.close();
            System.out.println("Consumer-" + consumerId + " å·²å…³é—­");
        }
    }
    
    /**
     * åœæ­¢æ‰€æœ‰æ¶ˆè´¹è€…
     */
    public void stop() {
        running = false;
        
        // ç­‰å¾…æ‰€æœ‰æ¶ˆè´¹è€…çº¿ç¨‹ç»“æŸ
        for (Thread thread : consumerThreads) {
            try {
                thread.join(5000); // æœ€å¤šç­‰å¾…5ç§’
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        }
        
        System.out.println("æ‰€æœ‰æ¶ˆè´¹è€…å·²åœæ­¢");
    }
    
    public static void main(String[] args) {
        MultiThreadConsumerGroup consumerGroup = new MultiThreadConsumerGroup(
            "localhost:9092",
            "multi-thread-group",
            Arrays.asList("user-events"),
            3 // å¯åŠ¨3ä¸ªæ¶ˆè´¹è€…
        );
        
        // å¯åŠ¨æ¶ˆè´¹è€…ç»„
        consumerGroup.start();
        
        // è¿è¡Œä¸€æ®µæ—¶é—´
        try {
            Thread.sleep(60000); // è¿è¡Œ1åˆ†é’Ÿ
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        // åœæ­¢æ¶ˆè´¹è€…ç»„
        consumerGroup.stop();
    }
}
```

</TabItem>
<TabItem value="rebalance" label="é‡å¹³è¡¡æœºåˆ¶">

```java title="æ¶ˆè´¹è€…é‡å¹³è¡¡ç›‘å¬å’Œå¤„ç†"
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;

/**
 * æ¶ˆè´¹è€…é‡å¹³è¡¡ç›‘å¬å™¨å®ç°
 * å¤„ç†åˆ†åŒºåˆ†é…å˜åŒ–ï¼Œå®ç°ä¼˜é›…çš„é‡å¹³è¡¡
 */
public class RebalanceAwareConsumer {
    
    private final KafkaConsumer<String, String> consumer;
    private final Map<TopicPartition, OffsetAndMetadata> currentOffsets = new ConcurrentHashMap<>();
    private volatile boolean running = false;
    
    public RebalanceAwareConsumer(String bootstrapServers, String groupId, List<String> topics) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        
        // é‡å¹³è¡¡ç›¸å…³é…ç½®
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);          // ç¦ç”¨è‡ªåŠ¨æäº¤
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);          // ä¼šè¯è¶…æ—¶30ç§’
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);       // å¿ƒè·³é—´éš”10ç§’
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000);       // æœ€å¤§pollé—´éš”5åˆ†é’Ÿ
        
        // ä½¿ç”¨åä½œå¼é‡å¹³è¡¡ç­–ç•¥ï¼ˆKafka 2.4+ï¼‰
        props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, 
                 "org.apache.kafka.clients.consumer.CooperativeStickyAssignor");
        
        this.consumer = new KafkaConsumer<>(props);
        
        // è®¢é˜…Topicå¹¶è®¾ç½®é‡å¹³è¡¡ç›‘å¬å™¨
        consumer.subscribe(topics, new RebalanceListener());
    }
    
    /**
     * å¼€å§‹æ¶ˆè´¹
     */
    public void startConsuming() {
        running = true;
        
        try {
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                for (ConsumerRecord<String, String> record : records) {
                    // å¤„ç†æ¶ˆæ¯
                    processRecord(record);
                    
                    // è·Ÿè¸ªå½“å‰ä½ç§»
                    currentOffsets.put(
                        new TopicPartition(record.topic(), record.partition()),
                        new OffsetAndMetadata(record.offset() + 1)
                    );
                }
                
                // å®šæœŸæäº¤ä½ç§»
                if (!records.isEmpty()) {
                    commitOffsets();
                }
            }
        } catch (Exception e) {
            System.err.println("æ¶ˆè´¹è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: " + e.getMessage());
        } finally {
            try {
                consumer.commitSync(currentOffsets);
            } catch (Exception e) {
                System.err.println("æœ€ç»ˆæäº¤ä½ç§»å¤±è´¥: " + e.getMessage());
            }
            consumer.close();
        }
    }
    
    /**
     * å¤„ç†å•æ¡æ¶ˆæ¯
     */
    private void processRecord(ConsumerRecord<String, String> record) {
        try {
            System.out.printf("å¤„ç†æ¶ˆæ¯: topic=%s, partition=%d, offset=%d, key=%s, value=%s%n",
                record.topic(), record.partition(), record.offset(), record.key(), record.value());
            
            // æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†æ—¶é—´
            Thread.sleep(50);
            
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            System.err.println("å¤„ç†æ¶ˆæ¯å¤±è´¥: " + e.getMessage());
        }
    }
    
    /**
     * æäº¤ä½ç§»
     */
    private void commitOffsets() {
        try {
            consumer.commitAsync(currentOffsets, (offsets, exception) -> {
                if (exception != null) {
                    System.err.println("å¼‚æ­¥æäº¤ä½ç§»å¤±è´¥: " + exception.getMessage());
                }
            });
        } catch (Exception e) {
            System.err.println("æäº¤ä½ç§»å¼‚å¸¸: " + e.getMessage());
        }
    }
    
    /**
     * åœæ­¢æ¶ˆè´¹
     */
    public void stop() {
        running = false;
    }
    
    /**
     * é‡å¹³è¡¡ç›‘å¬å™¨å®ç°
     */
    private class RebalanceListener implements ConsumerRebalanceListener {
        
        @Override
        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
            System.out.println("=== åˆ†åŒºæ’¤é”€å¼€å§‹ ===");
            System.out.println("è¢«æ’¤é”€çš„åˆ†åŒº: " + partitions);
            
            // åœ¨åˆ†åŒºè¢«æ’¤é”€å‰ï¼ŒåŒæ­¥æäº¤å½“å‰ä½ç§»
            try {
                Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();
                for (TopicPartition partition : partitions) {
                    OffsetAndMetadata offset = currentOffsets.get(partition);
                    if (offset != null) {
                        offsetsToCommit.put(partition, offset);
                    }
                }
                
                if (!offsetsToCommit.isEmpty()) {
                    consumer.commitSync(offsetsToCommit);
                    System.out.println("æ’¤é”€å‰ä½ç§»æäº¤æˆåŠŸ: " + offsetsToCommit);
                }
                
                // æ¸…ç†è¢«æ’¤é”€åˆ†åŒºçš„ä½ç§»è®°å½•
                for (TopicPartition partition : partitions) {
                    currentOffsets.remove(partition);
                }
                
            } catch (Exception e) {
                System.err.println("æ’¤é”€å‰æäº¤ä½ç§»å¤±è´¥: " + e.getMessage());
            }
            
            System.out.println("=== åˆ†åŒºæ’¤é”€å®Œæˆ ===");
        }
        
        @Override
        public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
            System.out.println("=== åˆ†åŒºåˆ†é…å¼€å§‹ ===");
            System.out.println("æ–°åˆ†é…çš„åˆ†åŒº: " + partitions);
            
            // å¯ä»¥åœ¨è¿™é‡Œè¿›è¡Œä¸€äº›åˆå§‹åŒ–å·¥ä½œ
            for (TopicPartition partition : partitions) {
                // è·å–å½“å‰åˆ†åŒºçš„ä½ç§»
                OffsetAndMetadata committed = consumer.committed(partition);
                if (committed != null) {
                    System.out.println("åˆ†åŒº " + partition + " çš„å·²æäº¤ä½ç§»: " + committed.offset());
                } else {
                    System.out.println("åˆ†åŒº " + partition + " æ²¡æœ‰å·²æäº¤çš„ä½ç§»");
                }
            }
            
            System.out.println("=== åˆ†åŒºåˆ†é…å®Œæˆ ===");
        }
        
        @Override
        public void onPartitionsLost(Collection<TopicPartition> partitions) {
            System.out.println("=== åˆ†åŒºä¸¢å¤± ===");
            System.out.println("ä¸¢å¤±çš„åˆ†åŒº: " + partitions);
            
            // æ¸…ç†ä¸¢å¤±åˆ†åŒºçš„ä½ç§»è®°å½•
            for (TopicPartition partition : partitions) {
                currentOffsets.remove(partition);
            }
            
            // æ³¨æ„ï¼šåˆ†åŒºä¸¢å¤±æ—¶ä¸åº”è¯¥æäº¤ä½ç§»ï¼Œå› ä¸ºå¯èƒ½å·²ç»è¢«å…¶ä»–æ¶ˆè´¹è€…æ¥ç®¡
            System.out.println("=== åˆ†åŒºä¸¢å¤±å¤„ç†å®Œæˆ ===");
        }
    }
    
    public static void main(String[] args) {
        // åˆ›å»ºå¤šä¸ªæ¶ˆè´¹è€…å®ä¾‹æ¥æ¼”ç¤ºé‡å¹³è¡¡
        List<RebalanceAwareConsumer> consumers = new ArrayList<>();
        List<Thread> threads = new ArrayList<>();
        
        // å¯åŠ¨ç¬¬ä¸€ä¸ªæ¶ˆè´¹è€…
        RebalanceAwareConsumer consumer1 = new RebalanceAwareConsumer(
            "localhost:9092", "rebalance-demo-group", Arrays.asList("user-events"));
        Thread thread1 = new Thread(consumer1::startConsuming, "Consumer-1");
        consumers.add(consumer1);
        threads.add(thread1);
        thread1.start();
        
        try {
            // ç­‰å¾…5ç§’åå¯åŠ¨ç¬¬äºŒä¸ªæ¶ˆè´¹è€…ï¼ˆè§¦å‘é‡å¹³è¡¡ï¼‰
            Thread.sleep(5000);
            System.out.println("\nå¯åŠ¨ç¬¬äºŒä¸ªæ¶ˆè´¹è€…...\n");
            
            RebalanceAwareConsumer consumer2 = new RebalanceAwareConsumer(
                "localhost:9092", "rebalance-demo-group", Arrays.asList("user-events"));
            Thread thread2 = new Thread(consumer2::startConsuming, "Consumer-2");
            consumers.add(consumer2);
            threads.add(thread2);
            thread2.start();
            
            // å†ç­‰å¾…5ç§’åå¯åŠ¨ç¬¬ä¸‰ä¸ªæ¶ˆè´¹è€…ï¼ˆå†æ¬¡è§¦å‘é‡å¹³è¡¡ï¼‰
            Thread.sleep(5000);
            System.out.println("\nå¯åŠ¨ç¬¬ä¸‰ä¸ªæ¶ˆè´¹è€…...\n");
            
            RebalanceAwareConsumer consumer3 = new RebalanceAwareConsumer(
                "localhost:9092", "rebalance-demo-group", Arrays.asList("user-events"));
            Thread thread3 = new Thread(consumer3::startConsuming, "Consumer-3");
            consumers.add(consumer3);
            threads.add(thread3);
            thread3.start();
            
            // è¿è¡Œä¸€æ®µæ—¶é—´
            Thread.sleep(10000);
            
            // åœæ­¢ç¬¬äºŒä¸ªæ¶ˆè´¹è€…ï¼ˆè§¦å‘é‡å¹³è¡¡ï¼‰
            System.out.println("\nåœæ­¢ç¬¬äºŒä¸ªæ¶ˆè´¹è€…...\n");
            consumer2.stop();
            thread2.join();
            
            // å†è¿è¡Œä¸€æ®µæ—¶é—´
            Thread.sleep(10000);
            
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        } finally {
            // åœæ­¢æ‰€æœ‰æ¶ˆè´¹è€…
            for (RebalanceAwareConsumer consumer : consumers) {
                consumer.stop();
            }
            
            for (Thread thread : threads) {
                try {
                    thread.join();
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                }
            }
        }
    }
}

/**
 * é‡å¹³è¡¡ç­–ç•¥å¯¹æ¯”æ¼”ç¤º
 */
public class PartitionAssignmentStrategyDemo {
    
    /**
     * æ¼”ç¤ºä¸åŒåˆ†åŒºåˆ†é…ç­–ç•¥çš„æ•ˆæœ
     */
    public static void demonstrateAssignmentStrategies() {
        System.out.println("=== åˆ†åŒºåˆ†é…ç­–ç•¥å¯¹æ¯” ===");
        
        // æ¨¡æ‹Ÿåœºæ™¯ï¼š3ä¸ªæ¶ˆè´¹è€…ï¼Œ6ä¸ªåˆ†åŒº
        int consumerCount = 3;
        int partitionCount = 6;
        
        System.out.println("åœºæ™¯: " + consumerCount + " ä¸ªæ¶ˆè´¹è€…, " + partitionCount + " ä¸ªåˆ†åŒº");
        
        // Rangeç­–ç•¥
        System.out.println("\n1. Rangeç­–ç•¥ (é»˜è®¤):");
        demonstrateRangeAssignment(consumerCount, partitionCount);
        
        // RoundRobinç­–ç•¥
        System.out.println("\n2. RoundRobinç­–ç•¥:");
        demonstrateRoundRobinAssignment(consumerCount, partitionCount);
        
        // Stickyç­–ç•¥
        System.out.println("\n3. Stickyç­–ç•¥:");
        demonstrateStickyAssignment(consumerCount, partitionCount);
        
        // CooperativeStickyç­–ç•¥
        System.out.println("\n4. CooperativeStickyç­–ç•¥:");
        demonstrateCooperativeStickyAssignment(consumerCount, partitionCount);
    }
    
    private static void demonstrateRangeAssignment(int consumers, int partitions) {
        // Rangeç­–ç•¥ï¼šæŒ‰åˆ†åŒºèŒƒå›´åˆ†é…
        int partitionsPerConsumer = partitions / consumers;
        int remainder = partitions % consumers;
        
        for (int i = 0; i < consumers; i++) {
            int start = i * partitionsPerConsumer + Math.min(i, remainder);
            int count = partitionsPerConsumer + (i < remainder ? 1 : 0);
            int end = start + count - 1;
            
            System.out.printf("Consumer-%d: åˆ†åŒº %d-%d (%dä¸ªåˆ†åŒº)%n", i, start, end, count);
        }
    }
    
    private static void demonstrateRoundRobinAssignment(int consumers, int partitions) {
        // RoundRobinç­–ç•¥ï¼šè½®è¯¢åˆ†é…
        Map<Integer, List<Integer>> assignment = new HashMap<>();
        for (int i = 0; i < consumers; i++) {
            assignment.put(i, new ArrayList<>());
        }
        
        for (int partition = 0; partition < partitions; partition++) {
            int consumer = partition % consumers;
            assignment.get(consumer).add(partition);
        }
        
        for (int i = 0; i < consumers; i++) {
            System.out.printf("Consumer-%d: åˆ†åŒº %s (%dä¸ªåˆ†åŒº)%n", 
                i, assignment.get(i), assignment.get(i).size());
        }
    }
    
    private static void demonstrateStickyAssignment(int consumers, int partitions) {
        // Stickyç­–ç•¥ï¼šå°½å¯èƒ½ä¿æŒåŸæœ‰åˆ†é…
        System.out.println("åˆå§‹åˆ†é…ï¼ˆç±»ä¼¼RoundRobinï¼‰:");
        demonstrateRoundRobinAssignment(consumers, partitions);
        
        System.out.println("Consumer-1 ç¦»å¼€åçš„é‡æ–°åˆ†é…:");
        // æ¨¡æ‹ŸConsumer-1ç¦»å¼€ï¼Œå…¶åˆ†åŒºè¢«å…¶ä»–æ¶ˆè´¹è€…æ¥ç®¡
        System.out.println("Consumer-0: åˆ†åŒº [0, 3, 1] (3ä¸ªåˆ†åŒº) - æ¥ç®¡äº†åˆ†åŒº1");
        System.out.println("Consumer-2: åˆ†åŒº [2, 5, 4] (3ä¸ªåˆ†åŒº) - æ¥ç®¡äº†åˆ†åŒº4");
    }
    
    private static void demonstrateCooperativeStickyAssignment(int consumers, int partitions) {
        // CooperativeStickyç­–ç•¥ï¼šå¢é‡é‡å¹³è¡¡
        System.out.println("æ”¯æŒå¢é‡é‡å¹³è¡¡ï¼Œå‡å°‘åœé¡¿æ—¶é—´");
        System.out.println("åªç§»åŠ¨å¿…è¦çš„åˆ†åŒºï¼Œå…¶ä»–åˆ†åŒºç»§ç»­å¤„ç†æ¶ˆæ¯");
        demonstrateStickyAssignment(consumers, partitions);
    }
    
    public static void main(String[] args) {
        demonstrateAssignmentStrategies();
    }
}
```

</TabItem>
</Tabs>

## 4. Kafka Streamsæµå¤„ç†æ¡†æ¶

### 4.1 Kafka Streamsæ ¸å¿ƒæ¦‚å¿µ

Kafka Streamsæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºå®æ—¶æµå¤„ç†åº”ç”¨ç¨‹åºçš„Javaåº“ï¼Œå®ƒå°†Kafkaä½œä¸ºæµå¤„ç†çš„åŸºç¡€è®¾æ–½ï¼Œæä¾›äº†é«˜çº§çš„æµå¤„ç†æŠ½è±¡å’Œä¸°å¯Œçš„æ“ä½œç¬¦ã€‚

<Tabs>
<TabItem value="concepts" label="æ ¸å¿ƒæ¦‚å¿µ">

```mermaid
graph TB
    subgraph "Kafka Streamsæ¶æ„"
        A[Stream] --> B[KStream]
        A --> C[KTable]
        A --> D[GlobalKTable]
        
        B --> E[æ— çŠ¶æ€æ“ä½œ]
        B --> F[æœ‰çŠ¶æ€æ“ä½œ]
        
        E --> E1[map/filter]
        E --> E2[flatMap]
        E --> E3[foreach]
        
        F --> F1[groupBy]
        F --> F2[aggregate]
        F --> F3[join]
        F --> F4[windowing]
        
        G[Topology] --> H[Source Processor]
        G --> I[Stream Processor]
        G --> J[Sink Processor]
        
        K[State Store] --> L[RocksDB]
        K --> M[In-Memory]
        K --> N[Custom Store]
    end
```

**æ ¸å¿ƒæŠ½è±¡**ï¼š
- **KStream**ï¼šè®°å½•æµï¼Œæ¯æ¡è®°å½•ä»£è¡¨ä¸€ä¸ªäº‹ä»¶
- **KTable**ï¼šå˜æ›´æµï¼Œæ¯æ¡è®°å½•ä»£è¡¨ä¸€ä¸ªçŠ¶æ€æ›´æ–°
- **GlobalKTable**ï¼šå…¨å±€è¡¨ï¼Œæ‰€æœ‰å®ä¾‹éƒ½æœ‰å®Œæ•´å‰¯æœ¬
- **Topology**ï¼šæµå¤„ç†æ‹“æ‰‘ï¼Œå®šä¹‰æ•°æ®æµè½¬æ¢é€»è¾‘

</TabItem>
<TabItem value="basic-example" label="åŸºç¡€ç¤ºä¾‹">

```java title="Kafka StreamsåŸºç¡€ç¤ºä¾‹"
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.*;
import org.apache.kafka.streams.kstream.*;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;

/**
 * Kafka StreamsåŸºç¡€ç¤ºä¾‹
 * å®ç°è¯é¢‘ç»Ÿè®¡çš„ç»å…¸æµå¤„ç†åº”ç”¨
 */
public class WordCountExample {
    
    public static void main(String[] args) {
        // é…ç½®Streamsåº”ç”¨
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "wordcount-application");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        
        // æ„å»ºæµå¤„ç†æ‹“æ‰‘
        StreamsBuilder builder = new StreamsBuilder();
        
        // 1. ä»è¾“å…¥Topicè¯»å–æ•°æ®
        KStream<String, String> textLines = builder.stream("text-input");
        
        // 2. æµå¤„ç†é€»è¾‘
        KTable<String, Long> wordCounts = textLines
            // å°†æ¯è¡Œæ–‡æœ¬è½¬æ¢ä¸ºå°å†™
            .mapValues(textLine -> textLine.toLowerCase())
            // æŒ‰ç©ºæ ¼åˆ†å‰²å•è¯
            .flatMapValues(textLine -> Arrays.asList(textLine.split("\\W+")))
            // è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
            .filter((key, word) -> !word.isEmpty())
            // é‡æ–°è®¾ç½®keyä¸ºå•è¯
            .selectKey((key, word) -> word)
            // æŒ‰å•è¯åˆ†ç»„
            .groupByKey()
            // è®¡æ•°èšåˆ
            .count(Materialized.as("counts-store"));
        
        // 3. è¾“å‡ºç»“æœåˆ°Topic
        wordCounts.toStream().to("wordcount-output", Produced.with(Serdes.String(), Serdes.Long()));
        
        // æ„å»ºå¹¶å¯åŠ¨Streamsåº”ç”¨
        Topology topology = builder.build();
        KafkaStreams streams = new KafkaStreams(topology, props);
        
        // ä¼˜é›…å…³é—­å¤„ç†
        CountDownLatch latch = new CountDownLatch(1);
        Runtime.getRuntime().addShutdownHook(new Thread("streams-shutdown-hook") {
            @Override
            public void run() {
                streams.close();
                latch.countDown();
            }
        });
        
        try {
            streams.start();
            System.out.println("WordCountåº”ç”¨å·²å¯åŠ¨");
            System.out.println("æ‹“æ‰‘ç»“æ„:\n" + topology.describe());
            latch.await();
        } catch (Throwable e) {
            System.err.println("åº”ç”¨å¼‚å¸¸é€€å‡º: " + e.getMessage());
            System.exit(1);
        }
        System.exit(0);
    }
}

/**
 * æ›´å¤æ‚çš„æµå¤„ç†ç¤ºä¾‹ - ç”¨æˆ·è¡Œä¸ºåˆ†æ
 */
public class UserBehaviorAnalysis {
    
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "user-behavior-analysis");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        
        StreamsBuilder builder = new StreamsBuilder();
        
        // ç”¨æˆ·äº‹ä»¶æµ
        KStream<String, String> userEvents = builder.stream("user-events");
        
        // è§£æJSONå¹¶è½¬æ¢ä¸ºUserEventå¯¹è±¡
        KStream<String, UserEvent> parsedEvents = userEvents
            .mapValues(json -> parseUserEvent(json))
            .filter((key, event) -> event != null);
        
        // 1. å®æ—¶ç”¨æˆ·æ´»è·ƒåº¦ç»Ÿè®¡
        KTable<String, Long> userActivityCount = parsedEvents
            .selectKey((key, event) -> event.getUserId())
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count(Materialized.as("user-activity-store"))
            .suppress(Suppressed.untilWindowCloses(Suppressed.BufferConfig.unbounded()))
            .toStream()
            .map((windowedKey, count) -> KeyValue.pair(windowedKey.key(), count))
            .groupByKey()
            .reduce((oldValue, newValue) -> newValue);
        
        // 2. é¡µé¢è®¿é—®çƒ­åº¦ç»Ÿè®¡
        KTable<String, Long> pageViewCount = parsedEvents
            .filter((key, event) -> "page_view".equals(event.getEventType()))
            .selectKey((key, event) -> event.getPageId())
            .groupByKey()
            .count(Materialized.as("page-view-store"));
        
        // 3. ç”¨æˆ·è½¬åŒ–æ¼æ–—åˆ†æ
        KStream<String, String> conversionFunnel = parsedEvents
            .selectKey((key, event) -> event.getUserId())
            .groupByKey()
            .aggregate(
                () -> new ConversionState(),
                (key, event, state) -> updateConversionState(state, event),
                Materialized.<String, ConversionState, KeyValueStore<Bytes, byte[]>>as("conversion-store")
                    .withValueSerde(new ConversionStateSerde())
            )
            .toStream()
            .filter((userId, state) -> state.isConverted())
            .mapValues(state -> "ç”¨æˆ· " + state.getUserId() + " å®Œæˆè½¬åŒ–");
        
        // è¾“å‡ºç»“æœ
        userActivityCount.toStream().to("user-activity-output", 
            Produced.with(Serdes.String(), Serdes.Long()));
        pageViewCount.toStream().to("page-view-output", 
            Produced.with(Serdes.String(), Serdes.Long()));
        conversionFunnel.to("conversion-output");
        
        // å¯åŠ¨åº”ç”¨
        Topology topology = builder.build();
        KafkaStreams streams = new KafkaStreams(topology, props);
        
        streams.setStateListener((newState, oldState) -> {
            System.out.println("çŠ¶æ€å˜æ›´: " + oldState + " -> " + newState);
        });
        
        streams.setUncaughtExceptionHandler((thread, exception) -> {
            System.err.println("æœªæ•è·å¼‚å¸¸: " + exception.getMessage());
            return StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_CLIENT;
        });
        
        streams.start();
        
        // æ·»åŠ å…³é—­é’©å­
        Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
    }
    
    private static UserEvent parseUserEvent(String json) {
        try {
            // ç®€åŒ–çš„JSONè§£æï¼Œå®é™…åº”ä½¿ç”¨Jacksonç­‰åº“
            // å‡è®¾JSONæ ¼å¼: {"userId":"user1","eventType":"page_view","pageId":"home","timestamp":1640995200000}
            return new UserEvent("user1", "page_view", "home", System.currentTimeMillis());
        } catch (Exception e) {
            System.err.println("è§£æç”¨æˆ·äº‹ä»¶å¤±è´¥: " + e.getMessage());
            return null;
        }
    }
    
    private static ConversionState updateConversionState(ConversionState state, UserEvent event) {
        // ç®€åŒ–çš„è½¬åŒ–çŠ¶æ€æ›´æ–°é€»è¾‘
        switch (event.getEventType()) {
            case "page_view":
                state.setViewedPage(true);
                break;
            case "add_to_cart":
                state.setAddedToCart(true);
                break;
            case "purchase":
                state.setPurchased(true);
                break;
        }
        state.setLastEventTime(event.getTimestamp());
        return state;
    }
    
    // ç”¨æˆ·äº‹ä»¶æ•°æ®ç±»
    public static class UserEvent {
        private String userId;
        private String eventType;
        private String pageId;
        private long timestamp;
        
        public UserEvent(String userId, String eventType, String pageId, long timestamp) {
            this.userId = userId;
            this.eventType = eventType;
            this.pageId = pageId;
            this.timestamp = timestamp;
        }
        
        // Getters
        public String getUserId() { return userId; }
        public String getEventType() { return eventType; }
        public String getPageId() { return pageId; }
        public long getTimestamp() { return timestamp; }
    }
    
    // è½¬åŒ–çŠ¶æ€ç±»
    public static class ConversionState {
        private String userId;
        private boolean viewedPage = false;
        private boolean addedToCart = false;
        private boolean purchased = false;
        private long lastEventTime;
        
        public boolean isConverted() {
            return viewedPage && addedToCart && purchased;
        }
        
        // Getters and Setters
        public String getUserId() { return userId; }
        public void setUserId(String userId) { this.userId = userId; }
        public boolean isViewedPage() { return viewedPage; }
        public void setViewedPage(boolean viewedPage) { this.viewedPage = viewedPage; }
        public boolean isAddedToCart() { return addedToCart; }
        public void setAddedToCart(boolean addedToCart) { this.addedToCart = addedToCart; }
        public boolean isPurchased() { return purchased; }
        public void setPurchased(boolean purchased) { this.purchased = purchased; }
        public long getLastEventTime() { return lastEventTime; }
        public void setLastEventTime(long lastEventTime) { this.lastEventTime = lastEventTime; }
    }
    
    // è‡ªå®šä¹‰Serde
    public static class ConversionStateSerde implements Serde<ConversionState> {
        @Override
        public Serializer<ConversionState> serializer() {
            return new ConversionStateSerializer();
        }
        
        @Override
        public Deserializer<ConversionState> deserializer() {
            return new ConversionStateDeserializer();
        }
    }
    
    public static class ConversionStateSerializer implements Serializer<ConversionState> {
        @Override
        public byte[] serialize(String topic, ConversionState data) {
            // ç®€åŒ–çš„åºåˆ—åŒ–å®ç°
            return data.toString().getBytes();
        }
    }
    
    public static class ConversionStateDeserializer implements Deserializer<ConversionState> {
        @Override
        public ConversionState deserialize(String topic, byte[] data) {
            // ç®€åŒ–çš„ååºåˆ—åŒ–å®ç°
            return new ConversionState();
        }
    }
}
```

</TabItem>
<TabItem value="windowing" label="çª—å£æ“ä½œ">

```java title="Kafka Streamsçª—å£æ“ä½œè¯¦è§£"
import org.apache.kafka.streams.kstream.*;
import java.time.Duration;

/**
 * Kafka Streamsçª—å£æ“ä½œç¤ºä¾‹
 * å±•ç¤ºä¸åŒç±»å‹çš„æ—¶é—´çª—å£å’Œçª—å£æ“ä½œ
 */
public class WindowingExample {
    
    /**
     * æ—¶é—´çª—å£ç±»å‹æ¼”ç¤º
     */
    public static void demonstrateWindowTypes(StreamsBuilder builder) {
        KStream<String, String> events = builder.stream("events");
        
        // 1. æ»šåŠ¨çª—å£ (Tumbling Window)
        // å›ºå®šå¤§å°ï¼Œä¸é‡å çš„æ—¶é—´çª—å£
        KTable<Windowed<String>, Long> tumblingWindowCount = events
            .selectKey((key, value) -> extractUserId(value))
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5))) // 5åˆ†é’Ÿæ»šåŠ¨çª—å£
            .count(Materialized.as("tumbling-window-store"));
        
        // 2. è·³è·ƒçª—å£ (Hopping Window)
        // å›ºå®šå¤§å°ï¼Œæœ‰é‡å çš„æ—¶é—´çª—å£
        KTable<Windowed<String>, Long> hoppingWindowCount = events
            .selectKey((key, value) -> extractUserId(value))
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5))
                       .advanceBy(Duration.ofMinutes(1))) // 5åˆ†é’Ÿçª—å£ï¼Œæ¯1åˆ†é’Ÿæ»‘åŠ¨
            .count(Materialized.as("hopping-window-store"));
        
        // 3. ä¼šè¯çª—å£ (Session Window)
        // åŸºäºæ´»åŠ¨çš„åŠ¨æ€çª—å£
        KTable<Windowed<String>, Long> sessionWindowCount = events
            .selectKey((key, value) -> extractUserId(value))
            .groupByKey()
            .windowedBy(SessionWindows.with(Duration.ofMinutes(30))) // 30åˆ†é’Ÿä¸æ´»è·ƒåˆ™å…³é—­ä¼šè¯
            .count(Materialized.as("session-window-store"));
        
        // 4. æ»‘åŠ¨çª—å£èšåˆç¤ºä¾‹
        KTable<Windowed<String>, Double> slidingAverage = events
            .selectKey((key, value) -> extractUserId(value))
            .mapValues(value -> extractValue(value))
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(10)))
            .aggregate(
                () -> new AggregateValue(0.0, 0),
                (key, value, aggregate) -> new AggregateValue(
                    aggregate.getSum() + value,
                    aggregate.getCount() + 1
                ),
                Materialized.<String, AggregateValue, WindowStore<Bytes, byte[]>>as("sliding-avg-store")
                    .withValueSerde(new AggregateValueSerde())
            )
            .mapValues(aggregate -> aggregate.getCount() > 0 ? 
                      aggregate.getSum() / aggregate.getCount() : 0.0);
        
        // è¾“å‡ºçª—å£ç»“æœ
        tumblingWindowCount.toStream()
            .map((windowedKey, count) -> KeyValue.pair(
                windowedKey.key() + "@" + windowedKey.window().start() + "-" + windowedKey.window().end(),
                count
            ))
            .to("tumbling-window-output", Produced.with(Serdes.String(), Serdes.Long()));
        
        hoppingWindowCount.toStream()
            .map((windowedKey, count) -> KeyValue.pair(
                windowedKey.key() + "@" + windowedKey.window().start() + "-" + windowedKey.window().end(),
                count
            ))
            .to("hopping-window-output", Produced.with(Serdes.String(), Serdes.Long()));
        
        sessionWindowCount.toStream()
            .map((windowedKey, count) -> KeyValue.pair(
                windowedKey.key() + "@" + windowedKey.window().start() + "-" + windowedKey.window().end(),
                count
            ))
            .to("session-window-output", Produced.with(Serdes.String(), Serdes.Long()));
    }
    
    /**
     * çª—å£æŠ‘åˆ¶å’Œè§¦å‘ç­–ç•¥
     */
    public static void demonstrateWindowSuppression(StreamsBuilder builder) {
        KStream<String, String> events = builder.stream("events");
        
        // 1. çª—å£å…³é—­æ—¶è§¦å‘ (é»˜è®¤è¡Œä¸º)
        KTable<Windowed<String>, Long> windowClosedTrigger = events
            .selectKey((key, value) -> extractUserId(value))
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count()
            .suppress(Suppressed.untilWindowCloses(Suppressed.BufferConfig.unbounded()));
        
        // 2. æ—¶é—´é—´éš”è§¦å‘
        KTable<Windowed<String>, Long> timeIntervalTrigger = events
            .selectKey((key, value) -> extractUserId(value))
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count()
            .suppress(Suppressed.untilTimeLimit(Duration.ofSeconds(30), 
                     Suppressed.BufferConfig.maxRecords(1000)));
        
        // 3. è®°å½•æ•°é‡è§¦å‘
        KTable<Windowed<String>, Long> recordCountTrigger = events
            .selectKey((key, value) -> extractUserId(value))
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .count()
            .suppress(Suppressed.untilTimeLimit(Duration.ofMinutes(1),
                     Suppressed.BufferConfig.maxRecords(100)));
        
        // è¾“å‡ºç»“æœ
        windowClosedTrigger.toStream().to("window-closed-output");
        timeIntervalTrigger.toStream().to("time-interval-output");
        recordCountTrigger.toStream().to("record-count-output");
    }
    
    /**
     * å¤æ‚çª—å£èšåˆ - å®æ—¶æŒ‡æ ‡è®¡ç®—
     */
    public static void realTimeMetricsCalculation(StreamsBuilder builder) {
        KStream<String, String> metrics = builder.stream("metrics");
        
        // è§£ææŒ‡æ ‡æ•°æ®
        KStream<String, MetricData> parsedMetrics = metrics
            .mapValues(json -> parseMetricData(json))
            .filter((key, metric) -> metric != null);
        
        // 1. å®æ—¶QPSè®¡ç®— (æ¯åˆ†é’Ÿè¯·æ±‚æ•°)
        KTable<Windowed<String>, Long> qpsMetrics = parsedMetrics
            .filter((key, metric) -> "request".equals(metric.getType()))
            .selectKey((key, metric) -> metric.getServiceName())
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
            .count(Materialized.as("qps-store"));
        
        // 2. å®æ—¶å“åº”æ—¶é—´ç»Ÿè®¡
        KTable<Windowed<String>, ResponseTimeStats> responseTimeStats = parsedMetrics
            .filter((key, metric) -> "response_time".equals(metric.getType()))
            .selectKey((key, metric) -> metric.getServiceName())
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .aggregate(
                ResponseTimeStats::new,
                (key, metric, stats) -> stats.update(metric.getValue()),
                Materialized.<String, ResponseTimeStats, WindowStore<Bytes, byte[]>>as("response-time-store")
                    .withValueSerde(new ResponseTimeStatsSerde())
            );
        
        // 3. é”™è¯¯ç‡è®¡ç®—
        KTable<Windowed<String>, Double> errorRateMetrics = parsedMetrics
            .filter((key, metric) -> "request".equals(metric.getType()))
            .selectKey((key, metric) -> metric.getServiceName())
            .groupByKey()
            .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
            .aggregate(
                () -> new ErrorRateData(0, 0),
                (key, metric, data) -> {
                    data.incrementTotal();
                    if (metric.getValue() >= 400) { // HTTPé”™è¯¯çŠ¶æ€ç 
                        data.incrementError();
                    }
                    return data;
                },
                Materialized.<String, ErrorRateData, WindowStore<Bytes, byte[]>>as("error-rate-store")
                    .withValueSerde(new ErrorRateDataSerde())
            )
            .mapValues(data -> data.getTotal() > 0 ? 
                      (double) data.getError() / data.getTotal() * 100 : 0.0);
        
        // è¾“å‡ºå®æ—¶æŒ‡æ ‡
        qpsMetrics.toStream()
            .map((windowedKey, qps) -> KeyValue.pair(
                "QPS:" + windowedKey.key() + "@" + windowedKey.window().start(),
                qps
            ))
            .to("realtime-qps", Produced.with(Serdes.String(), Serdes.Long()));
        
        responseTimeStats.toStream()
            .map((windowedKey, stats) -> KeyValue.pair(
                "RT:" + windowedKey.key() + "@" + windowedKey.window().start(),
                String.format("avg:%.2f,p95:%.2f,p99:%.2f", 
                             stats.getAverage(), stats.getP95(), stats.getP99())
            ))
            .to("realtime-response-time", Produced.with(Serdes.String(), Serdes.String()));
        
        errorRateMetrics.toStream()
            .map((windowedKey, errorRate) -> KeyValue.pair(
                "ErrorRate:" + windowedKey.key() + "@" + windowedKey.window().start(),
                errorRate
            ))
            .to("realtime-error-rate", Produced.with(Serdes.String(), Serdes.Double()));
    }
    
    // è¾…åŠ©æ–¹æ³•
    private static String extractUserId(String value) {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è§£æJSON
        return "user1";
    }
    
    private static Double extractValue(String value) {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”è§£æJSON
        return 1.0;
    }
    
    private static MetricData parseMetricData(String json) {
        // ç®€åŒ–å®ç°ï¼Œå®é™…åº”ä½¿ç”¨JSONåº“
        return new MetricData("request", "user-service", 200.0, System.currentTimeMillis());
    }
    
    // æ•°æ®ç±»å®šä¹‰
    public static class AggregateValue {
        private double sum;
        private int count;
        
        public AggregateValue(double sum, int count) {
            this.sum = sum;
            this.count = count;
        }
        
        public double getSum() { return sum; }
        public int getCount() { return count; }
    }
    
    public static class MetricData {
        private String type;
        private String serviceName;
        private double value;
        private long timestamp;
        
        public MetricData(String type, String serviceName, double value, long timestamp) {
            this.type = type;
            this.serviceName = serviceName;
            this.value = value;
            this.timestamp = timestamp;
        }
        
        public String getType() { return type; }
        public String getServiceName() { return serviceName; }
        public double getValue() { return value; }
        public long getTimestamp() { return timestamp; }
    }
    
    public static class ResponseTimeStats {
        private List<Double> values = new ArrayList<>();
        
        public ResponseTimeStats update(double value) {
            values.add(value);
            return this;
        }
        
        public double getAverage() {
            return values.stream().mapToDouble(Double::doubleValue).average().orElse(0.0);
        }
        
        public double getP95() {
            if (values.isEmpty()) return 0.0;
            List<Double> sorted = new ArrayList<>(values);
            Collections.sort(sorted);
            int index = (int) Math.ceil(0.95 * sorted.size()) - 1;
            return sorted.get(Math.max(0, index));
        }
        
        public double getP99() {
            if (values.isEmpty()) return 0.0;
            List<Double> sorted = new ArrayList<>(values);
            Collections.sort(sorted);
            int index = (int) Math.ceil(0.99 * sorted.size()) - 1;
            return sorted.get(Math.max(0, index));
        }
    }
    
    public static class ErrorRateData {
        private int total;
        private int error;
        
        public ErrorRateData(int total, int error) {
            this.total = total;
            this.error = error;
        }
        
        public void incrementTotal() { total++; }
        public void incrementError() { error++; }
        public int getTotal() { return total; }
        public int getError() { return error; }
    }
    
    // Serdeå®ç°ï¼ˆç®€åŒ–ï¼‰
    public static class AggregateValueSerde implements Serde<AggregateValue> {
        @Override
        public Serializer<AggregateValue> serializer() { return null; }
        @Override
        public Deserializer<AggregateValue> deserializer() { return null; }
    }
    
    public static class ResponseTimeStatsSerde implements Serde<ResponseTimeStats> {
        @Override
        public Serializer<ResponseTimeStats> serializer() { return null; }
        @Override
        public Deserializer<ResponseTimeStats> deserializer() { return null; }
    }
    
    public static class ErrorRateDataSerde implements Serde<ErrorRateData> {
        @Override
        public Serializer<ErrorRateData> serializer() { return null; }
        @Override
        public Deserializer<ErrorRateData> deserializer() { return null; }
    }
}
```

</TabItem>
</Tabs>

### 4.2 çŠ¶æ€å­˜å‚¨ä¸å®¹é”™æœºåˆ¶

Kafka Streamsæä¾›äº†å¼ºå¤§çš„çŠ¶æ€ç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒæœ¬åœ°çŠ¶æ€å­˜å‚¨å’Œåˆ†å¸ƒå¼å®¹é”™æœºåˆ¶ã€‚

<Tabs>
<TabItem value="state-stores" label="çŠ¶æ€å­˜å‚¨">

```java title="Kafka StreamsçŠ¶æ€å­˜å‚¨è¯¦è§£"
import org.apache.kafka.streams.processor.api.*;
import org.apache.kafka.streams.state.*;
import org.apache.kafka.common.serialization.Serdes;

/**
 * Kafka StreamsçŠ¶æ€å­˜å‚¨ç¤ºä¾‹
 * å±•ç¤ºä¸åŒç±»å‹çš„çŠ¶æ€å­˜å‚¨å’Œè‡ªå®šä¹‰å¤„ç†å™¨
 */
public class StateStoreExample {
    
    /**
     * è‡ªå®šä¹‰å¤„ç†å™¨ä½¿ç”¨çŠ¶æ€å­˜å‚¨
     */
    public static class UserSessionProcessor implements Processor<String, String, String, String> {
        
        private ProcessorContext<String, String> context;
        private KeyValueStore<String, UserSession> sessionStore;
        
        @Override
        public void init(ProcessorContext<String, String> context) {
            this.context = context;
            this.sessionStore = context.getStateStore("user-session-store");
            
            // å®šæœŸæ¸…ç†è¿‡æœŸä¼šè¯
            context.schedule(Duration.ofMinutes(1), PunctuationType.WALL_CLOCK_TIME, this::cleanupExpiredSessions);
        }
        
        @Override
        public void process(Record<String, String> record) {
            String userId = record.key();
            String eventData = record.value();
            
            // è·å–æˆ–åˆ›å»ºç”¨æˆ·ä¼šè¯
            UserSession session = sessionStore.get(userId);
            if (session == null) {
                session = new UserSession(userId, System.currentTimeMillis());
            }
            
            // æ›´æ–°ä¼šè¯ä¿¡æ¯
            session.updateActivity(eventData, System.currentTimeMillis());
            
            // ä¿å­˜ä¼šè¯çŠ¶æ€
            sessionStore.put(userId, session);
            
            // è¾“å‡ºä¼šè¯æ›´æ–°äº‹ä»¶
            context.forward(record.withValue("ä¼šè¯æ›´æ–°: " + session.toString()));
        }
        
        /**
         * æ¸…ç†è¿‡æœŸä¼šè¯
         */
        private void cleanupExpiredSessions(long timestamp) {
            long expireTime = timestamp - Duration.ofHours(1).toMillis(); // 1å°æ—¶è¿‡æœŸ
            
            try (KeyValueIterator<String, UserSession> iterator = sessionStore.all()) {
                while (iterator.hasNext()) {
                    KeyValue<String, UserSession> entry = iterator.next();
                    if (entry.value.getLastActivityTime() < expireTime) {
                        sessionStore.delete(entry.key);
                        System.out.println("æ¸…ç†è¿‡æœŸä¼šè¯: " + entry.key);
                    }
                }
            }
        }
        
        @Override
        public void close() {
            // æ¸…ç†èµ„æº
        }
    }
    
    /**
     * æ„å»ºä½¿ç”¨çŠ¶æ€å­˜å‚¨çš„æ‹“æ‰‘
     */
    public static Topology buildTopologyWithStateStore() {
        StreamsBuilder builder = new StreamsBuilder();
        
        // 1. åˆ›å»ºçŠ¶æ€å­˜å‚¨
        StoreBuilder<KeyValueStore<String, UserSession>> sessionStoreBuilder = 
            Stores.keyValueStoreBuilder(
                Stores.persistentKeyValueStore("user-session-store"),
                Serdes.String(),
                new UserSessionSerde()
            )
            .withLoggingEnabled(Collections.singletonMap("cleanup.policy", "compact"))
            .withCachingEnabled();
        
        // 2. æ·»åŠ çŠ¶æ€å­˜å‚¨åˆ°æ‹“æ‰‘
        Topology topology = builder.build();
        topology.addStateStore(sessionStoreBuilder);
        
        // 3. æ·»åŠ å¤„ç†å™¨å¹¶è¿æ¥çŠ¶æ€å­˜å‚¨
        topology.addSource("source", "user-events")
                .addProcessor("session-processor", UserSessionProcessor::new, "source")
                .connectProcessorAndStateStores("session-processor", "user-session-store")
                .addSink("sink", "user-sessions-output", "session-processor");
        
        return topology;
    }
    
    /**
     * çª—å£çŠ¶æ€å­˜å‚¨ç¤ºä¾‹
     */
    public static void windowStateStoreExample(StreamsBuilder builder) {
        // åˆ›å»ºçª—å£çŠ¶æ€å­˜å‚¨
        StoreBuilder<WindowStore<String, Long>> windowStoreBuilder = 
            Stores.windowStoreBuilder(
                Stores.persistentWindowStore("page-view-window-store", 
                                            Duration.ofDays(1), // ä¿ç•™1å¤©
                                            Duration.ofMinutes(5), // 5åˆ†é’Ÿçª—å£
                                            false), // ä¸ä¿ç•™é‡å¤
                Serdes.String(),
                Serdes.Long()
            )
            .withLoggingEnabled(Collections.emptyMap())
            .withCachingEnabled();
        
        // ä½¿ç”¨çª—å£çŠ¶æ€å­˜å‚¨çš„å¤„ç†å™¨
        class PageViewWindowProcessor implements Processor<String, String, String, Long> {
            private ProcessorContext<String, Long> context;
            private WindowStore<String, Long> windowStore;
            
            @Override
            public void init(ProcessorContext<String, Long> context) {
                this.context = context;
                this.windowStore = context.getStateStore("page-view-window-store");
            }
            
            @Override
            public void process(Record<String, String> record) {
                String pageId = record.key();
                long timestamp = record.timestamp();
                
                // è·å–å½“å‰çª—å£çš„è®¿é—®é‡
                Long currentCount = windowStore.fetch(pageId, timestamp);
                if (currentCount == null) {
                    currentCount = 0L;
                }
                
                // æ›´æ–°è®¿é—®é‡
                windowStore.put(pageId, currentCount + 1, timestamp);
                
                // è¾“å‡ºæ›´æ–°åçš„è®¿é—®é‡
                context.forward(record.withKey(pageId).withValue(currentCount + 1));
            }
        }
        
        // æ·»åŠ åˆ°æ‹“æ‰‘
        Topology topology = builder.build();
        topology.addStateStore(windowStoreBuilder)
                .addSource("page-view-source", "page-views")
                .addProcessor("page-view-window-processor", PageViewWindowProcessor::new, "page-view-source")
                .connectProcessorAndStateStores("page-view-window-processor", "page-view-window-store")
                .addSink("page-view-sink", "page-view-counts", "page-view-window-processor");
    }
    
    /**
     * ä¼šè¯çŠ¶æ€å­˜å‚¨ç¤ºä¾‹
     */
    public static void sessionStateStoreExample(StreamsBuilder builder) {
        // åˆ›å»ºä¼šè¯çŠ¶æ€å­˜å‚¨
        StoreBuilder<SessionStore<String, String>> sessionStoreBuilder = 
            Stores.sessionStoreBuilder(
                Stores.persistentSessionStore("user-session-events-store", 
                                             Duration.ofMinutes(30)), // 30åˆ†é’Ÿä¼šè¯è¶…æ—¶
                Serdes.String(),
                Serdes.String()
            )
            .withLoggingEnabled(Collections.emptyMap());
        
        // ä½¿ç”¨ä¼šè¯çŠ¶æ€å­˜å‚¨çš„å¤„ç†å™¨
        class UserSessionEventProcessor implements Processor<String, String, String, String> {
            private ProcessorContext<String, String> context;
            private SessionStore<String, String> sessionStore;
            
            @Override
            public void init(ProcessorContext<String, String> context) {
                this.context = context;
                this.sessionStore = context.getStateStore("user-session-events-store");
            }
            
            @Override
            public void process(Record<String, String> record) {
                String userId = record.key();
                String event = record.value();
                long timestamp = record.timestamp();
                
                // æŸ¥æ‰¾æ´»è·ƒä¼šè¯
                try (KeyValueIterator<Windowed<String>, String> sessions = 
                     sessionStore.findSessions(userId, timestamp - Duration.ofMinutes(30).toMillis(), timestamp)) {
                    
                    if (sessions.hasNext()) {
                        // æ›´æ–°ç°æœ‰ä¼šè¯
                        KeyValue<Windowed<String>, String> session = sessions.next();
                        String updatedEvents = session.value + "," + event;
                        sessionStore.put(new Windowed<>(userId, new SessionWindow(session.key.window().start(), timestamp)), 
                                       updatedEvents);
                    } else {
                        // åˆ›å»ºæ–°ä¼šè¯
                        sessionStore.put(new Windowed<>(userId, new SessionWindow(timestamp, timestamp)), event);
                    }
                }
                
                context.forward(record.withValue("ä¼šè¯äº‹ä»¶å·²è®°å½•: " + event));
            }
        }
        
        // æ·»åŠ åˆ°æ‹“æ‰‘
        Topology topology = builder.build();
        topology.addStateStore(sessionStoreBuilder)
                .addSource("session-event-source", "user-session-events")
                .addProcessor("session-event-processor", UserSessionEventProcessor::new, "session-event-source")
                .connectProcessorAndStateStores("session-event-processor", "user-session-events-store")
                .addSink("session-event-sink", "processed-session-events", "session-event-processor");
    }
    
    /**
     * çŠ¶æ€å­˜å‚¨æŸ¥è¯¢ç¤ºä¾‹
     */
    public static class StateStoreQueryService {
        private final KafkaStreams streams;
        
        public StateStoreQueryService(KafkaStreams streams) {
            this.streams = streams;
        }
        
        /**
         * æŸ¥è¯¢é”®å€¼çŠ¶æ€å­˜å‚¨
         */
        public UserSession getUserSession(String userId) {
            ReadOnlyKeyValueStore<String, UserSession> store = 
                streams.store(StoreQueryParameters.fromNameAndType("user-session-store", QueryableStoreTypes.keyValueStore()));
            
            return store.get(userId);
        }
        
        /**
         * æŸ¥è¯¢æ‰€æœ‰ç”¨æˆ·ä¼šè¯
         */
        public Map<String, UserSession> getAllUserSessions() {
            ReadOnlyKeyValueStore<String, UserSession> store = 
                streams.store(StoreQueryParameters.fromNameAndType("user-session-store", QueryableStoreTypes.keyValueStore()));
            
            Map<String, UserSession> sessions = new HashMap<>();
            try (KeyValueIterator<String, UserSession> iterator = store.all()) {
                while (iterator.hasNext()) {
                    KeyValue<String, UserSession> entry = iterator.next();
                    sessions.put(entry.key, entry.value);
                }
            }
            return sessions;
        }
        
        /**
         * æŸ¥è¯¢çª—å£çŠ¶æ€å­˜å‚¨
         */
        public Long getPageViewCount(String pageId, long timestamp) {
            ReadOnlyWindowStore<String, Long> store = 
                streams.store(StoreQueryParameters.fromNameAndType("page-view-window-store", QueryableStoreTypes.windowStore()));
            
            return store.fetch(pageId, timestamp);
        }
        
        /**
         * æŸ¥è¯¢æ—¶é—´èŒƒå›´å†…çš„é¡µé¢è®¿é—®é‡
         */
        public Map<Long, Long> getPageViewCounts(String pageId, long startTime, long endTime) {
            ReadOnlyWindowStore<String, Long> store = 
                streams.store(StoreQueryParameters.fromNameAndType("page-view-window-store", QueryableStoreTypes.windowStore()));
            
            Map<Long, Long> counts = new HashMap<>();
            try (WindowStoreIterator<Long> iterator = store.fetch(pageId, startTime, endTime)) {
                while (iterator.hasNext()) {
                    KeyValue<Long, Long> entry = iterator.next();
                    counts.put(entry.key, entry.value);
                }
            }
            return counts;
        }
    }
    
    // ç”¨æˆ·ä¼šè¯æ•°æ®ç±»
    public static class UserSession {
        private String userId;
        private long startTime;
        private long lastActivityTime;
        private int eventCount;
        private List<String> events;
        
        public UserSession(String userId, long startTime) {
            this.userId = userId;
            this.startTime = startTime;
            this.lastActivityTime = startTime;
            this.eventCount = 0;
            this.events = new ArrayList<>();
        }
        
        public void updateActivity(String event, long timestamp) {
            this.lastActivityTime = timestamp;
            this.eventCount++;
            this.events.add(event);
            
            // é™åˆ¶äº‹ä»¶åˆ—è¡¨å¤§å°
            if (events.size() > 100) {
                events.remove(0);
            }
        }
        
        @Override
        public String toString() {
            return String.format("UserSession{userId='%s', startTime=%d, lastActivityTime=%d, eventCount=%d}",
                               userId, startTime, lastActivityTime, eventCount);
        }
        
        // Getters
        public String getUserId() { return userId; }
        public long getStartTime() { return startTime; }
        public long getLastActivityTime() { return lastActivityTime; }
        public int getEventCount() { return eventCount; }
        public List<String> getEvents() { return events; }
    }
    
    // ç”¨æˆ·ä¼šè¯åºåˆ—åŒ–å™¨
    public static class UserSessionSerde implements Serde<UserSession> {
        @Override
        public Serializer<UserSession> serializer() {
            return new UserSessionSerializer();
        }
        
        @Override
        public Deserializer<UserSession> deserializer() {
            return new UserSessionDeserializer();
        }
    }
    
    public static class UserSessionSerializer implements Serializer<UserSession> {
        @Override
        public byte[] serialize(String topic, UserSession data) {
            // ç®€åŒ–å®ç°ï¼Œå®é™…åº”ä½¿ç”¨æ›´é«˜æ•ˆçš„åºåˆ—åŒ–æ–¹å¼
            return data.toString().getBytes();
        }
    }
    
    public static class UserSessionDeserializer implements Deserializer<UserSession> {
        @Override
        public UserSession deserialize(String topic, byte[] data) {
            // ç®€åŒ–å®ç°ï¼Œå®é™…åº”å®ç°å®Œæ•´çš„ååºåˆ—åŒ–é€»è¾‘
            return new UserSession("default", System.currentTimeMillis());
        }
    }
}
```

</TabItem>
<TabItem value="fault-tolerance" label="å®¹é”™æœºåˆ¶">

```java title="Kafka Streamså®¹é”™å’Œæ¢å¤æœºåˆ¶"
import org.apache.kafka.streams.*;
import org.apache.kafka.streams.errors.*;

/**
 * Kafka Streamså®¹é”™æœºåˆ¶ç¤ºä¾‹
 * å±•ç¤ºå¼‚å¸¸å¤„ç†ã€çŠ¶æ€æ¢å¤ã€é‡è¯•ç­–ç•¥ç­‰å®¹é”™ç‰¹æ€§
 */
public class FaultToleranceExample {
    
    /**
     * é…ç½®å®¹é”™ç›¸å…³å‚æ•°
     */
    public static Properties createFaultTolerantConfig() {
        Properties props = new Properties();
        
        // åŸºç¡€é…ç½®
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, "fault-tolerant-app");
        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
        
        // å®¹é”™é…ç½®
        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2);              // 2ä¸ªæµå¤„ç†çº¿ç¨‹
        props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);              // çŠ¶æ€å­˜å‚¨å‰¯æœ¬å› å­
        props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);            // 1ä¸ªå¤‡ç”¨å‰¯æœ¬
        
        // çŠ¶æ€å­˜å‚¨é…ç½®
        props.put(StreamsConfig.STATE_DIR_CONFIG, "/tmp/kafka-streams");    // çŠ¶æ€å­˜å‚¨ç›®å½•
        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);           // 1ç§’æäº¤ä¸€æ¬¡
        
        // é‡è¯•é…ç½®
        props.put(StreamsConfig.RETRY_BACKOFF_MS_CONFIG, 1000);             // é‡è¯•é—´éš”1ç§’
        props.put(StreamsConfig.RECONNECT_BACKOFF_MS_CONFIG, 1000);         // é‡è¿é—´éš”1ç§’
        props.put(StreamsConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG, 10000);    // æœ€å¤§é‡è¿é—´éš”10ç§’
        
        // ç¼“å­˜é…ç½®
        props.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 10 * 1024 * 1024); // 10MBç¼“å­˜
        
        return props;
    }
    
    /**
     * åˆ›å»ºå…·æœ‰å¼‚å¸¸å¤„ç†çš„Streamsåº”ç”¨
     */
    public static KafkaStreams createFaultTolerantStreamsApp() {
        StreamsBuilder builder = new StreamsBuilder();
        
        // æ„å»ºå¤„ç†æ‹“æ‰‘
        KStream<String, String> input = builder.stream("input-topic");
        
        KStream<String, String> processed = input
            .mapValues(value -> {
                try {
                    // æ¨¡æ‹Ÿå¯èƒ½å¤±è´¥çš„å¤„ç†é€»è¾‘
                    if (value.contains("error")) {
                        throw new RuntimeException("å¤„ç†å¤±è´¥: " + value);
                    }
                    return "processed: " + value.toUpperCase();
                } catch (Exception e) {
                    // è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­æµå¤„ç†
                    System.err.println("å¤„ç†æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯: " + e.getMessage());
                    return "error: " + value;
                }
            })
            .filter((key, value) -> !value.startsWith("error"));
        
        processed.to("output-topic");
        
        // åˆ›å»ºStreamså®ä¾‹
        Properties props = createFaultTolerantConfig();
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        
        // è®¾ç½®å¼‚å¸¸å¤„ç†å™¨
        streams.setUncaughtExceptionHandler(new CustomUncaughtExceptionHandler());
        
        // è®¾ç½®çŠ¶æ€ç›‘å¬å™¨
        streams.setStateListener(new CustomStateListener());
        
        // è®¾ç½®å…¨å±€å¼‚å¸¸å¤„ç†å™¨
        streams.setGlobalStateRestoreListener(new CustomGlobalStateRestoreListener());
        
        return streams;
    }
    
    /**
     * è‡ªå®šä¹‰æœªæ•è·å¼‚å¸¸å¤„ç†å™¨
     */
    public static class CustomUncaughtExceptionHandler implements StreamsUncaughtExceptionHandler {
        
        @Override
        public StreamThreadExceptionResponse handle(Throwable exception) {
            System.err.println("æµå¤„ç†çº¿ç¨‹å‘ç”Ÿæœªæ•è·å¼‚å¸¸: " + exception.getMessage());
            exception.printStackTrace();
            
            // æ ¹æ®å¼‚å¸¸ç±»å‹å†³å®šå¤„ç†ç­–ç•¥
            if (exception instanceof StreamsException) {
                StreamsException streamsException = (StreamsException) exception;
                
                if (streamsException.getCause() instanceof org.apache.kafka.common.errors.SerializationException) {
                    // åºåˆ—åŒ–å¼‚å¸¸ï¼Œè·³è¿‡æœ‰é—®é¢˜çš„è®°å½•
                    System.err.println("åºåˆ—åŒ–å¼‚å¸¸ï¼Œè·³è¿‡è®°å½•å¹¶ç»§ç»­å¤„ç†");
                    return StreamThreadExceptionResponse.REPLACE_THREAD;
                }
                
                if (streamsException.getCause() instanceof org.apache.kafka.common.errors.TimeoutException) {
                    // è¶…æ—¶å¼‚å¸¸ï¼Œæ›¿æ¢çº¿ç¨‹
                    System.err.println("è¶…æ—¶å¼‚å¸¸ï¼Œæ›¿æ¢å¤„ç†çº¿ç¨‹");
                    return StreamThreadExceptionResponse.REPLACE_THREAD;
                }
            }
            
            // å…¶ä»–å¼‚å¸¸ï¼Œå…³é—­å®¢æˆ·ç«¯
            System.err.println("ä¸¥é‡å¼‚å¸¸ï¼Œå…³é—­Streamså®¢æˆ·ç«¯");
            return StreamThreadExceptionResponse.SHUTDOWN_CLIENT;
        }
    }
    
    /**
     * è‡ªå®šä¹‰çŠ¶æ€ç›‘å¬å™¨
     */
    public static class CustomStateListener implements KafkaStreams.StateListener {
        
        @Override
        public void onChange(KafkaStreams.State newState, KafkaStreams.State oldState) {
            System.out.println("StreamsçŠ¶æ€å˜æ›´: " + oldState + " -> " + newState);
            
            switch (newState) {
                case RUNNING:
                    System.out.println("Streamsåº”ç”¨æ­£åœ¨è¿è¡Œ");
                    break;
                case REBALANCING:
                    System.out.println("Streamsåº”ç”¨æ­£åœ¨é‡å¹³è¡¡");
                    break;
                case ERROR:
                    System.err.println("Streamsåº”ç”¨è¿›å…¥é”™è¯¯çŠ¶æ€");
                    // å¯ä»¥åœ¨è¿™é‡Œå®ç°å‘Šè­¦é€»è¾‘
                    break;
                case NOT_RUNNING:
                    System.out.println("Streamsåº”ç”¨å·²åœæ­¢");
                    break;
                case PENDING_SHUTDOWN:
                    System.out.println("Streamsåº”ç”¨æ­£åœ¨å…³é—­");
                    break;
            }
        }
    }
    
    /**
     * è‡ªå®šä¹‰å…¨å±€çŠ¶æ€æ¢å¤ç›‘å¬å™¨
     */
    public static class CustomGlobalStateRestoreListener implements StateRestoreListener {
        
        @Override
        public void onRestoreStart(TopicPartition topicPartition, String storeName, long startingOffset, long endingOffset) {
            System.out.printf("å¼€å§‹æ¢å¤çŠ¶æ€å­˜å‚¨: %s, åˆ†åŒº: %s, èµ·å§‹ä½ç§»: %d, ç»“æŸä½ç§»: %d%n",
                             storeName, topicPartition, startingOffset, endingOffset);
        }
        
        @Override
        public void onBatchRestored(TopicPartition topicPartition, String storeName, long batchEndOffset, long numRestored) {
            System.out.printf("æ‰¹é‡æ¢å¤å®Œæˆ: %s, åˆ†åŒº: %s, æ‰¹æ¬¡ç»“æŸä½ç§»: %d, æ¢å¤è®°å½•æ•°: %d%n",
                             storeName, topicPartition, batchEndOffset, numRestored);
        }
        
        @Override
        public void onRestoreEnd(TopicPartition topicPartition, String storeName, long totalRestored) {
            System.out.printf("çŠ¶æ€æ¢å¤å®Œæˆ: %s, åˆ†åŒº: %s, æ€»æ¢å¤è®°å½•æ•°: %d%n",
                             storeName, topicPartition, totalRestored);
        }
    }
    
    /**
     * è‡ªå®šä¹‰ååºåˆ—åŒ–å¼‚å¸¸å¤„ç†å™¨
     */
    public static class CustomDeserializationExceptionHandler implements DeserializationExceptionHandler {
        
        @Override
        public DeserializationHandlerResponse handle(ProcessorContext context, ConsumerRecord<byte[], byte[]> record, Exception exception) {
            System.err.printf("ååºåˆ—åŒ–å¼‚å¸¸: topic=%s, partition=%d, offset=%d, å¼‚å¸¸=%s%n",
                             record.topic(), record.partition(), record.offset(), exception.getMessage());
            
            // è®°å½•æœ‰é—®é¢˜çš„æ¶ˆæ¯åˆ°æ­»ä¿¡é˜Ÿåˆ—
            logToDeadLetterQueue(record, exception);
            
            // è·³è¿‡æœ‰é—®é¢˜çš„è®°å½•ï¼Œç»§ç»­å¤„ç†
            return DeserializationHandlerResponse.CONTINUE;
        }
        
        private void logToDeadLetterQueue(ConsumerRecord<byte[], byte[]> record, Exception exception) {
            // å®ç°æ­»ä¿¡é˜Ÿåˆ—é€»è¾‘
            System.out.println("å°†æœ‰é—®é¢˜çš„æ¶ˆæ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: " + record.offset());
        }
        
        @Override
        public void configure(Map<String, ?> configs) {
            // é…ç½®åˆå§‹åŒ–
        }
    }
    
    /**
     * è‡ªå®šä¹‰ç”Ÿäº§å¼‚å¸¸å¤„ç†å™¨
     */
    public static class CustomProductionExceptionHandler implements ProductionExceptionHandler {
        
        @Override
        public ProductionExceptionHandlerResponse handle(ProducerRecord<byte[], byte[]> record, Exception exception) {
            System.err.printf("ç”Ÿäº§æ¶ˆæ¯å¼‚å¸¸: topic=%s, å¼‚å¸¸=%s%n", record.topic(), exception.getMessage());
            
            // æ ¹æ®å¼‚å¸¸ç±»å‹å†³å®šå¤„ç†ç­–ç•¥
            if (exception instanceof org.apache.kafka.common.errors.RetriableException) {
                // å¯é‡è¯•å¼‚å¸¸ï¼Œç»§ç»­é‡è¯•
                System.out.println("å¯é‡è¯•å¼‚å¸¸ï¼Œç»§ç»­é‡è¯•");
                return ProductionExceptionHandlerResponse.CONTINUE;
            } else {
                // ä¸å¯é‡è¯•å¼‚å¸¸ï¼Œå¤±è´¥å¤„ç†
                System.err.println("ä¸å¯é‡è¯•å¼‚å¸¸ï¼Œå¤±è´¥å¤„ç†");
                return ProductionExceptionHandlerResponse.FAIL;
            }
        }
        
        @Override
        public void configure(Map<String, ?> configs) {
            // é…ç½®åˆå§‹åŒ–
        }
    }
    
    /**
     * å¸¦æœ‰å®Œæ•´å®¹é”™æœºåˆ¶çš„Streamsåº”ç”¨
     */
    public static void runFaultTolerantApplication() {
        Properties props = createFaultTolerantConfig();
        
        // è®¾ç½®å¼‚å¸¸å¤„ç†å™¨
        props.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, 
                 CustomDeserializationExceptionHandler.class);
        props.put(StreamsConfig.DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, 
                 CustomProductionExceptionHandler.class);
        
        StreamsBuilder builder = new StreamsBuilder();
        
        // æ„å»ºå®¹é”™çš„å¤„ç†æ‹“æ‰‘
        KStream<String, String> input = builder.stream("fault-tolerant-input");
        
        KStream<String, String> processed = input
            .mapValues(value -> {
                try {
                    // æ¨¡æ‹Ÿä¸šåŠ¡å¤„ç†é€»è¾‘
                    return processBusinessLogic(value);
                } catch (Exception e) {
                    // ä¸šåŠ¡å¼‚å¸¸å¤„ç†
                    System.err.println("ä¸šåŠ¡å¤„ç†å¼‚å¸¸: " + e.getMessage());
                    return "FAILED:" + value;
                }
            })
            .filter((key, value) -> !value.startsWith("FAILED:"));
        
        processed.to("fault-tolerant-output");
        
        // åˆ›å»ºå¹¶é…ç½®Streamså®ä¾‹
        KafkaStreams streams = new KafkaStreams(builder.build(), props);
        
        // è®¾ç½®å„ç§ç›‘å¬å™¨å’Œå¼‚å¸¸å¤„ç†å™¨
        streams.setUncaughtExceptionHandler(new CustomUncaughtExceptionHandler());
        streams.setStateListener(new CustomStateListener());
        streams.setGlobalStateRestoreListener(new CustomGlobalStateRestoreListener());
        
        // å¯åŠ¨åº”ç”¨
        streams.start();
        
        // æ·»åŠ å…³é—­é’©å­
        Runtime.getRuntime().addShutdownHook(new Thread(() -> {
            System.out.println("æ­£åœ¨ä¼˜é›…å…³é—­Streamsåº”ç”¨...");
            streams.close(Duration.ofSeconds(10));
            System.out.println("Streamsåº”ç”¨å·²å…³é—­");
        }));
        
        // ç›‘æ§åº”ç”¨çŠ¶æ€
        monitorApplicationHealth(streams);
    }
    
    /**
     * æ¨¡æ‹Ÿä¸šåŠ¡å¤„ç†é€»è¾‘
     */
    private static String processBusinessLogic(String value) throws Exception {
        // æ¨¡æ‹Ÿå¯èƒ½å¤±è´¥çš„ä¸šåŠ¡é€»è¾‘
        if (value.contains("exception")) {
            throw new RuntimeException("ä¸šåŠ¡å¤„ç†å¤±è´¥");
        }
        
        // æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        Thread.sleep(10);
        
        return "PROCESSED:" + value.toUpperCase();
    }
    
    /**
     * ç›‘æ§åº”ç”¨å¥åº·çŠ¶æ€
     */
    private static void monitorApplicationHealth(KafkaStreams streams) {
        Timer timer = new Timer();
        timer.scheduleAtFixedRate(new TimerTask() {
            @Override
            public void run() {
                KafkaStreams.State state = streams.state();
                System.out.println("å½“å‰åº”ç”¨çŠ¶æ€: " + state);
                
                if (state == KafkaStreams.State.ERROR) {
                    System.err.println("åº”ç”¨å¤„äºé”™è¯¯çŠ¶æ€ï¼Œå¯èƒ½éœ€è¦é‡å¯");
                    // å¯ä»¥åœ¨è¿™é‡Œå®ç°è‡ªåŠ¨é‡å¯é€»è¾‘
                }
                
                // æ‰“å°çº¿ç¨‹ä¿¡æ¯
                StreamsMetadata metadata = streams.localThreadsMetadata().iterator().next();
                System.out.println("æ´»è·ƒä»»åŠ¡æ•°: " + metadata.activeTasks().size());
                System.out.println("å¤‡ç”¨ä»»åŠ¡æ•°: " + metadata.standbyTasks().size());
            }
        }, 0, 30000); // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
    }
    
    public static void main(String[] args) {
        runFaultTolerantApplication();
    }
}
```

</TabItem>
</Tabs>

## 5. Kafkaæ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

### 5.1 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

Kafkaæ€§èƒ½ä¼˜åŒ–æ˜¯ä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹ï¼Œéœ€è¦ä»ç¡¬ä»¶ã€æ“ä½œç³»ç»Ÿã€Kafkaé…ç½®ã€åº”ç”¨ç¨‹åºç­‰å¤šä¸ªå±‚é¢è¿›è¡Œç»¼åˆä¼˜åŒ–ã€‚

<Tabs>
<TabItem value="hardware-os" label="ç¡¬ä»¶ä¸æ“ä½œç³»ç»Ÿä¼˜åŒ–">

```bash title="ç¡¬ä»¶å’Œæ“ä½œç³»ç»Ÿä¼˜åŒ–é…ç½®"
# ==================== ç¡¬ä»¶é€‰æ‹©å»ºè®® ====================
# CPU: 
# - ç”Ÿäº§è€…å¯†é›†å‹ï¼šé«˜é¢‘ç‡CPUï¼Œ8-16æ ¸å¿ƒ
# - æ¶ˆè´¹è€…å¯†é›†å‹ï¼šå¤šæ ¸å¿ƒCPUï¼Œ16-32æ ¸å¿ƒ
# - æ··åˆè´Ÿè½½ï¼šå¹³è¡¡å‹CPUï¼Œ16-24æ ¸å¿ƒ

# å†…å­˜:
# - æœ€å°8GBï¼Œæ¨è32GBä»¥ä¸Š
# - ä¸ºé¡µç¼“å­˜é¢„ç•™è¶³å¤Ÿå†…å­˜ï¼šæ€»å†…å­˜çš„50-75%
# - JVMå †å†…å­˜ï¼š6-8GBï¼ˆä¸è¶…è¿‡32GBï¼‰

# å­˜å‚¨:
# - ä½¿ç”¨SSDå­˜å‚¨æé«˜I/Oæ€§èƒ½
# - å¤šç£ç›˜RAIDé…ç½®ï¼šRAID 10ï¼ˆæ€§èƒ½ï¼‰æˆ–RAID 6ï¼ˆå®¹é‡ï¼‰
# - åˆ†ç¦»æ—¥å¿—å’Œç´¢å¼•æ–‡ä»¶åˆ°ä¸åŒç£ç›˜

# ç½‘ç»œ:
# - ä¸‡å…†ç½‘å¡ï¼ˆ10Gbpsï¼‰
# - ä½å»¶è¿Ÿç½‘ç»œäº¤æ¢æœº
# - ä¼˜åŒ–ç½‘ç»œç¼“å†²åŒºå¤§å°

# ==================== æ“ä½œç³»ç»Ÿä¼˜åŒ– ====================

# 1. æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
# ä½¿ç”¨ext4æˆ–xfsæ–‡ä»¶ç³»ç»Ÿï¼Œæ¨èxfs
mkfs.xfs -f /dev/sdb1

# æŒ‚è½½é€‰é¡¹ä¼˜åŒ–
mount -t xfs -o noatime,nodiratime,nobarrier /dev/sdb1 /var/kafka-logs

# /etc/fstab é…ç½®
echo "/dev/sdb1 /var/kafka-logs xfs noatime,nodiratime,nobarrier 0 0" >> /etc/fstab

# 2. å†…æ ¸å‚æ•°ä¼˜åŒ–
cat >> /etc/sysctl.conf << EOF
# ç½‘ç»œä¼˜åŒ–
net.core.rmem_default = 262144
net.core.rmem_max = 16777216
net.core.wmem_default = 262144
net.core.wmem_max = 16777216
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_rmem = 4096 65536 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.ipv4.tcp_congestion_control = bbr

# æ–‡ä»¶ç³»ç»Ÿä¼˜åŒ–
vm.swappiness = 1
vm.dirty_ratio = 80
vm.dirty_background_ratio = 5
vm.dirty_expire_centisecs = 12000
vm.dirty_writeback_centisecs = 1500

# æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
fs.file-max = 2097152
fs.nr_open = 2097152

# è¿›ç¨‹é™åˆ¶
kernel.pid_max = 4194304
kernel.threads-max = 4194304
EOF

# åº”ç”¨å†…æ ¸å‚æ•°
sysctl -p

# 3. ç”¨æˆ·é™åˆ¶ä¼˜åŒ–
cat >> /etc/security/limits.conf << EOF
kafka soft nofile 1048576
kafka hard nofile 1048576
kafka soft nproc 1048576
kafka hard nproc 1048576
kafka soft memlock unlimited
kafka hard memlock unlimited
EOF

# 4. ç£ç›˜è°ƒåº¦å™¨ä¼˜åŒ–
# å¯¹äºSSDä½¿ç”¨noopæˆ–deadlineè°ƒåº¦å™¨
echo noop > /sys/block/sdb/queue/scheduler

# å¯¹äºæœºæ¢°ç¡¬ç›˜ä½¿ç”¨cfqè°ƒåº¦å™¨
echo cfq > /sys/block/sda/queue/scheduler

# 5. CPUé¢‘ç‡è°ƒèŠ‚å™¨
# è®¾ç½®ä¸ºperformanceæ¨¡å¼
cpupower frequency-set -g performance

# 6. é€æ˜å¤§é¡µä¼˜åŒ–
# ç¦ç”¨é€æ˜å¤§é¡µï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag

# 7. NUMAä¼˜åŒ–
# æŸ¥çœ‹NUMAæ‹“æ‰‘
numactl --hardware

# ç»‘å®šKafkaè¿›ç¨‹åˆ°ç‰¹å®šNUMAèŠ‚ç‚¹
numactl --cpunodebind=0 --membind=0 /opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties

# ==================== JVMä¼˜åŒ– ====================

# Kafka JVMå¯åŠ¨å‚æ•°ä¼˜åŒ–
cat > /opt/kafka/bin/kafka-server-start-optimized.sh << 'EOF'
#!/bin/bash

# JVMå †å†…å­˜è®¾ç½®ï¼ˆæ ¹æ®å®é™…å†…å­˜è°ƒæ•´ï¼‰
export KAFKA_HEAP_OPTS="-Xmx6g -Xms6g"

# GCä¼˜åŒ–é…ç½®
export KAFKA_JVM_PERFORMANCE_OPTS="-server \
-XX:+UseG1GC \
-XX:MaxGCPauseMillis=20 \
-XX:InitiatingHeapOccupancyPercent=35 \
-XX:+ExplicitGCInvokesConcurrent \
-XX:MaxInlineLevel=15 \
-Djava.awt.headless=true"

# JVMç›‘æ§å’Œè°ƒè¯•å‚æ•°
export KAFKA_JVM_PERFORMANCE_OPTS="$KAFKA_JVM_PERFORMANCE_OPTS \
-XX:+UnlockCommercialFeatures \
-XX:+FlightRecorder \
-XX:+UnlockDiagnosticVMOptions \
-XX:+DebugNonSafepoints \
-XX:+PrintGC \
-XX:+PrintGCDetails \
-XX:+PrintGCTimeStamps \
-XX:+PrintGCApplicationStoppedTime \
-Xloggc:/var/log/kafka/kafka-gc.log \
-XX:+UseGCLogFileRotation \
-XX:NumberOfGCLogFiles=10 \
-XX:GCLogFileSize=100M"

# å¯åŠ¨Kafka
exec /opt/kafka/bin/kafka-server-start.sh "$@"
EOF

chmod +x /opt/kafka/bin/kafka-server-start-optimized.sh

# ==================== ç›‘æ§è„šæœ¬ ====================

# ç³»ç»Ÿæ€§èƒ½ç›‘æ§è„šæœ¬
cat > /opt/kafka/bin/system-monitor.sh << 'EOF'
#!/bin/bash

LOG_FILE="/var/log/kafka/system-monitor.log"

while true; do
    echo "=== $(date) ===" >> $LOG_FILE
    
    # CPUä½¿ç”¨ç‡
    echo "CPUä½¿ç”¨ç‡:" >> $LOG_FILE
    top -bn1 | grep "Cpu(s)" >> $LOG_FILE
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    echo "å†…å­˜ä½¿ç”¨æƒ…å†µ:" >> $LOG_FILE
    free -h >> $LOG_FILE
    
    # ç£ç›˜I/O
    echo "ç£ç›˜I/O:" >> $LOG_FILE
    iostat -x 1 1 >> $LOG_FILE
    
    # ç½‘ç»œæµé‡
    echo "ç½‘ç»œæµé‡:" >> $LOG_FILE
    sar -n DEV 1 1 >> $LOG_FILE
    
    # Kafkaè¿›ç¨‹çŠ¶æ€
    echo "Kafkaè¿›ç¨‹:" >> $LOG_FILE
    ps aux | grep kafka >> $LOG_FILE
    
    echo "" >> $LOG_FILE
    sleep 60
done
EOF

chmod +x /opt/kafka/bin/system-monitor.sh

# å¯åŠ¨ç³»ç»Ÿç›‘æ§
nohup /opt/kafka/bin/system-monitor.sh &
```

</TabItem>
<TabItem value="broker-optimization" label="Brokeré…ç½®ä¼˜åŒ–">

```bash title="Brokeræ€§èƒ½ä¼˜åŒ–é…ç½®"
# server.properties - ç”Ÿäº§ç¯å¢ƒä¼˜åŒ–é…ç½®

############################# åŸºç¡€é…ç½® #############################
# Broker IDï¼ˆé›†ç¾¤ä¸­å”¯ä¸€ï¼‰
broker.id=1

# ç›‘å¬é…ç½®
listeners=PLAINTEXT://0.0.0.0:9092
advertised.listeners=PLAINTEXT://kafka-broker-1:9092

############################# ç½‘ç»œå’Œçº¿ç¨‹é…ç½® #############################
# ç½‘ç»œçº¿ç¨‹æ•° - å¤„ç†ç½‘ç»œè¯·æ±‚
# å»ºè®®ï¼šCPUæ ¸æ•°ï¼Œé«˜å¹¶å‘åœºæ™¯å¯è®¾ç½®ä¸ºCPUæ ¸æ•°çš„1.5-2å€
num.network.threads=16

# I/Oçº¿ç¨‹æ•° - å¤„ç†ç£ç›˜è¯»å†™
# å»ºè®®ï¼šç£ç›˜æ•°é‡çš„2-3å€ï¼Œæˆ–CPUæ ¸æ•°
num.io.threads=32

# Socketç¼“å†²åŒºå¤§å°
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

# è¯·æ±‚é˜Ÿåˆ—å¤§å°
queued.max.requests=500

############################# æ—¥å¿—é…ç½® #############################
# æ—¥å¿—ç›®å½• - ä½¿ç”¨å¤šä¸ªç›®å½•åˆ†æ•£I/Oè´Ÿè½½
log.dirs=/var/kafka-logs-1,/var/kafka-logs-2,/var/kafka-logs-3,/var/kafka-logs-4

# åˆ†åŒºæ•°é…ç½®
num.partitions=6
default.replication.factor=3
min.insync.replicas=2

# æ—¥å¿—æ®µé…ç½®
log.segment.bytes=1073741824          # 1GBæ®µå¤§å°
log.roll.hours=168                    # 7å¤©æ»šåŠ¨
log.retention.hours=168               # 7å¤©ä¿ç•™
log.retention.bytes=1073741824000     # 1TBä¿ç•™å¤§å°

# æ—¥å¿—æ¸…ç†é…ç½®
log.cleanup.policy=delete
log.retention.check.interval.ms=300000
log.cleaner.enable=true
log.cleaner.threads=2
log.cleaner.io.max.bytes.per.second=104857600

############################# å‰¯æœ¬é…ç½® #############################
# å‰¯æœ¬æ‹‰å–é…ç½®
replica.fetch.max.bytes=1048576
replica.fetch.wait.max.ms=500
replica.high.watermark.checkpoint.interval.ms=5000
replica.lag.time.max.ms=30000

# Leaderé€‰ä¸¾é…ç½®
unclean.leader.election.enable=false
leader.imbalance.per.broker.percentage=10
leader.imbalance.check.interval.seconds=300

############################# å‹ç¼©é…ç½® #############################
# å¯ç”¨å‹ç¼©
compression.type=lz4

# æ—¥å¿—å‹ç¼©é…ç½®ï¼ˆé’ˆå¯¹compacted topicsï¼‰
log.cleaner.min.cleanable.ratio=0.5
log.cleaner.min.compaction.lag.ms=0
log.cleaner.max.compaction.lag.ms=9223372036854775807

############################# å†…å­˜å’Œç¼“å­˜é…ç½® #############################
# é¡µç¼“å­˜åˆ·ç›˜é…ç½®
log.flush.interval.messages=10000
log.flush.interval.ms=1000

# ç´¢å¼•é…ç½®
log.index.size.max.bytes=10485760
log.index.interval.bytes=4096

############################# è¿æ¥å’Œè¶…æ—¶é…ç½® #############################
# è¿æ¥é…ç½®
connections.max.idle.ms=600000
request.timeout.ms=30000

# ZooKeeperé…ç½®
zookeeper.connect=zk1:2181,zk2:2181,zk3:2181/kafka
zookeeper.connection.timeout.ms=18000
zookeeper.session.timeout.ms=18000

############################# ç›‘æ§é…ç½® #############################
# JMXé…ç½®
jmx.port=9999

# æŒ‡æ ‡æŠ¥å‘Šé…ç½®
metric.reporters=
metrics.num.samples=2
metrics.sample.window.ms=30000

############################# å®‰å…¨é…ç½® #############################
# è‡ªåŠ¨åˆ›å»ºTopicï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­ï¼‰
auto.create.topics.enable=false
delete.topic.enable=true

# äº‹åŠ¡é…ç½®
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
transaction.state.log.num.partitions=50

############################# é«˜çº§é…ç½® #############################
# ç»„åè°ƒå™¨é…ç½®
group.initial.rebalance.delay.ms=3000
group.max.session.timeout.ms=1800000
group.min.session.timeout.ms=6000

# ç”Ÿäº§è€…é…ç½®
producer.purgatory.purge.interval.requests=1000
fetch.purgatory.purge.interval.requests=1000

# é…ç½®åŠ¨æ€æ›´æ–°
config.providers=file
config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider
```

```java title="Brokeræ€§èƒ½ç›‘æ§å·¥å…·"
import org.apache.kafka.clients.admin.*;
import javax.management.*;
import javax.management.remote.*;
import java.util.*;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;

/**
 * Kafka Brokeræ€§èƒ½ç›‘æ§å·¥å…·
 * é€šè¿‡JMXç›‘æ§Brokerçš„å…³é”®æ€§èƒ½æŒ‡æ ‡
 */
public class BrokerPerformanceMonitor {
    
    private final MBeanServerConnection mbeanConnection;
    private final ScheduledExecutorService scheduler;
    private final Map<String, Double> previousValues = new HashMap<>();
    
    public BrokerPerformanceMonitor(String jmxUrl) throws Exception {
        JMXServiceURL serviceURL = new JMXServiceURL(jmxUrl);
        JMXConnector connector = JMXConnectorFactory.connect(serviceURL);
        this.mbeanConnection = connector.getMBeanServerConnection();
        this.scheduler = Executors.newScheduledThreadPool(1);
    }
    
    /**
     * å¯åŠ¨æ€§èƒ½ç›‘æ§
     */
    public void startMonitoring() {
        scheduler.scheduleAtFixedRate(this::collectAndReportMetrics, 0, 30, TimeUnit.SECONDS);
    }
    
    /**
     * æ”¶é›†å¹¶æŠ¥å‘Šæ€§èƒ½æŒ‡æ ‡
     */
    private void collectAndReportMetrics() {
        try {
            System.out.println("=== Kafka Brokeræ€§èƒ½æŒ‡æ ‡ (" + new Date() + ") ===");
            
            // 1. æ¶ˆæ¯ååé‡æŒ‡æ ‡
            collectThroughputMetrics();
            
            // 2. ç½‘ç»œæŒ‡æ ‡
            collectNetworkMetrics();
            
            // 3. ç£ç›˜I/OæŒ‡æ ‡
            collectDiskIOMetrics();
            
            // 4. JVMæŒ‡æ ‡
            collectJVMMetrics();
            
            // 5. å‰¯æœ¬æŒ‡æ ‡
            collectReplicationMetrics();
            
            // 6. è¯·æ±‚å¤„ç†æŒ‡æ ‡
            collectRequestMetrics();
            
            System.out.println();
            
        } catch (Exception e) {
            System.err.println("æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: " + e.getMessage());
        }
    }
    
    /**
     * æ”¶é›†ååé‡æŒ‡æ ‡
     */
    private void collectThroughputMetrics() throws Exception {
        // æ¶ˆæ¯è¾“å…¥é€Ÿç‡
        ObjectName messagesInPerSec = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec");
        Double messagesInRate = (Double) mbeanConnection.getAttribute(messagesInPerSec, "OneMinuteRate");
        
        // å­—èŠ‚è¾“å…¥é€Ÿç‡
        ObjectName bytesInPerSec = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec");
        Double bytesInRate = (Double) mbeanConnection.getAttribute(bytesInPerSec, "OneMinuteRate");
        
        // å­—èŠ‚è¾“å‡ºé€Ÿç‡
        ObjectName bytesOutPerSec = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec");
        Double bytesOutRate = (Double) mbeanConnection.getAttribute(bytesOutPerSec, "OneMinuteRate");
        
        System.out.printf("ååé‡æŒ‡æ ‡ - æ¶ˆæ¯è¾“å…¥: %.2f msg/s, å­—èŠ‚è¾“å…¥: %.2f MB/s, å­—èŠ‚è¾“å‡º: %.2f MB/s%n",
                         messagesInRate, bytesInRate / 1024 / 1024, bytesOutRate / 1024 / 1024);
    }
    
    /**
     * æ”¶é›†ç½‘ç»œæŒ‡æ ‡
     */
    private void collectNetworkMetrics() throws Exception {
        // ç½‘ç»œå¤„ç†å™¨å¹³å‡ç©ºé—²ç‡
        ObjectName networkProcessorAvgIdlePercent = new ObjectName("kafka.network:type=SocketServer,name=NetworkProcessorAvgIdlePercent");
        Double networkIdlePercent = (Double) mbeanConnection.getAttribute(networkProcessorAvgIdlePercent, "Value");
        
        // è¯·æ±‚é˜Ÿåˆ—å¤§å°
        ObjectName requestQueueSize = new ObjectName("kafka.network:type=RequestChannel,name=RequestQueueSize");
        Double queueSize = (Double) mbeanConnection.getAttribute(requestQueueSize, "Value");
        
        System.out.printf("ç½‘ç»œæŒ‡æ ‡ - ç½‘ç»œå¤„ç†å™¨ç©ºé—²ç‡: %.2f%%, è¯·æ±‚é˜Ÿåˆ—å¤§å°: %.0f%n",
                         networkIdlePercent * 100, queueSize);
    }
    
    /**
     * æ”¶é›†ç£ç›˜I/OæŒ‡æ ‡
     */
    private void collectDiskIOMetrics() throws Exception {
        // æ—¥å¿—åˆ·ç›˜é€Ÿç‡
        ObjectName logFlushRateAndTimeMs = new ObjectName("kafka.log:type=LogFlushStats,name=LogFlushRateAndTimeMs");
        Double logFlushRate = (Double) mbeanConnection.getAttribute(logFlushRateAndTimeMs, "OneMinuteRate");
        Double logFlushTime = (Double) mbeanConnection.getAttribute(logFlushRateAndTimeMs, "Mean");
        
        System.out.printf("ç£ç›˜I/OæŒ‡æ ‡ - æ—¥å¿—åˆ·ç›˜é€Ÿç‡: %.2f/s, å¹³å‡åˆ·ç›˜æ—¶é—´: %.2f ms%n",
                         logFlushRate, logFlushTime);
    }
    
    /**
     * æ”¶é›†JVMæŒ‡æ ‡
     */
    private void collectJVMMetrics() throws Exception {
        // å †å†…å­˜ä½¿ç”¨
        ObjectName heapMemoryUsage = new ObjectName("java.lang:type=Memory");
        CompositeData heapMemory = (CompositeData) mbeanConnection.getAttribute(heapMemoryUsage, "HeapMemoryUsage");
        Long heapUsed = (Long) heapMemory.get("used");
        Long heapMax = (Long) heapMemory.get("max");
        
        // GCä¿¡æ¯
        ObjectName gcInfo = new ObjectName("java.lang:type=GarbageCollector,name=G1 Young Generation");
        Long gcCount = (Long) mbeanConnection.getAttribute(gcInfo, "CollectionCount");
        Long gcTime = (Long) mbeanConnection.getAttribute(gcInfo, "CollectionTime");
        
        System.out.printf("JVMæŒ‡æ ‡ - å †å†…å­˜ä½¿ç”¨: %.2f%% (%d MB / %d MB), GCæ¬¡æ•°: %d, GCæ—¶é—´: %d ms%n",
                         (double) heapUsed / heapMax * 100, heapUsed / 1024 / 1024, heapMax / 1024 / 1024,
                         gcCount, gcTime);
    }
    
    /**
     * æ”¶é›†å‰¯æœ¬æŒ‡æ ‡
     */
    private void collectReplicationMetrics() throws Exception {
        // æœªåŒæ­¥å‰¯æœ¬æ•°é‡
        ObjectName underReplicatedPartitions = new ObjectName("kafka.server:type=ReplicaManager,name=UnderReplicatedPartitions");
        Double underReplicated = (Double) mbeanConnection.getAttribute(underReplicatedPartitions, "Value");
        
        // ç¦»çº¿åˆ†åŒºæ•°é‡
        ObjectName offlinePartitionsCount = new ObjectName("kafka.controller:type=KafkaController,name=OfflinePartitionsCount");
        Double offlinePartitions = (Double) mbeanConnection.getAttribute(offlinePartitionsCount, "Value");
        
        System.out.printf("å‰¯æœ¬æŒ‡æ ‡ - æœªåŒæ­¥å‰¯æœ¬: %.0f, ç¦»çº¿åˆ†åŒº: %.0f%n",
                         underReplicated, offlinePartitions);
    }
    
    /**
     * æ”¶é›†è¯·æ±‚å¤„ç†æŒ‡æ ‡
     */
    private void collectRequestMetrics() throws Exception {
        // ç”Ÿäº§è¯·æ±‚å¤„ç†æ—¶é—´
        ObjectName produceRequestTime = new ObjectName("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce");
        Double produceTime = (Double) mbeanConnection.getAttribute(produceRequestTime, "Mean");
        
        // æ‹‰å–è¯·æ±‚å¤„ç†æ—¶é—´
        ObjectName fetchRequestTime = new ObjectName("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer");
        Double fetchTime = (Double) mbeanConnection.getAttribute(fetchRequestTime, "Mean");
        
        System.out.printf("è¯·æ±‚å¤„ç†æŒ‡æ ‡ - ç”Ÿäº§è¯·æ±‚å¹³å‡æ—¶é—´: %.2f ms, æ‹‰å–è¯·æ±‚å¹³å‡æ—¶é—´: %.2f ms%n",
                         produceTime, fetchTime);
    }
    
    /**
     * ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
     */
    public void generatePerformanceReport() {
        try {
            System.out.println("=== Kafka Brokeræ€§èƒ½æŠ¥å‘Š ===");
            
            // æ”¶é›†è¯¦ç»†æŒ‡æ ‡
            Map<String, Object> metrics = new HashMap<>();
            
            // ååé‡æŒ‡æ ‡
            ObjectName messagesInPerSec = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=MessagesInPerSec");
            metrics.put("messagesInPerSec", mbeanConnection.getAttribute(messagesInPerSec, "OneMinuteRate"));
            
            ObjectName bytesInPerSec = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=BytesInPerSec");
            metrics.put("bytesInPerSec", mbeanConnection.getAttribute(bytesInPerSec, "OneMinuteRate"));
            
            ObjectName bytesOutPerSec = new ObjectName("kafka.server:type=BrokerTopicMetrics,name=BytesOutPerSec");
            metrics.put("bytesOutPerSec", mbeanConnection.getAttribute(bytesOutPerSec, "OneMinuteRate"));
            
            // å»¶è¿ŸæŒ‡æ ‡
            ObjectName produceRequestTime = new ObjectName("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=Produce");
            metrics.put("produceLatency", mbeanConnection.getAttribute(produceRequestTime, "Mean"));
            
            ObjectName fetchRequestTime = new ObjectName("kafka.network:type=RequestMetrics,name=TotalTimeMs,request=FetchConsumer");
            metrics.put("fetchLatency", mbeanConnection.getAttribute(fetchRequestTime, "Mean"));
            
            // èµ„æºä½¿ç”¨æŒ‡æ ‡
            ObjectName heapMemoryUsage = new ObjectName("java.lang:type=Memory");
            CompositeData heapMemory = (CompositeData) mbeanConnection.getAttribute(heapMemoryUsage, "HeapMemoryUsage");
            metrics.put("heapUsedPercent", (double) ((Long) heapMemory.get("used")) / ((Long) heapMemory.get("max")) * 100);
            
            // æ‰“å°æŠ¥å‘Š
            System.out.println("æ€§èƒ½æŒ‡æ ‡æ‘˜è¦:");
            System.out.printf("  æ¶ˆæ¯ååé‡: %.2f msg/s%n", (Double) metrics.get("messagesInPerSec"));
            System.out.printf("  æ•°æ®è¾“å…¥é€Ÿç‡: %.2f MB/s%n", (Double) metrics.get("bytesInPerSec") / 1024 / 1024);
            System.out.printf("  æ•°æ®è¾“å‡ºé€Ÿç‡: %.2f MB/s%n", (Double) metrics.get("bytesOutPerSec") / 1024 / 1024);
            System.out.printf("  ç”Ÿäº§å»¶è¿Ÿ: %.2f ms%n", (Double) metrics.get("produceLatency"));
            System.out.printf("  æ‹‰å–å»¶è¿Ÿ: %.2f ms%n", (Double) metrics.get("fetchLatency"));
            System.out.printf("  å †å†…å­˜ä½¿ç”¨ç‡: %.2f%%%n", (Double) metrics.get("heapUsedPercent"));
            
            // æ€§èƒ½è¯„ä¼°
            evaluatePerformance(metrics);
            
        } catch (Exception e) {
            System.err.println("ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šå¤±è´¥: " + e.getMessage());
        }
    }
    
    /**
     * æ€§èƒ½è¯„ä¼°å’Œå»ºè®®
     */
    private void evaluatePerformance(Map<String, Object> metrics) {
        System.out.println("\næ€§èƒ½è¯„ä¼°å’Œå»ºè®®:");
        
        Double messagesInPerSec = (Double) metrics.get("messagesInPerSec");
        Double produceLatency = (Double) metrics.get("produceLatency");
        Double fetchLatency = (Double) metrics.get("fetchLatency");
        Double heapUsedPercent = (Double) metrics.get("heapUsedPercent");
        
        // ååé‡è¯„ä¼°
        if (messagesInPerSec < 1000) {
            System.out.println("  âš ï¸  æ¶ˆæ¯ååé‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç”Ÿäº§è€…é…ç½®å’Œç½‘ç»œçŠ¶å†µ");
        } else if (messagesInPerSec > 50000) {
            System.out.println("  âœ… æ¶ˆæ¯ååé‡è‰¯å¥½");
        }
        
        // å»¶è¿Ÿè¯„ä¼°
        if (produceLatency > 100) {
            System.out.println("  âš ï¸  ç”Ÿäº§å»¶è¿Ÿè¾ƒé«˜ï¼Œå»ºè®®ä¼˜åŒ–ç£ç›˜I/Oå’Œç½‘ç»œé…ç½®");
        } else {
            System.out.println("  âœ… ç”Ÿäº§å»¶è¿Ÿæ­£å¸¸");
        }
        
        if (fetchLatency > 50) {
            System.out.println("  âš ï¸  æ‹‰å–å»¶è¿Ÿè¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥æ¶ˆè´¹è€…é…ç½®");
        } else {
            System.out.println("  âœ… æ‹‰å–å»¶è¿Ÿæ­£å¸¸");
        }
        
        // å†…å­˜è¯„ä¼°
        if (heapUsedPercent > 80) {
            System.out.println("  âš ï¸  å †å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ å †å†…å­˜æˆ–ä¼˜åŒ–åº”ç”¨");
        } else {
            System.out.println("  âœ… å †å†…å­˜ä½¿ç”¨ç‡æ­£å¸¸");
        }
    }
    
    public void shutdown() {
        scheduler.shutdown();
    }
    
    public static void main(String[] args) {
        try {
            // JMXè¿æ¥URLï¼Œæ ¹æ®å®é™…é…ç½®è°ƒæ•´
            String jmxUrl = "service:jmx:rmi:///jndi/rmi://localhost:9999/jmxrmi";
            
            BrokerPerformanceMonitor monitor = new BrokerPerformanceMonitor(jmxUrl);
            
            // å¯åŠ¨ç›‘æ§
            monitor.startMonitoring();
            
            // ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            Thread.sleep(60000); // ç­‰å¾…1åˆ†é’Ÿæ”¶é›†æ•°æ®
            monitor.generatePerformanceReport();
            
            // æ·»åŠ å…³é—­é’©å­
            Runtime.getRuntime().addShutdownHook(new Thread(monitor::shutdown));
            
        } catch (Exception e) {
            System.err.println("å¯åŠ¨æ€§èƒ½ç›‘æ§å¤±è´¥: " + e.getMessage());
            e.printStackTrace();
        }
    }
}
```

</TabItem>
<TabItem value="client-optimization" label="å®¢æˆ·ç«¯ä¼˜åŒ–">

```java title="ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…æ€§èƒ½ä¼˜åŒ–"
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.*;
import java.util.*;
import java.util.concurrent.*;

/**
 * Kafkaå®¢æˆ·ç«¯æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹
 * å±•ç¤ºç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…çš„æœ€ä½³é…ç½®å®è·µ
 */
public class ClientOptimizationExample {
    
    /**
     * é«˜æ€§èƒ½ç”Ÿäº§è€…é…ç½®
     */
    public static Properties createOptimizedProducerConfig(String bootstrapServers) {
        Properties props = new Properties();
        
        // åŸºç¡€è¿æ¥é…ç½®
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        
        // å¯é æ€§é…ç½®
        props.put(ProducerConfig.ACKS_CONFIG, "1");                     // å¹³è¡¡æ€§èƒ½å’Œå¯é æ€§
        props.put(ProducerConfig.RETRIES_CONFIG, 3);                    // é‡è¯•3æ¬¡
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG, 100);         // é‡è¯•é—´éš”100ms
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);      // å¯ç”¨å¹‚ç­‰æ€§
        
        // æ‰¹å¤„ç†ä¼˜åŒ–
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 65536);             // 64KBæ‰¹æ¬¡å¤§å°
        props.put(ProducerConfig.LINGER_MS_CONFIG, 20);                 // ç­‰å¾…20msæ”¶é›†æ›´å¤šæ¶ˆæ¯
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 134217728);      // 128MBç¼“å†²åŒº
        
        // å‹ç¼©é…ç½®
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");       // LZ4å‹ç¼©ï¼Œå¹³è¡¡å‹ç¼©ç‡å’ŒCPU
        
        // ç½‘ç»œä¼˜åŒ–
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);     // è¯·æ±‚è¶…æ—¶30ç§’
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 120000);   // æŠ•é€’è¶…æ—¶2åˆ†é’Ÿ
        
        // åˆ†åŒºå™¨é…ç½®
        props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, "org.apache.kafka.clients.producer.RoundRobinPartitioner");
        
        return props;
    }
    
    /**
     * é«˜æ€§èƒ½æ¶ˆè´¹è€…é…ç½®
     */
    public static Properties createOptimizedConsumerConfig(String bootstrapServers, String groupId) {
        Properties props = new Properties();
        
        // åŸºç¡€è¿æ¥é…ç½®
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        
        // ä½ç§»ç®¡ç†é…ç½®
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);     // æ‰‹åŠ¨æäº¤ä½ç§»
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // ä»æœ€æ—©ä½ç§»å¼€å§‹
        
        // æ‹‰å–ä¼˜åŒ–é…ç½®
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 50000);        // æœ€å°æ‹‰å–50KB
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);        // æœ€å¤§ç­‰å¾…500ms
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 1000);        // æ¯æ¬¡æœ€å¤šæ‹‰å–1000æ¡
        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 2097152); // æ¯åˆ†åŒºæœ€å¤§2MB
        
        // ä¼šè¯å’Œå¿ƒè·³é…ç½®
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);     // ä¼šè¯è¶…æ—¶30ç§’
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);  // å¿ƒè·³é—´éš”10ç§’
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000);  // æœ€å¤§pollé—´éš”5åˆ†é’Ÿ
        
        // åˆ†åŒºåˆ†é…ç­–ç•¥
        props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, 
                 "org.apache.kafka.clients.consumer.CooperativeStickyAssignor");
        
        return props;
    }
    
    /**
     * é«˜æ€§èƒ½ç”Ÿäº§è€…å®ç°
     */
    public static class HighPerformanceProducer {
        private final KafkaProducer<String, String> producer;
        private final String topicName;
        private final AtomicLong messageCount = new AtomicLong(0);
        private final AtomicLong byteCount = new AtomicLong(0);
        
        public HighPerformanceProducer(String bootstrapServers, String topicName) {
            this.topicName = topicName;
            this.producer = new KafkaProducer<>(createOptimizedProducerConfig(bootstrapServers));
        }
        
        /**
         * å¼‚æ­¥æ‰¹é‡å‘é€
         */
        public void sendBatchAsync(List<String> messages) {
            CountDownLatch latch = new CountDownLatch(messages.size());
            
            for (String message : messages) {
                ProducerRecord<String, String> record = new ProducerRecord<>(topicName, message);
                
                producer.send(record, (metadata, exception) -> {
                    if (exception == null) {
                        messageCount.incrementAndGet();
                        byteCount.addAndGet(message.getBytes().length);
                    } else {
                        System.err.println("å‘é€å¤±è´¥: " + exception.getMessage());
                    }
                    latch.countDown();
                });
            }
            
            try {
                latch.await(30, TimeUnit.SECONDS);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        }
        
        /**
         * è·å–æ€§èƒ½ç»Ÿè®¡
         */
        public void printStats() {
            System.out.printf("ç”Ÿäº§è€…ç»Ÿè®¡ - æ¶ˆæ¯æ•°: %d, å­—èŠ‚æ•°: %d%n", 
                             messageCount.get(), byteCount.get());
        }
        
        public void close() {
            producer.close();
        }
    }
    
    /**
     * é«˜æ€§èƒ½æ¶ˆè´¹è€…å®ç°
     */
    public static class HighPerformanceConsumer {
        private final KafkaConsumer<String, String> consumer;
        private final ExecutorService executorService;
        private final AtomicLong messageCount = new AtomicLong(0);
        private final AtomicLong byteCount = new AtomicLong(0);
        private volatile boolean running = false;
        
        public HighPerformanceConsumer(String bootstrapServers, String groupId, List<String> topics) {
            this.consumer = new KafkaConsumer<>(createOptimizedConsumerConfig(bootstrapServers, groupId));
            this.executorService = Executors.newFixedThreadPool(4); // 4ä¸ªå¤„ç†çº¿ç¨‹
            consumer.subscribe(topics);
        }
        
        /**
         * å¼€å§‹é«˜æ€§èƒ½æ¶ˆè´¹
         */
        public void startConsuming() {
            running = true;
            
            while (running) {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
                
                if (!records.isEmpty()) {
                    // æŒ‰åˆ†åŒºå¹¶è¡Œå¤„ç†
                    Map<TopicPartition, List<ConsumerRecord<String, String>>> partitionRecords = new HashMap<>();
                    
                    for (ConsumerRecord<String, String> record : records) {
                        TopicPartition partition = new TopicPartition(record.topic(), record.partition());
                        partitionRecords.computeIfAbsent(partition, k -> new ArrayList<>()).add(record);
                    }
                    
                    // æäº¤å¤„ç†ä»»åŠ¡
                    List<Future<?>> futures = new ArrayList<>();
                    for (List<ConsumerRecord<String, String>> partitionBatch : partitionRecords.values()) {
                        Future<?> future = executorService.submit(() -> processBatch(partitionBatch));
                        futures.add(future);
                    }
                    
                    // ç­‰å¾…æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆ
                    for (Future<?> future : futures) {
                        try {
                            future.get(30, TimeUnit.SECONDS);
                        } catch (Exception e) {
                            System.err.println("æ‰¹æ¬¡å¤„ç†å¤±è´¥: " + e.getMessage());
                        }
                    }
                    
                    // æäº¤ä½ç§»
                    try {
                        consumer.commitSync();
                    } catch (Exception e) {
                        System.err.println("ä½ç§»æäº¤å¤±è´¥: " + e.getMessage());
                    }
                }
            }
        }
        
        /**
         * æ‰¹é‡å¤„ç†æ¶ˆæ¯
         */
        private void processBatch(List<ConsumerRecord<String, String>> batch) {
            for (ConsumerRecord<String, String> record : batch) {
                try {
                    // æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†
                    processMessage(record);
                    
                    messageCount.incrementAndGet();
                    byteCount.addAndGet(record.value().getBytes().length);
                    
                } catch (Exception e) {
                    System.err.println("å¤„ç†æ¶ˆæ¯å¤±è´¥: " + e.getMessage());
                }
            }
        }
        
        private void processMessage(ConsumerRecord<String, String> record) {
            // å®é™…çš„æ¶ˆæ¯å¤„ç†é€»è¾‘
            // è¿™é‡Œåªæ˜¯æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            try {
                Thread.sleep(1);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        }
        
        /**
         * è·å–æ€§èƒ½ç»Ÿè®¡
         */
        public void printStats() {
            System.out.printf("æ¶ˆè´¹è€…ç»Ÿè®¡ - æ¶ˆæ¯æ•°: %d, å­—èŠ‚æ•°: %d%n", 
                             messageCount.get(), byteCount.get());
        }
        
        public void stop() {
            running = false;
            executorService.shutdown();
            consumer.close();
        }
    }
    
    /**
     * æ€§èƒ½æµ‹è¯•å·¥å…·
     */
    public static class PerformanceTest {
        
        /**
         * ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•
         */
        public static void testProducerPerformance(String bootstrapServers, String topic, 
                                                 int messageCount, int messageSize) {
            System.out.println("å¼€å§‹ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•...");
            
            HighPerformanceProducer producer = new HighPerformanceProducer(bootstrapServers, topic);
            
            // ç”Ÿæˆæµ‹è¯•æ¶ˆæ¯
            List<String> messages = new ArrayList<>();
            String messageTemplate = "A".repeat(messageSize);
            
            for (int i = 0; i < messageCount; i++) {
                messages.add(messageTemplate + "-" + i);
            }
            
            // æ€§èƒ½æµ‹è¯•
            long startTime = System.currentTimeMillis();
            
            // åˆ†æ‰¹å‘é€
            int batchSize = 1000;
            for (int i = 0; i < messages.size(); i += batchSize) {
                int endIndex = Math.min(i + batchSize, messages.size());
                List<String> batch = messages.subList(i, endIndex);
                producer.sendBatchAsync(batch);
            }
            
            long endTime = System.currentTimeMillis();
            
            // æ‰“å°ç»“æœ
            long duration = endTime - startTime;
            double throughput = (double) messageCount / duration * 1000; // æ¶ˆæ¯/ç§’
            double mbps = (double) messageCount * messageSize / duration / 1024; // MB/ç§’
            
            System.out.printf("ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•ç»“æœ:%n");
            System.out.printf("  æ¶ˆæ¯æ•°é‡: %d%n", messageCount);
            System.out.printf("  æ¶ˆæ¯å¤§å°: %d å­—èŠ‚%n", messageSize);
            System.out.printf("  æ€»è€—æ—¶: %d ms%n", duration);
            System.out.printf("  ååé‡: %.2f æ¶ˆæ¯/ç§’%n", throughput);
            System.out.printf("  æ•°æ®é€Ÿç‡: %.2f MB/ç§’%n", mbps);
            
            producer.close();
        }
        
        /**
         * æ¶ˆè´¹è€…æ€§èƒ½æµ‹è¯•
         */
        public static void testConsumerPerformance(String bootstrapServers, String groupId, 
                                                 List<String> topics, int durationSeconds) {
            System.out.println("å¼€å§‹æ¶ˆè´¹è€…æ€§èƒ½æµ‹è¯•...");
            
            HighPerformanceConsumer consumer = new HighPerformanceConsumer(bootstrapServers, groupId, topics);
            
            // å¯åŠ¨æ¶ˆè´¹çº¿ç¨‹
            Thread consumerThread = new Thread(consumer::startConsuming);
            consumerThread.start();
            
            // è¿è¡ŒæŒ‡å®šæ—¶é—´
            try {
                Thread.sleep(durationSeconds * 1000);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
            
            // åœæ­¢æ¶ˆè´¹è€…
            consumer.stop();
            
            try {
                consumerThread.join();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
            
            // æ‰“å°ç»“æœ
            consumer.printStats();
        }
    }
    
    public static void main(String[] args) {
        String bootstrapServers = "localhost:9092";
        String topic = "performance-test";
        
        // ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•
        PerformanceTest.testProducerPerformance(bootstrapServers, topic, 10000, 1024);
        
        // ç­‰å¾…ä¸€æ®µæ—¶é—´
        try {
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
        
        // æ¶ˆè´¹è€…æ€§èƒ½æµ‹è¯•
        PerformanceTest.testConsumerPerformance(bootstrapServers, "perf-test-group", 
                                              Arrays.asList(topic), 30);
    }
}
```

</TabItem>
</Tabs>

## 6. Kafkaé¢è¯•é¢˜ç²¾é€‰ä¸æ·±åº¦è§£æ

### 6.1 åŸºç¡€æ¦‚å¿µä¸æ¶æ„é¢˜

**Q1: è¯¦ç»†è§£é‡ŠKafkaçš„æ ¸å¿ƒç»„ä»¶åŠå…¶ä½œç”¨ï¼Ÿ**

**A:** Kafkaçš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š

**Brokerï¼ˆä»£ç†æœåŠ¡å™¨ï¼‰**ï¼š
- ä½œç”¨ï¼šKafkaé›†ç¾¤ä¸­çš„æœåŠ¡å™¨èŠ‚ç‚¹ï¼Œè´Ÿè´£å­˜å‚¨å’Œå¤„ç†æ¶ˆæ¯
- èŒè´£ï¼šç®¡ç†Topicåˆ†åŒºã€å¤„ç†ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…è¯·æ±‚ã€å‚ä¸Leaderé€‰ä¸¾
- ç‰¹ç‚¹ï¼šæ— çŠ¶æ€è®¾è®¡ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•

**Topicï¼ˆä¸»é¢˜ï¼‰**ï¼š
- ä½œç”¨ï¼šæ¶ˆæ¯çš„é€»è¾‘åˆ†ç±»ï¼Œç±»ä¼¼æ•°æ®åº“ä¸­çš„è¡¨
- ç‰¹ç‚¹ï¼šæ”¯æŒå¤šåˆ†åŒºã€å¤šå‰¯æœ¬ï¼Œæä¾›å¹¶è¡Œå¤„ç†èƒ½åŠ›
- å‘½åï¼šå»ºè®®ä½¿ç”¨ä¸šåŠ¡åŸŸ.æ•°æ®ç±»å‹.ç‰ˆæœ¬çš„æ ¼å¼

**Partitionï¼ˆåˆ†åŒºï¼‰**ï¼š
- ä½œç”¨ï¼šTopicçš„ç‰©ç†åˆ†å‰²ï¼Œæä¾›å¹¶è¡Œå¤„ç†å’Œè´Ÿè½½åˆ†æ•£
- ç‰¹ç‚¹ï¼šæœ‰åºã€ä¸å¯å˜çš„æ¶ˆæ¯åºåˆ—ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•
- åˆ†é…ï¼šé€šè¿‡åˆ†åŒºå™¨å†³å®šæ¶ˆæ¯åˆ†é…åˆ°å“ªä¸ªåˆ†åŒº

**Producerï¼ˆç”Ÿäº§è€…ï¼‰**ï¼š
- ä½œç”¨ï¼šå‘Kafkaå‘é€æ¶ˆæ¯çš„å®¢æˆ·ç«¯
- ç‰¹ç‚¹ï¼šæ”¯æŒåŒæ­¥/å¼‚æ­¥å‘é€ã€æ‰¹å¤„ç†ã€å‹ç¼©
- é…ç½®ï¼šå¯é…ç½®å¯é æ€§ã€æ€§èƒ½ã€åˆ†åŒºç­–ç•¥ç­‰å‚æ•°

**Consumerï¼ˆæ¶ˆè´¹è€…ï¼‰**ï¼š
- ä½œç”¨ï¼šä»Kafkaè¯»å–æ¶ˆæ¯çš„å®¢æˆ·ç«¯
- ç‰¹ç‚¹ï¼šæ”¯æŒæ¶ˆè´¹è€…ç»„ã€ä½ç§»ç®¡ç†ã€é‡å¹³è¡¡
- æ¨¡å¼ï¼šæ¨æ¨¡å¼ï¼ˆå®é™…æ˜¯æ‹‰æ¨¡å¼ï¼‰ã€æ”¯æŒæ‰¹é‡æ¶ˆè´¹

**Q2: Kafkaå¦‚ä½•ä¿è¯æ¶ˆæ¯çš„é¡ºåºæ€§ï¼Ÿåœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¼šå‡ºç°ä¹±åºï¼Ÿ**

**A:** Kafkaçš„æ¶ˆæ¯é¡ºåºæ€§ä¿è¯æœºåˆ¶ï¼š

**åˆ†åŒºçº§åˆ«æœ‰åº**ï¼š
- Kafkaåªåœ¨å•ä¸ªåˆ†åŒºå†…ä¿è¯æ¶ˆæ¯é¡ºåº
- åŒä¸€åˆ†åŒºå†…çš„æ¶ˆæ¯æŒ‰ç…§å‘é€é¡ºåºå­˜å‚¨å’Œæ¶ˆè´¹
- è·¨åˆ†åŒºæ— æ³•ä¿è¯å…¨å±€é¡ºåº

**ç”Ÿäº§è€…ç«¯é¡ºåºä¿è¯**ï¼š
```java
// ç¡®ä¿é¡ºåºçš„é…ç½®
props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
```

**æ¶ˆè´¹è€…ç«¯é¡ºåºä¿è¯**ï¼š
- å•çº¿ç¨‹æ¶ˆè´¹åŒä¸€åˆ†åŒº
- æŒ‰ä½ç§»é¡ºåºå¤„ç†æ¶ˆæ¯
- é¿å…å¹¶è¡Œå¤„ç†åŒä¸€åˆ†åŒºçš„æ¶ˆæ¯

**å¯èƒ½å‡ºç°ä¹±åºçš„æƒ…å†µ**ï¼š
1. **ç”Ÿäº§è€…é‡è¯•**ï¼šç½‘ç»œå¼‚å¸¸å¯¼è‡´é‡è¯•æ—¶å¯èƒ½ä¹±åº
2. **å¤šåˆ†åŒº**ï¼šä¸åŒåˆ†åŒºé—´æ— æ³•ä¿è¯é¡ºåº
3. **å¹¶è¡Œæ¶ˆè´¹**ï¼šå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†åŒä¸€åˆ†åŒºæ¶ˆæ¯
4. **å¼‚æ­¥å¤„ç†**ï¼šæ¶ˆè´¹è€…å¼‚æ­¥å¤„ç†æ¶ˆæ¯æ—¶å¯èƒ½ä¹±åº

**Q3: è¯¦ç»†è¯´æ˜Kafkaçš„å‰¯æœ¬æœºåˆ¶å’ŒISRçš„ä½œç”¨ï¼Ÿ**

**A:** Kafkaå‰¯æœ¬æœºåˆ¶è¯¦è§£ï¼š

**å‰¯æœ¬ç±»å‹**ï¼š
- **Leaderå‰¯æœ¬**ï¼šå¤„ç†æ‰€æœ‰è¯»å†™è¯·æ±‚ï¼Œç»´æŠ¤æ¶ˆæ¯é¡ºåº
- **Followerå‰¯æœ¬**ï¼šè¢«åŠ¨å¤åˆ¶Leaderæ•°æ®ï¼Œä¸å¤„ç†å®¢æˆ·ç«¯è¯·æ±‚
- **ISRå‰¯æœ¬**ï¼šIn-Sync Replicasï¼Œä¸Leaderä¿æŒåŒæ­¥çš„å‰¯æœ¬é›†åˆ

**ISRæœºåˆ¶**ï¼š
```java
// ISRç›¸å…³é…ç½®
replica.lag.time.max.ms=30000          // å‰¯æœ¬æœ€å¤§æ»åæ—¶é—´
min.insync.replicas=2                  // æœ€å°åŒæ­¥å‰¯æœ¬æ•°
unclean.leader.election.enable=false   // ç¦ç”¨ä¸å®‰å…¨Leaderé€‰ä¸¾
```

**å‰¯æœ¬åŒæ­¥è¿‡ç¨‹**ï¼š
1. Followerå‘Leaderå‘é€æ‹‰å–è¯·æ±‚
2. Leaderè¿”å›æ¶ˆæ¯æ•°æ®
3. Followerå†™å…¥æœ¬åœ°æ—¥å¿—
4. Followerå‘é€ç¡®è®¤ç»™Leader
5. Leaderæ›´æ–°é«˜æ°´ä½æ ‡è®°

**ISRç®¡ç†**ï¼š
- å‰¯æœ¬æ»åè¶…è¿‡`replica.lag.time.max.ms`ä¼šè¢«ç§»å‡ºISR
- å‰¯æœ¬è¿½ä¸Šè¿›åº¦åä¼šé‡æ–°åŠ å…¥ISR
- åªæœ‰ISRä¸­çš„å‰¯æœ¬æ‰èƒ½è¢«é€‰ä¸ºæ–°Leader

### 6.2 æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜é¢˜

**Q4: å¦‚ä½•ä¼˜åŒ–Kafkaçš„ååé‡ï¼Ÿä»å“ªäº›æ–¹é¢å…¥æ‰‹ï¼Ÿ**

**A:** Kafkaååé‡ä¼˜åŒ–ç­–ç•¥ï¼š

**ç”Ÿäº§è€…ä¼˜åŒ–**ï¼š
```java
// é«˜ååé‡ç”Ÿäº§è€…é…ç½®
props.put(ProducerConfig.BATCH_SIZE_CONFIG, 65536);        // å¢å¤§æ‰¹æ¬¡
props.put(ProducerConfig.LINGER_MS_CONFIG, 100);           // å¢åŠ ç­‰å¾…æ—¶é—´
props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");  // å¯ç”¨å‹ç¼©
props.put(ProducerConfig.ACKS_CONFIG, "1");                // é™ä½ç¡®è®¤çº§åˆ«
props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 134217728); // å¢å¤§ç¼“å†²åŒº
```

**æ¶ˆè´¹è€…ä¼˜åŒ–**ï¼š
```java
// é«˜ååé‡æ¶ˆè´¹è€…é…ç½®
props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 100000);     // å¢å¤§æ‹‰å–å¤§å°
props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 2000);      // å¢åŠ æ‹‰å–è®°å½•æ•°
props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);      // é€‚å½“ç­‰å¾…æ—¶é—´
```

**Brokerä¼˜åŒ–**ï¼š
```bash
# å¢åŠ ç½‘ç»œå’ŒI/Oçº¿ç¨‹
num.network.threads=16
num.io.threads=32

# ä¼˜åŒ–æ—¥å¿—é…ç½®
log.segment.bytes=1073741824
log.flush.interval.messages=10000
```

**ç¡¬ä»¶ä¼˜åŒ–**ï¼š
- ä½¿ç”¨SSDå­˜å‚¨æé«˜I/Oæ€§èƒ½
- å¢åŠ ç½‘ç»œå¸¦å®½ï¼ˆä¸‡å…†ç½‘å¡ï¼‰
- ä¼˜åŒ–JVMå‚æ•°å’ŒGCç­–ç•¥
- ä½¿ç”¨å¤šç£ç›˜åˆ†æ•£I/Oè´Ÿè½½

**Q5: Kafkaæ¶ˆæ¯ä¸¢å¤±çš„åœºæ™¯æœ‰å“ªäº›ï¼Ÿå¦‚ä½•é¿å…ï¼Ÿ**

**A:** Kafkaæ¶ˆæ¯ä¸¢å¤±åœºæ™¯åŠè§£å†³æ–¹æ¡ˆï¼š

**ç”Ÿäº§è€…ç«¯ä¸¢å¤±**ï¼š
```java
// åœºæ™¯ï¼šç½‘ç»œå¼‚å¸¸ã€Brokeræ•…éšœã€ç¼“å†²åŒºæ»¡
// è§£å†³æ–¹æ¡ˆï¼š
props.put(ProducerConfig.ACKS_CONFIG, "all");              // ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE); // æ— é™é‡è¯•
props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);   // å¯ç”¨å¹‚ç­‰æ€§
props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // ä¿è¯é¡ºåº

// åŒæ­¥å‘é€ç¡®ä¿æ¶ˆæ¯é€è¾¾
RecordMetadata metadata = producer.send(record).get();
```

**Brokerç«¯ä¸¢å¤±**ï¼š
```bash
# åœºæ™¯ï¼šç£ç›˜æ•…éšœã€å‰¯æœ¬ä¸è¶³ã€ä¸å®‰å…¨Leaderé€‰ä¸¾
# è§£å†³æ–¹æ¡ˆï¼š
default.replication.factor=3           # å¢åŠ å‰¯æœ¬å› å­
min.insync.replicas=2                  # è®¾ç½®æœ€å°åŒæ­¥å‰¯æœ¬
unclean.leader.election.enable=false   # ç¦ç”¨ä¸å®‰å…¨é€‰ä¸¾
log.flush.interval.messages=1          # å¼ºåˆ¶åˆ·ç›˜
```

**æ¶ˆè´¹è€…ç«¯ä¸¢å¤±**ï¼š
```java
// åœºæ™¯ï¼šè‡ªåŠ¨æäº¤ä½ç§»ã€å¤„ç†å¼‚å¸¸æœªå¤„ç†
// è§£å†³æ–¹æ¡ˆï¼š
props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // ç¦ç”¨è‡ªåŠ¨æäº¤

// æ‰‹åŠ¨æäº¤ä½ç§»
while (true) {
    ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
    for (ConsumerRecord<String, String> record : records) {
        try {
            processMessage(record); // å¤„ç†æ¶ˆæ¯
        } catch (Exception e) {
            // å¤„ç†å¼‚å¸¸ï¼Œå†³å®šæ˜¯å¦è·³è¿‡æˆ–é‡è¯•
            handleException(record, e);
        }
    }
    consumer.commitSync(); // å¤„ç†å®Œæˆåæäº¤ä½ç§»
}
```

### 6.3 æ¶æ„è®¾è®¡ä¸å®æˆ˜é¢˜

**Q6: è®¾è®¡ä¸€ä¸ªé«˜å¯ç”¨çš„Kafkaé›†ç¾¤æ¶æ„ï¼Œéœ€è¦è€ƒè™‘å“ªäº›å› ç´ ï¼Ÿ**

**A:** é«˜å¯ç”¨Kafkaé›†ç¾¤è®¾è®¡è¦ç‚¹ï¼š

**é›†ç¾¤è§„åˆ’**ï¼š
```bash
# æœ€å°3èŠ‚ç‚¹é›†ç¾¤ï¼ˆå¥‡æ•°èŠ‚ç‚¹ï¼‰
Broker-1: 192.168.1.101:9092
Broker-2: 192.168.1.102:9092  
Broker-3: 192.168.1.103:9092

# ZooKeeperé›†ç¾¤ï¼ˆ3æˆ–5èŠ‚ç‚¹ï¼‰
ZK-1: 192.168.1.201:2181
ZK-2: 192.168.1.202:2181
ZK-3: 192.168.1.203:2181
```

**å‰¯æœ¬é…ç½®**ï¼š
```bash
# å‰¯æœ¬å› å­å’ŒISRé…ç½®
default.replication.factor=3
min.insync.replicas=2
unclean.leader.election.enable=false
```

**ç½‘ç»œå’Œå­˜å‚¨**ï¼š
- è·¨æœºæ¶éƒ¨ç½²ï¼Œé¿å…å•ç‚¹æ•…éšœ
- ä½¿ç”¨ä¸“ç”¨ç½‘ç»œï¼Œä¿è¯ç½‘ç»œç¨³å®šæ€§
- å¤šç£ç›˜RAIDé…ç½®ï¼Œæé«˜å­˜å‚¨å¯é æ€§
- å®šæœŸå¤‡ä»½é‡è¦æ•°æ®

**ç›‘æ§å‘Šè­¦**ï¼š
```java
// å…³é”®æŒ‡æ ‡ç›‘æ§
- Brokerå­˜æ´»çŠ¶æ€
- åˆ†åŒºLeaderåˆ†å¸ƒ
- ISRå‰¯æœ¬çŠ¶æ€
- æ¶ˆæ¯ç§¯å‹æƒ…å†µ
- ç£ç›˜ä½¿ç”¨ç‡
- ç½‘ç»œå»¶è¿Ÿ
```

**æ•…éšœæ¢å¤**ï¼š
- åˆ¶å®šæ•…éšœæ¢å¤æµç¨‹
- å®šæœŸæ¼”ç»ƒæ•…éšœåˆ‡æ¢
- å‡†å¤‡å¤‡ç”¨ç¡¬ä»¶èµ„æº
- å»ºç«‹è¿ç»´æ–‡æ¡£

**Q7: å¦‚ä½•è®¾è®¡ä¸€ä¸ªæ”¯æŒç™¾ä¸‡çº§TPSçš„Kafkaæ¶ˆæ¯ç³»ç»Ÿï¼Ÿ**

**A:** ç™¾ä¸‡çº§TPS Kafkaç³»ç»Ÿè®¾è®¡ï¼š

**ç¡¬ä»¶é…ç½®**ï¼š
```bash
# æœåŠ¡å™¨é…ç½®ï¼ˆæ¯å°ï¼‰
CPU: 32æ ¸å¿ƒ 2.4GHz
å†…å­˜: 128GB
å­˜å‚¨: 8å—2TB NVMe SSD RAID10
ç½‘ç»œ: åŒä¸‡å…†ç½‘å¡ç»‘å®š
```

**é›†ç¾¤è§„æ¨¡**ï¼š
```bash
# é›†ç¾¤é…ç½®
Brokeræ•°é‡: 12å°
ZooKeeper: 5å°
åˆ†åŒºæ€»æ•°: 1000+
å‰¯æœ¬å› å­: 3
```

**Topicè®¾è®¡**ï¼š
```java
// é«˜å¹¶å‘Topicé…ç½®
åˆ†åŒºæ•°: 100ä¸ªåˆ†åŒºï¼ˆæ”¯æŒ100ä¸ªå¹¶å‘æ¶ˆè´¹è€…ï¼‰
å‰¯æœ¬å› å­: 3
å‹ç¼©ç±»å‹: lz4
æ¸…ç†ç­–ç•¥: delete
ä¿ç•™æ—¶é—´: 3å¤©
```

**ç”Ÿäº§è€…ä¼˜åŒ–**ï¼š
```java
// é«˜æ€§èƒ½ç”Ÿäº§è€…é…ç½®
props.put(ProducerConfig.BATCH_SIZE_CONFIG, 131072);       // 128KBæ‰¹æ¬¡
props.put(ProducerConfig.LINGER_MS_CONFIG, 50);            // 50msç­‰å¾…
props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 268435456); // 256MBç¼“å†²åŒº
props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");
props.put(ProducerConfig.ACKS_CONFIG, "1");                // å¹³è¡¡æ€§èƒ½å’Œå¯é æ€§
```

**æ¶ˆè´¹è€…ä¼˜åŒ–**ï¼š
```java
// é«˜æ€§èƒ½æ¶ˆè´¹è€…é…ç½®
props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1048576);    // 1MBæœ€å°æ‹‰å–
props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 5000);      // 5000æ¡è®°å½•
props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 10485760); // 10MBåˆ†åŒºæ‹‰å–
```

**ç³»ç»Ÿä¼˜åŒ–**ï¼š
```bash
# æ“ä½œç³»ç»Ÿä¼˜åŒ–
vm.swappiness=1
vm.dirty_ratio=80
net.core.rmem_max=134217728
net.core.wmem_max=134217728

# JVMä¼˜åŒ–
-Xmx12g -Xms12g
-XX:+UseG1GC
-XX:MaxGCPauseMillis=20
```

### 6.4 æ•…éšœæ’æŸ¥ä¸è¿ç»´é¢˜

**Q8: Kafkaé›†ç¾¤å‡ºç°æ¶ˆæ¯ç§¯å‹ï¼Œå¦‚ä½•æ’æŸ¥å’Œè§£å†³ï¼Ÿ**

**A:** æ¶ˆæ¯ç§¯å‹æ’æŸ¥å’Œè§£å†³æ–¹æ¡ˆï¼š

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥æ¶ˆè´¹è€…ç»„çŠ¶æ€
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group my-group --describe

# 2. æŸ¥çœ‹Topicåˆ†åŒºçŠ¶æ€
kafka-topics.sh --bootstrap-server localhost:9092 \
  --topic my-topic --describe

# 3. æ£€æŸ¥Brokeræ€§èƒ½æŒ‡æ ‡
# é€šè¿‡JMXç›‘æ§æˆ–Kafka ManageræŸ¥çœ‹
```

**å¸¸è§åŸå› åŠè§£å†³æ–¹æ¡ˆ**ï¼š

**æ¶ˆè´¹è€…æ€§èƒ½ä¸è¶³**ï¼š
```java
// è§£å†³æ–¹æ¡ˆï¼š
1. å¢åŠ æ¶ˆè´¹è€…å®ä¾‹æ•°é‡
2. ä¼˜åŒ–æ¶ˆè´¹è€…é…ç½®
props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 1000);
props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 50000);

3. å¹¶è¡Œå¤„ç†æ¶ˆæ¯
ExecutorService executor = Executors.newFixedThreadPool(10);
for (ConsumerRecord<String, String> record : records) {
    executor.submit(() -> processMessage(record));
}
```

**åˆ†åŒºæ•°é‡ä¸è¶³**ï¼š
```bash
# å¢åŠ åˆ†åŒºæ•°é‡
kafka-topics.sh --bootstrap-server localhost:9092 \
  --topic my-topic --alter --partitions 20
```

**æ¶ˆè´¹è€…å¤„ç†é€»è¾‘æ…¢**ï¼š
```java
// ä¼˜åŒ–å¤„ç†é€»è¾‘
1. å¼‚æ­¥å¤„ç†éå…³é”®é€»è¾‘
2. æ‰¹é‡å¤„ç†æ•°æ®åº“æ“ä½œ
3. ä½¿ç”¨ç¼“å­˜å‡å°‘å¤–éƒ¨è°ƒç”¨
4. ä¼˜åŒ–ç®—æ³•å’Œæ•°æ®ç»“æ„
```

**ç½‘ç»œæˆ–ç£ç›˜I/Oç“¶é¢ˆ**ï¼š
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
iostat -x 1
sar -n DEV 1
top -p $(pgrep java)

# ä¼˜åŒ–å»ºè®®
1. å‡çº§ç¡¬ä»¶é…ç½®
2. ä¼˜åŒ–ç½‘ç»œé…ç½®
3. ä½¿ç”¨SSDå­˜å‚¨
4. è°ƒæ•´JVMå‚æ•°
```

**Q9: å¦‚ä½•ç›‘æ§Kafkaé›†ç¾¤çš„å¥åº·çŠ¶æ€ï¼Ÿ**

**A:** Kafkaé›†ç¾¤ç›‘æ§ä½“ç³»ï¼š

**å…³é”®æŒ‡æ ‡ç›‘æ§**ï¼š
```java
// JMXæŒ‡æ ‡ç›‘æ§
1. ååé‡æŒ‡æ ‡
   - MessagesInPerSec: æ¶ˆæ¯è¾“å…¥é€Ÿç‡
   - BytesInPerSec: å­—èŠ‚è¾“å…¥é€Ÿç‡
   - BytesOutPerSec: å­—èŠ‚è¾“å‡ºé€Ÿç‡

2. å»¶è¿ŸæŒ‡æ ‡
   - ProduceRequestTime: ç”Ÿäº§è¯·æ±‚æ—¶é—´
   - FetchRequestTime: æ‹‰å–è¯·æ±‚æ—¶é—´
   - NetworkProcessorAvgIdlePercent: ç½‘ç»œå¤„ç†å™¨ç©ºé—²ç‡

3. å¯ç”¨æ€§æŒ‡æ ‡
   - UnderReplicatedPartitions: æœªå……åˆ†å¤åˆ¶çš„åˆ†åŒº
   - OfflinePartitionsCount: ç¦»çº¿åˆ†åŒºæ•°é‡
   - ActiveControllerCount: æ´»è·ƒæ§åˆ¶å™¨æ•°é‡

4. èµ„æºæŒ‡æ ‡
   - JVMå †å†…å­˜ä½¿ç”¨ç‡
   - GCé¢‘ç‡å’Œæ—¶é—´
   - ç£ç›˜ä½¿ç”¨ç‡
   - ç½‘ç»œI/O
```

**ç›‘æ§å·¥å…·é€‰æ‹©**ï¼š
```bash
# å¼€æºç›‘æ§æ–¹æ¡ˆ
1. Kafka Manager (CMAK)
2. Kafka Eagle
3. Prometheus + Grafana
4. ELK Stack

# å•†ä¸šç›‘æ§æ–¹æ¡ˆ
1. Confluent Control Center
2. DataDog
3. New Relic
4. AppDynamics
```

**å‘Šè­¦é…ç½®**ï¼š
```yaml
# Prometheuså‘Šè­¦è§„åˆ™ç¤ºä¾‹
groups:
- name: kafka.rules
  rules:
  - alert: KafkaBrokerDown
    expr: up{job="kafka"} == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Kafka broker is down"
      
  - alert: KafkaUnderReplicatedPartitions
    expr: kafka_server_replicamanager_underreplicatedpartitions > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Kafka has under-replicated partitions"
```

## 7. Kafkaå®æˆ˜é¡¹ç›®æ¡ˆä¾‹

### 7.1 ç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿ

è¿™æ˜¯ä¸€ä¸ªåŸºäº Kafka çš„å®Œæ•´ç”µå•†è®¢å•å¤„ç†ç³»ç»Ÿï¼Œå±•ç¤ºäº† Kafka åœ¨å®é™…ä¸šåŠ¡åœºæ™¯ä¸­çš„åº”ç”¨ã€‚

#### 7.1.1 ç³»ç»Ÿæ¶æ„è®¾è®¡

```mermaid
graph TB
    subgraph "å‰ç«¯å±‚"
        A[ç”¨æˆ·ä¸‹å•] --> B[è®¢å•æœåŠ¡]
        C[æ”¯ä»˜æœåŠ¡] --> D[åº“å­˜æœåŠ¡]
    end
    
    subgraph "Kafkaæ¶ˆæ¯å±‚"
        E[è®¢å•äº‹ä»¶Topic]
        F[æ”¯ä»˜äº‹ä»¶Topic] 
        G[åº“å­˜äº‹ä»¶Topic]
        H[é€šçŸ¥äº‹ä»¶Topic]
    end
    
    subgraph "ä¸šåŠ¡å¤„ç†å±‚"
        I[è®¢å•å¤„ç†å™¨]
        J[æ”¯ä»˜å¤„ç†å™¨]
        K[åº“å­˜å¤„ç†å™¨]
        L[é€šçŸ¥å¤„ç†å™¨]
    end
    
    subgraph "æ•°æ®å­˜å‚¨å±‚"
        M[è®¢å•æ•°æ®åº“]
        N[åº“å­˜æ•°æ®åº“]
        O[ç”¨æˆ·æ•°æ®åº“]
    end
    
    B --> E
    C --> F
    D --> G
    
    E --> I
    F --> J
    G --> K
    H --> L
    
    I --> M
    J --> M
    K --> N
    L --> O
```

#### 7.1.2 é¡¹ç›®ç»“æ„

```
ecommerce-kafka-system/
â”œâ”€â”€ pom.xml
â”œâ”€â”€ src/main/java/com/ecommerce/
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ KafkaConfig.java
â”‚   â”‚   â””â”€â”€ DatabaseConfig.java
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ Order.java
â”‚   â”‚   â”œâ”€â”€ Payment.java
â”‚   â”‚   â””â”€â”€ Inventory.java
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â”œâ”€â”€ OrderEventProducer.java
â”‚   â”‚   â””â”€â”€ PaymentEventProducer.java
â”‚   â”œâ”€â”€ consumer/
â”‚   â”‚   â”œâ”€â”€ OrderEventConsumer.java
â”‚   â”‚   â””â”€â”€ PaymentEventConsumer.java
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ OrderService.java
â”‚   â”‚   â””â”€â”€ PaymentService.java
â”‚   â””â”€â”€ controller/
â”‚       â””â”€â”€ OrderController.java
â”œâ”€â”€ src/main/resources/
â”‚   â”œâ”€â”€ application.yml
â”‚   â””â”€â”€ logback-spring.xml
â””â”€â”€ docker-compose.yml
```

#### 7.1.3 æ ¸å¿ƒä»£ç å®ç°

**è®¢å•äº‹ä»¶æ¨¡å‹**ï¼š

```java
@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class OrderEvent {
    private String orderId;
    private String userId;
    private String productId;
    private Integer quantity;
    private BigDecimal amount;
    private OrderStatus status;
    private LocalDateTime timestamp;
    private String correlationId; // ç”¨äºè¿½è¸ªæ¶ˆæ¯é“¾è·¯
    
    public enum OrderStatus {
        CREATED, PAID, SHIPPED, DELIVERED, CANCELLED
    }
}
```

**è®¢å•äº‹ä»¶ç”Ÿäº§è€…**ï¼š

```java
@Component
@Slf4j
public class OrderEventProducer {
    
    @Autowired
    private KafkaTemplate<String, OrderEvent> kafkaTemplate;
    
    @Value("${kafka.topic.order}")
    private String orderTopic;
    
    public void sendOrderEvent(OrderEvent orderEvent) {
        try {
            // è®¾ç½®æ¶ˆæ¯å¤´
            ProducerRecord<String, OrderEvent> record = new ProducerRecord<>(
                orderTopic, 
                orderEvent.getOrderId(), 
                orderEvent
            );
            
            // æ·»åŠ æ¶ˆæ¯å¤´ç”¨äºè¿½è¸ª
            record.headers().add("correlation-id", 
                orderEvent.getCorrelationId().getBytes());
            record.headers().add("event-type", 
                "ORDER_CREATED".getBytes());
            
            // å¼‚æ­¥å‘é€
            kafkaTemplate.send(record).addCallback(
                result -> {
                    if (result != null) {
                        log.info("è®¢å•äº‹ä»¶å‘é€æˆåŠŸ: orderId={}, partition={}, offset={}", 
                            orderEvent.getOrderId(),
                            result.getRecordMetadata().partition(),
                            result.getRecordMetadata().offset());
                    }
                },
                failure -> {
                    log.error("è®¢å•äº‹ä»¶å‘é€å¤±è´¥: orderId={}, error={}", 
                        orderEvent.getOrderId(), failure.getMessage());
                    // å®ç°é‡è¯•æœºåˆ¶
                    retrySendOrderEvent(orderEvent);
                }
            );
            
        } catch (Exception e) {
            log.error("å‘é€è®¢å•äº‹ä»¶å¼‚å¸¸: {}", e.getMessage(), e);
            throw new RuntimeException("å‘é€è®¢å•äº‹ä»¶å¤±è´¥", e);
        }
    }
    
    private void retrySendOrderEvent(OrderEvent orderEvent) {
        // å®ç°æŒ‡æ•°é€€é¿é‡è¯•
        CompletableFuture.delayedExecutor(1, TimeUnit.SECONDS)
            .execute(() -> sendOrderEvent(orderEvent));
    }
}
```

**è®¢å•äº‹ä»¶æ¶ˆè´¹è€…**ï¼š

```java
@Component
@Slf4j
public class OrderEventConsumer {
    
    @Autowired
    private OrderService orderService;
    
    @Autowired
    private PaymentEventProducer paymentEventProducer;
    
    @KafkaListener(
        topics = "${kafka.topic.order}",
        groupId = "order-processing-group",
        containerFactory = "kafkaListenerContainerFactory"
    )
    public void handleOrderEvent(
            @Payload OrderEvent orderEvent,
            @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,
            @Header(KafkaHeaders.RECEIVED_PARTITION_ID) int partition,
            @Header(KafkaHeaders.OFFSET) long offset,
            @Header("correlation-id") String correlationId) {
        
        try {
            log.info("æ¥æ”¶åˆ°è®¢å•äº‹ä»¶: orderId={}, partition={}, offset={}", 
                orderEvent.getOrderId(), partition, offset);
            
            // å¹‚ç­‰æ€§æ£€æŸ¥
            if (orderService.isOrderProcessed(orderEvent.getOrderId())) {
                log.warn("è®¢å•å·²å¤„ç†ï¼Œè·³è¿‡: orderId={}", orderEvent.getOrderId());
                return;
            }
            
            // ä¸šåŠ¡å¤„ç†
            processOrderEvent(orderEvent);
            
            // æ‰‹åŠ¨æäº¤åç§»é‡
            // æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…é…ç½®å†³å®šæ˜¯å¦æ‰‹åŠ¨æäº¤
            
        } catch (Exception e) {
            log.error("å¤„ç†è®¢å•äº‹ä»¶å¤±è´¥: orderId={}, error={}", 
                orderEvent.getOrderId(), e.getMessage(), e);
            
            // å®ç°æ­»ä¿¡é˜Ÿåˆ—å¤„ç†
            handleFailedOrderEvent(orderEvent, e);
        }
    }
    
    private void processOrderEvent(OrderEvent orderEvent) {
        // 1. éªŒè¯è®¢å•
        validateOrder(orderEvent);
        
        // 2. æ£€æŸ¥åº“å­˜
        checkInventory(orderEvent);
        
        // 3. åˆ›å»ºæ”¯ä»˜äº‹ä»¶
        PaymentEvent paymentEvent = PaymentEvent.builder()
            .orderId(orderEvent.getOrderId())
            .amount(orderEvent.getAmount())
            .userId(orderEvent.getUserId())
            .correlationId(orderEvent.getCorrelationId())
            .build();
        
        // 4. å‘é€æ”¯ä»˜äº‹ä»¶
        paymentEventProducer.sendPaymentEvent(paymentEvent);
        
        // 5. æ›´æ–°è®¢å•çŠ¶æ€
        orderService.updateOrderStatus(orderEvent.getOrderId(), 
            OrderEvent.OrderStatus.PENDING_PAYMENT);
    }
    
    private void handleFailedOrderEvent(OrderEvent orderEvent, Exception e) {
        // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—æˆ–é”™è¯¯å¤„ç†Topic
        log.error("è®¢å•å¤„ç†å¤±è´¥ï¼Œå‘é€åˆ°é”™è¯¯å¤„ç†é˜Ÿåˆ—: orderId={}", 
            orderEvent.getOrderId());
    }
}
```

#### 7.1.4 é…ç½®ç®¡ç†

**application.yml**ï¼š

```yaml
spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
      acks: all
      retries: 3
      batch-size: 16384
      linger-ms: 5
      buffer-memory: 33554432
      properties:
        enable.idempotence: true
        max.in.flight.requests.per.connection: 5
    consumer:
      group-id: ecommerce-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      auto-offset-reset: earliest
      enable-auto-commit: false
      properties:
        spring.json.trusted.packages: "com.ecommerce.model"
        max.poll.records: 500
        session.timeout.ms: 30000
        heartbeat.interval.ms: 10000
    listener:
      ack-mode: manual_immediate
      concurrency: 3
      missing-topics-fatal: false

kafka:
  topic:
    order: order-events
    payment: payment-events
    inventory: inventory-events
    notification: notification-events
    dlq: dead-letter-queue
```

#### 7.1.5 Dockeréƒ¨ç½²é…ç½®

**docker-compose.yml**ï¼š

```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092

  mysql:
    image: mysql:8.0
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: ecommerce
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql

volumes:
  mysql_data:
```

#### 7.1.6 ç›‘æ§å’Œå‘Šè­¦

**Prometheusé…ç½®**ï¼š

```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka:9092']
    metrics_path: /metrics
    scrape_interval: 5s

  - job_name: 'kafka-jmx'
    static_configs:
      - targets: ['kafka:9101']
    scrape_interval: 5s
```

**Grafanaä»ªè¡¨æ¿**ï¼š

```json
{
  "dashboard": {
    "title": "Kafkaç”µå•†ç³»ç»Ÿç›‘æ§",
    "panels": [
      {
        "title": "æ¶ˆæ¯ç”Ÿäº§é€Ÿç‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(kafka_server_brokertopicmetrics_messagesin_total[5m])",
            "legendFormat": "{{topic}}"
          }
        ]
      },
      {
        "title": "æ¶ˆè´¹è€…å»¶è¿Ÿ",
        "type": "graph", 
        "targets": [
          {
            "expr": "kafka_consumer_lag_sum",
            "legendFormat": "{{consumer_group}}"
          }
        ]
      }
    ]
  }
}
```

### 7.2 æ€§èƒ½æµ‹è¯•å®è·µ

#### 7.2.1 å‹åŠ›æµ‹è¯•è„šæœ¬

```bash
#!/bin/bash
# kafka-performance-test.sh

# æµ‹è¯•ç”Ÿäº§è€…æ€§èƒ½
kafka-producer-perf-test.sh \
  --topic order-events \
  --num-records 1000000 \
  --record-size 1024 \
  --throughput 10000 \
  --producer-props bootstrap.servers=localhost:9092 \
  --producer-props acks=all \
  --producer-props retries=3

# æµ‹è¯•æ¶ˆè´¹è€…æ€§èƒ½  
kafka-consumer-perf-test.sh \
  --topic order-events \
  --bootstrap-server localhost:9092 \
  --messages 1000000 \
  --threads 4
```

#### 7.2.2 JMeteræµ‹è¯•è®¡åˆ’

```xml
<?xml version="1.0" encoding="UTF-8"?>
<jmeterTestPlan version="1.2">
  <hashTree>
    <TestPlan testname="Kafkaæ€§èƒ½æµ‹è¯•">
      <elementProp name="TestPlan.arguments" elementType="Arguments" guiclass="ArgumentsPanel">
        <collectionProp name="Arguments.arguments"/>
      </elementProp>
      <stringProp name="TestPlan.user_define_classpath"></stringProp>
      <boolProp name="TestPlan.functional_mode">false</boolProp>
      <boolProp name="TestPlan.tearDown_on_shutdown">true</boolProp>
      <boolProp name="TestPlan.serialize_threadgroups">false</boolProp>
    </TestPlan>
  </hashTree>
</jmeterTestPlan>
```

### 7.3 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

#### 7.3.1 é›†ç¾¤è§„åˆ’

**ç¡¬ä»¶é…ç½®å»ºè®®**ï¼š

| ç»„ä»¶ | CPU | å†…å­˜ | ç£ç›˜ | ç½‘ç»œ | è¯´æ˜ |
|------|-----|------|------|------|------|
| **Kafka Broker** | 8æ ¸+ | 32GB+ | SSD 1TB+ | ä¸‡å…† | é«˜æ€§èƒ½å­˜å‚¨å’Œç½‘ç»œ |
| **Zookeeper** | 4æ ¸+ | 8GB+ | SSD 100GB+ | åƒå…† | å…ƒæ•°æ®å­˜å‚¨ |
| **ç›‘æ§èŠ‚ç‚¹** | 4æ ¸+ | 16GB+ | SSD 500GB+ | åƒå…† | Prometheus/Grafana |

**é›†ç¾¤è§„æ¨¡è§„åˆ’**ï¼š

```yaml
# ç”Ÿäº§ç¯å¢ƒé›†ç¾¤é…ç½®
production-cluster:
  brokers: 3-5ä¸ªèŠ‚ç‚¹
  zookeeper: 3ä¸ªèŠ‚ç‚¹ï¼ˆå¥‡æ•°ï¼‰
  partitions: æ ¹æ®ä¸šåŠ¡é‡è®¡ç®—
  replication-factor: 3
  min-insync-replicas: 2
```

#### 7.3.2 é…ç½®æ–‡ä»¶ä¼˜åŒ–

**server.properties ç”Ÿäº§é…ç½®**ï¼š

```properties
# åŸºç¡€é…ç½®
broker.id=1
listeners=PLAINTEXT://0.0.0.0:9092
advertised.listeners=PLAINTEXT://kafka-1.example.com:9092

# æ—¥å¿—é…ç½®
log.dirs=/data/kafka-logs
num.partitions=3
default.replication.factor=3
min.insync.replicas=2

# æ€§èƒ½ä¼˜åŒ–
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

# æ—¥å¿—æ®µé…ç½®
log.segment.bytes=1073741824
log.retention.hours=168
log.retention.bytes=107374182400
log.cleanup.policy=delete

# å‹ç¼©é…ç½®
compression.type=snappy
message.max.bytes=1000000
replica.fetch.max.bytes=1048576

# å‰¯æœ¬é…ç½®
replica.lag.time.max.ms=10000
replica.fetch.wait.max.ms=500
replica.socket.timeout.ms=30000

# æ§åˆ¶å™¨é…ç½®
controller.socket.timeout.ms=30000
```

#### 7.3.3 å®‰å…¨é…ç½®

**SSL/TLS åŠ å¯†é…ç½®**ï¼š

```properties
# SSLé…ç½®
listeners=SSL://0.0.0.0:9093
security.inter.broker.protocol=SSL
ssl.keystore.location=/opt/kafka/ssl/kafka.server.keystore.jks
ssl.keystore.password=keystore_password
ssl.key.password=key_password
ssl.truststore.location=/opt/kafka/ssl/kafka.server.truststore.jks
ssl.truststore.password=truststore_password
ssl.client.auth=required
```

**SASLè®¤è¯é…ç½®**ï¼š

```properties
# SASLé…ç½®
listeners=SASL_SSL://0.0.0.0:9094
security.inter.broker.protocol=SASL_SSL
sasl.mechanism.inter.broker.protocol=PLAIN
sasl.enabled.mechanisms=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
  username="admin" \
  password="admin-secret" \
  user_admin="admin-secret" \
  user_alice="alice-secret";
```

#### 7.3.4 ç›‘æ§é…ç½®

**JMXç›‘æ§é…ç½®**ï¼š

```properties
# JMXé…ç½®
jmx.port=9999
kafka.jmx.port=9999
```

**Prometheus JMX Exporteré…ç½®**ï¼š

```yaml
# jmx_prometheus_javaagent.yml
startDelaySeconds: 0
ssl: false
lowercaseOutputName: false
lowercaseOutputLabelNames: false
rules:
  - pattern: kafka.server<type=BrokerTopicMetrics, name=MessagesInPerSec, topic=(.+)><>Count
    name: kafka_server_brokertopicmetrics_messagesin_total
    labels:
      topic: "$1"
    type: COUNTER
  - pattern: kafka.server<type=BrokerTopicMetrics, name=BytesInPerSec, topic=(.+)><>Count
    name: kafka_server_brokertopicmetrics_bytesin_total
    labels:
      topic: "$1"
    type: COUNTER
```

### 7.4 è¿ç»´ç®¡ç†å®è·µ

#### 7.4.1 æ—¥å¸¸è¿ç»´è„šæœ¬

**é›†ç¾¤å¥åº·æ£€æŸ¥è„šæœ¬**ï¼š

```bash
#!/bin/bash
# kafka-health-check.sh

KAFKA_HOME="/opt/kafka"
BROKER_LIST="localhost:9092"

echo "=== Kafkaé›†ç¾¤å¥åº·æ£€æŸ¥ ==="

# æ£€æŸ¥BrokerçŠ¶æ€
echo "1. æ£€æŸ¥BrokerçŠ¶æ€..."
$KAFKA_HOME/bin/kafka-broker-api-versions.sh --bootstrap-server $BROKER_LIST

# æ£€æŸ¥Topicåˆ—è¡¨
echo "2. æ£€æŸ¥Topicåˆ—è¡¨..."
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server $BROKER_LIST --list

# æ£€æŸ¥æ¶ˆè´¹è€…ç»„
echo "3. æ£€æŸ¥æ¶ˆè´¹è€…ç»„..."
$KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server $BROKER_LIST --list

# æ£€æŸ¥åˆ†åŒºçŠ¶æ€
echo "4. æ£€æŸ¥åˆ†åŒºçŠ¶æ€..."
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server $BROKER_LIST --describe

# æ£€æŸ¥æ—¥å¿—å¤§å°
echo "5. æ£€æŸ¥æ—¥å¿—å¤§å°..."
du -sh /data/kafka-logs/*

# æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
echo "6. æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡..."
df -h /data

echo "=== å¥åº·æ£€æŸ¥å®Œæˆ ==="
```

**è‡ªåŠ¨å¤‡ä»½è„šæœ¬**ï¼š

```bash
#!/bin/bash
# kafka-backup.sh

BACKUP_DIR="/backup/kafka"
DATE=$(date +%Y%m%d_%H%M%S)
KAFKA_HOME="/opt/kafka"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR/$DATE

# å¤‡ä»½é…ç½®æ–‡ä»¶
cp -r $KAFKA_HOME/config $BACKUP_DIR/$DATE/

# å¤‡ä»½Topicå…ƒæ•°æ®
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list > $BACKUP_DIR/$DATE/topics.txt

# å¤‡ä»½æ¶ˆè´¹è€…ç»„ä¿¡æ¯
$KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list > $BACKUP_DIR/$DATE/consumer-groups.txt

# å‹ç¼©å¤‡ä»½
tar -czf $BACKUP_DIR/kafka-backup-$DATE.tar.gz -C $BACKUP_DIR $DATE

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™7å¤©ï¼‰
find $BACKUP_DIR -name "kafka-backup-*.tar.gz" -mtime +7 -delete

echo "å¤‡ä»½å®Œæˆ: kafka-backup-$DATE.tar.gz"
```

#### 7.4.2 æ•…éšœæ’æŸ¥æŒ‡å—

**å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**ï¼š

| é—®é¢˜ | ç—‡çŠ¶ | åŸå›  | è§£å†³æ–¹æ¡ˆ |
|------|------|------|----------|
| **Leaderä¸å¯ç”¨** | åˆ†åŒºæ— Leader | Brokerå®•æœº | é‡å¯Brokeræˆ–é‡æ–°åˆ†é…åˆ†åŒº |
| **æ¶ˆæ¯ä¸¢å¤±** | æ¶ˆè´¹è€…æ”¶ä¸åˆ°æ¶ˆæ¯ | å‰¯æœ¬ä¸è¶³ | å¢åŠ å‰¯æœ¬å› å­ |
| **æ€§èƒ½ä¸‹é™** | ååé‡é™ä½ | ç£ç›˜IOç“¶é¢ˆ | ä¼˜åŒ–ç£ç›˜é…ç½® |
| **å†…å­˜æº¢å‡º** | Brokerå´©æºƒ | å†…å­˜ä¸è¶³ | è°ƒæ•´JVMå‚æ•° |

**æ•…éšœæ’æŸ¥è„šæœ¬**ï¼š

```bash
#!/bin/bash
# kafka-troubleshoot.sh

echo "=== Kafkaæ•…éšœæ’æŸ¥ ==="

# 1. æ£€æŸ¥BrokerçŠ¶æ€
echo "1. æ£€æŸ¥BrokerçŠ¶æ€..."
$KAFKA_HOME/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092

# 2. æ£€æŸ¥åˆ†åŒºçŠ¶æ€
echo "2. æ£€æŸ¥åˆ†åŒºçŠ¶æ€..."
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --unavailable-partitions

# 3. æ£€æŸ¥å‰¯æœ¬çŠ¶æ€
echo "3. æ£€æŸ¥å‰¯æœ¬çŠ¶æ€..."
$KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --under-replicated-partitions

# 4. æ£€æŸ¥æ¶ˆè´¹è€…å»¶è¿Ÿ
echo "4. æ£€æŸ¥æ¶ˆè´¹è€…å»¶è¿Ÿ..."
$KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --all-groups

# 5. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶
echo "5. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶..."
tail -n 100 $KAFKA_HOME/logs/server.log | grep ERROR

# 6. æ£€æŸ¥ç³»ç»Ÿèµ„æº
echo "6. æ£€æŸ¥ç³»ç»Ÿèµ„æº..."
echo "CPUä½¿ç”¨ç‡:"
top -bn1 | grep "Cpu(s)"
echo "å†…å­˜ä½¿ç”¨ç‡:"
free -h
echo "ç£ç›˜ä½¿ç”¨ç‡:"
df -h

echo "=== æ•…éšœæ’æŸ¥å®Œæˆ ==="
```

#### 7.4.3 æ€§èƒ½è°ƒä¼˜å®è·µ

**JVMå‚æ•°ä¼˜åŒ–**ï¼š

```bash
# kafka-server-start.sh
export KAFKA_HEAP_OPTS="-Xmx8g -Xms8g"
export KAFKA_JVM_PERFORMANCE_OPTS="-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true"
```

**ç³»ç»Ÿå‚æ•°ä¼˜åŒ–**ï¼š

```bash
# /etc/sysctl.conf
# ç½‘ç»œå‚æ•°
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 65536 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728

# æ–‡ä»¶æè¿°ç¬¦
fs.file-max = 1000000

# è™šæ‹Ÿå†…å­˜
vm.swappiness = 1
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
```

### 7.5 ä¸å…¶ä»–æŠ€æœ¯æ ˆé›†æˆ

#### 7.5.1 Spring Booté›†æˆ

**Mavenä¾èµ–**ï¼š

```xml
<dependencies>
    <dependency>
        <groupId>org.springframework.kafka</groupId>
        <artifactId>spring-kafka</artifactId>
        <version>2.9.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-streams</artifactId>
        <version>3.4.0</version>
    </dependency>
</dependencies>
```

**é…ç½®ç±»**ï¼š

```java
@Configuration
@EnableKafka
public class KafkaConfig {
    
    @Bean
    public ProducerFactory<String, Object> producerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        configProps.put(ProducerConfig.ACKS_CONFIG, "all");
        configProps.put(ProducerConfig.RETRIES_CONFIG, 3);
        configProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        return new DefaultKafkaProducerFactory<>(configProps);
    }
    
    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate() {
        return new KafkaTemplate<>(producerFactory());
    }
}
```

#### 7.5.2 ä¸æ•°æ®åº“é›†æˆ

**Kafka Connecté…ç½®**ï¼š

```json
{
  "name": "mysql-source-connector",
  "config": {
    "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url": "jdbc:mysql://localhost:3306/ecommerce",
    "connection.user": "root",
    "connection.password": "password",
    "table.whitelist": "orders",
    "mode": "incrementing",
    "incrementing.column.name": "id",
    "topic.prefix": "mysql-",
    "poll.interval.ms": 1000
  }
}
```

#### 7.5.3 ä¸Elasticsearché›†æˆ

**Logstashé…ç½®**ï¼š

```ruby
input {
  kafka {
    bootstrap_servers => "localhost:9092"
    topics => ["order-events"]
    group_id => "logstash"
    consumer_threads => 4
  }
}

filter {
  json {
    source => "message"
  }
  
  date {
    match => ["timestamp", "ISO8601"]
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "kafka-events-%{+YYYY.MM.dd}"
  }
}
```

### 7.6 é«˜çº§é”™è¯¯å¤„ç†ä¸å®¹é”™æœºåˆ¶

#### 7.6.1 ç”Ÿäº§è€…é”™è¯¯å¤„ç†

**é‡è¯•æœºåˆ¶å®ç°**ï¼š

```java
@Component
@Slf4j
public class RobustKafkaProducer {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final RetryTemplate retryTemplate;
    
    public RobustKafkaProducer(KafkaTemplate<String, Object> kafkaTemplate) {
        this.kafkaTemplate = kafkaTemplate;
        this.retryTemplate = createRetryTemplate();
    }
    
    private RetryTemplate createRetryTemplate() {
        RetryTemplate template = new RetryTemplate();
        
        // é‡è¯•ç­–ç•¥ï¼šæŒ‡æ•°é€€é¿
        ExponentialBackOffPolicy backOffPolicy = new ExponentialBackOffPolicy();
        backOffPolicy.setInitialInterval(1000);
        backOffPolicy.setMultiplier(2.0);
        backOffPolicy.setMaxInterval(10000);
        template.setBackOffPolicy(backOffPolicy);
        
        // é‡è¯•æ¡ä»¶ï¼šå¯é‡è¯•çš„å¼‚å¸¸
        SimpleRetryPolicy retryPolicy = new SimpleRetryPolicy(3, 
            Map.of(
                RetriableException.class, true,
                TimeoutException.class, true,
                NetworkException.class, true
            ));
        template.setRetryPolicy(retryPolicy);
        
        return template;
    }
    
    public void sendMessageWithRetry(String topic, String key, Object message) {
        try {
            retryTemplate.execute(context -> {
                try {
                    ListenableFuture<SendResult<String, Object>> future = 
                        kafkaTemplate.send(topic, key, message);
                    
                    return future.get(5, TimeUnit.SECONDS);
                } catch (Exception e) {
                    log.warn("å‘é€æ¶ˆæ¯å¤±è´¥ï¼Œé‡è¯•ç¬¬{}æ¬¡: {}", 
                        context.getRetryCount() + 1, e.getMessage());
                    throw new RetriableException("æ¶ˆæ¯å‘é€å¤±è´¥", e);
                }
            });
        } catch (Exception e) {
            log.error("æ¶ˆæ¯å‘é€æœ€ç»ˆå¤±è´¥: topic={}, key={}, error={}", 
                topic, key, e.getMessage());
            // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
            sendToDeadLetterQueue(topic, key, message, e);
        }
    }
    
    private void sendToDeadLetterQueue(String topic, String key, 
                                     Object message, Exception error) {
        DeadLetterMessage dlqMessage = DeadLetterMessage.builder()
            .originalTopic(topic)
            .originalKey(key)
            .originalMessage(message)
            .errorMessage(error.getMessage())
            .timestamp(Instant.now())
            .build();
        
        kafkaTemplate.send("dlq-topic", key, dlqMessage);
    }
}
```

**æ­»ä¿¡é˜Ÿåˆ—å¤„ç†**ï¼š

```java
@Component
@Slf4j
public class DeadLetterQueueHandler {
    
    @KafkaListener(topics = "dlq-topic", groupId = "dlq-handler")
    public void handleDeadLetterMessage(@Payload DeadLetterMessage dlqMessage) {
        log.error("å¤„ç†æ­»ä¿¡æ¶ˆæ¯: topic={}, key={}, error={}", 
            dlqMessage.getOriginalTopic(),
            dlqMessage.getOriginalKey(),
            dlqMessage.getErrorMessage());
        
        // 1. è®°å½•åˆ°æ•°æ®åº“
        saveDeadLetterRecord(dlqMessage);
        
        // 2. å‘é€å‘Šè­¦
        sendAlert(dlqMessage);
        
        // 3. å°è¯•ä¿®å¤æˆ–äººå·¥å¤„ç†
        attemptRecovery(dlqMessage);
    }
    
    private void saveDeadLetterRecord(DeadLetterMessage dlqMessage) {
        // ä¿å­˜åˆ°æ•°æ®åº“ç”¨äºåç»­åˆ†æ
        DeadLetterRecord record = DeadLetterRecord.builder()
            .originalTopic(dlqMessage.getOriginalTopic())
            .originalKey(dlqMessage.getOriginalKey())
            .errorMessage(dlqMessage.getErrorMessage())
            .timestamp(dlqMessage.getTimestamp())
            .status(DeadLetterStatus.PENDING)
            .build();
        
        deadLetterRepository.save(record);
    }
    
    private void sendAlert(DeadLetterMessage dlqMessage) {
        AlertMessage alert = AlertMessage.builder()
            .level(AlertLevel.ERROR)
            .title("Kafkaæ¶ˆæ¯å‘é€å¤±è´¥")
            .content(String.format("Topic: %s, Key: %s, Error: %s", 
                dlqMessage.getOriginalTopic(),
                dlqMessage.getOriginalKey(),
                dlqMessage.getErrorMessage()))
            .timestamp(Instant.now())
            .build();
        
        alertService.sendAlert(alert);
    }
}
```

#### 7.6.2 æ¶ˆè´¹è€…é”™è¯¯å¤„ç†

**æ¶ˆè´¹è€…å®¹é”™æœºåˆ¶**ï¼š

```java
@Component
@Slf4j
public class FaultTolerantConsumer {
    
    private final OrderService orderService;
    private final CircuitBreaker circuitBreaker;
    
    public FaultTolerantConsumer(OrderService orderService) {
        this.orderService = orderService;
        this.circuitBreaker = createCircuitBreaker();
    }
    
    private CircuitBreaker createCircuitBreaker() {
        return CircuitBreaker.ofDefaults("order-processing")
            .toBuilder()
            .failureRateThreshold(50)
            .waitDurationInOpenState(Duration.ofSeconds(30))
            .slidingWindowSize(10)
            .build();
    }
    
    @KafkaListener(topics = "order-events", groupId = "order-processor")
    public void handleOrderEvent(@Payload OrderEvent orderEvent,
                               Acknowledgment acknowledgment) {
        try {
            // ä½¿ç”¨æ–­è·¯å™¨ä¿æŠ¤
            circuitBreaker.executeSupplier(() -> {
                processOrderEvent(orderEvent);
                return null;
            });
            
            // å¤„ç†æˆåŠŸï¼Œæäº¤åç§»é‡
            acknowledgment.acknowledge();
            
        } catch (Exception e) {
            log.error("å¤„ç†è®¢å•äº‹ä»¶å¤±è´¥: orderId={}, error={}", 
                orderEvent.getOrderId(), e.getMessage(), e);
            
            // æ ¹æ®å¼‚å¸¸ç±»å‹å†³å®šå¤„ç†ç­–ç•¥
            handleConsumerError(orderEvent, e, acknowledgment);
        }
    }
    
    private void handleConsumerError(OrderEvent orderEvent, Exception error, 
                                   Acknowledgment acknowledgment) {
        if (isRetryableError(error)) {
            // å¯é‡è¯•é”™è¯¯ï¼šå»¶è¿Ÿé‡è¯•
            scheduleRetry(orderEvent, error);
            // ä¸æäº¤åç§»é‡ï¼Œç­‰å¾…é‡è¯•
        } else if (isBusinessError(error)) {
            // ä¸šåŠ¡é”™è¯¯ï¼šè®°å½•å¹¶è·³è¿‡
            logBusinessError(orderEvent, error);
            acknowledgment.acknowledge();
        } else {
            // ç³»ç»Ÿé”™è¯¯ï¼šå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
            sendToDeadLetterQueue(orderEvent, error);
            acknowledgment.acknowledge();
        }
    }
    
    private boolean isRetryableError(Exception error) {
        return error instanceof TimeoutException ||
               error instanceof ConnectException ||
               error instanceof SocketTimeoutException;
    }
    
    private boolean isBusinessError(Exception error) {
        return error instanceof ValidationException ||
               error instanceof BusinessLogicException;
    }
    
    private void scheduleRetry(OrderEvent orderEvent, Exception error) {
        // ä½¿ç”¨å»¶è¿Ÿé˜Ÿåˆ—å®ç°é‡è¯•
        RetryMessage retryMessage = RetryMessage.builder()
            .originalMessage(orderEvent)
            .retryCount(0)
            .maxRetries(3)
            .nextRetryTime(Instant.now().plusSeconds(30))
            .build();
        
        kafkaTemplate.send("retry-topic", orderEvent.getOrderId(), retryMessage);
    }
}
```

#### 7.6.3 æ¶ˆæ¯å»é‡ä¸å¹‚ç­‰æ€§

**å¹‚ç­‰æ€§ä¿è¯**ï¼š

```java
@Component
@Slf4j
public class IdempotentMessageProcessor {
    
    private final RedisTemplate<String, String> redisTemplate;
    private final OrderService orderService;
    
    @KafkaListener(topics = "order-events", groupId = "idempotent-processor")
    public void processOrderEvent(@Payload OrderEvent orderEvent,
                                @Header("message-id") String messageId) {
        
        // 1. æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²å¤„ç†
        if (isMessageProcessed(messageId)) {
            log.info("æ¶ˆæ¯å·²å¤„ç†ï¼Œè·³è¿‡: messageId={}", messageId);
            return;
        }
        
        // 2. æ ‡è®°æ¶ˆæ¯ä¸ºå¤„ç†ä¸­
        markMessageAsProcessing(messageId);
        
        try {
            // 3. ä¸šåŠ¡å¤„ç†
            orderService.processOrder(orderEvent);
            
            // 4. æ ‡è®°æ¶ˆæ¯ä¸ºå·²å¤„ç†
            markMessageAsProcessed(messageId);
            
        } catch (Exception e) {
            // 5. å¤„ç†å¤±è´¥ï¼Œæ¸…é™¤å¤„ç†æ ‡è®°
            clearProcessingMark(messageId);
            throw e;
        }
    }
    
    private boolean isMessageProcessed(String messageId) {
        String key = "processed:" + messageId;
        return redisTemplate.hasKey(key);
    }
    
    private void markMessageAsProcessing(String messageId) {
        String key = "processing:" + messageId;
        String processingKey = "processed:" + messageId;
        
        // ä½¿ç”¨RedisåŸå­æ“ä½œä¿è¯å¹¶å‘å®‰å…¨
        Boolean success = redisTemplate.opsForValue()
            .setIfAbsent(key, "processing", Duration.ofMinutes(5));
        
        if (!success) {
            throw new DuplicateMessageException("æ¶ˆæ¯æ­£åœ¨å¤„ç†ä¸­: " + messageId);
        }
    }
    
    private void markMessageAsProcessed(String messageId) {
        String processingKey = "processing:" + messageId;
        String processedKey = "processed:" + messageId;
        
        // åŸå­æ“ä½œï¼šåˆ é™¤å¤„ç†æ ‡è®°ï¼Œæ·»åŠ å·²å¤„ç†æ ‡è®°
        redisTemplate.execute(new SessionCallback<Object>() {
            @Override
            public Object execute(RedisOperations operations) {
                operations.multi();
                operations.delete(processingKey);
                operations.opsForValue().set(processedKey, "processed", 
                    Duration.ofHours(24));
                return operations.exec();
            }
        });
    }
}
```

#### 7.6.4 æ¶ˆæ¯é¡ºåºä¿è¯

**åˆ†åŒºå†…é¡ºåºå¤„ç†**ï¼š

```java
@Component
@Slf4j
public class OrderedMessageProcessor {
    
    private final Map<String, BlockingQueue<OrderEvent>> partitionQueues = new ConcurrentHashMap<>();
    private final ExecutorService executorService = Executors.newFixedThreadPool(10);
    
    @KafkaListener(topics = "order-events", groupId = "ordered-processor")
    public void handleOrderEvent(@Payload OrderEvent orderEvent,
                               @Header(KafkaHeaders.RECEIVED_PARTITION_ID) int partition) {
        
        String partitionKey = "partition-" + partition;
        
        // å°†æ¶ˆæ¯æ”¾å…¥å¯¹åº”åˆ†åŒºçš„é˜Ÿåˆ—
        partitionQueues.computeIfAbsent(partitionKey, 
            k -> new LinkedBlockingQueue<>()).offer(orderEvent);
        
        // å¼‚æ­¥å¤„ç†è¯¥åˆ†åŒºçš„æ¶ˆæ¯
        executorService.submit(() -> processPartitionMessages(partitionKey));
    }
    
    private void processPartitionMessages(String partitionKey) {
        BlockingQueue<OrderEvent> queue = partitionQueues.get(partitionKey);
        
        while (!queue.isEmpty()) {
            try {
                OrderEvent orderEvent = queue.take();
                processOrderEvent(orderEvent);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            } catch (Exception e) {
                log.error("å¤„ç†åˆ†åŒºæ¶ˆæ¯å¤±è´¥: partition={}, error={}", 
                    partitionKey, e.getMessage(), e);
                // é”™è¯¯å¤„ç†é€»è¾‘
                handleProcessingError(partitionKey, e);
            }
        }
    }
    
    private void processOrderEvent(OrderEvent orderEvent) {
        // ç¡®ä¿åŒä¸€åˆ†åŒºçš„æ¶ˆæ¯æŒ‰é¡ºåºå¤„ç†
        log.info("å¤„ç†è®¢å•äº‹ä»¶: orderId={}, partition={}", 
            orderEvent.getOrderId(), orderEvent.getPartition());
        
        // ä¸šåŠ¡å¤„ç†é€»è¾‘
        orderService.processOrder(orderEvent);
    }
}
```

#### 7.6.5 ç›‘æ§ä¸å‘Šè­¦

**è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡**ï¼š

```java
@Component
@Slf4j
public class KafkaMetricsCollector {
    
    private final MeterRegistry meterRegistry;
    private final Counter messageProcessedCounter;
    private final Counter messageFailedCounter;
    private final Timer messageProcessingTimer;
    private final Gauge consumerLagGauge;
    
    public KafkaMetricsCollector(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        
        // æ¶ˆæ¯å¤„ç†è®¡æ•°å™¨
        this.messageProcessedCounter = Counter.builder("kafka.messages.processed")
            .description("å·²å¤„ç†çš„æ¶ˆæ¯æ•°é‡")
            .register(meterRegistry);
        
        // æ¶ˆæ¯å¤±è´¥è®¡æ•°å™¨
        this.messageFailedCounter = Counter.builder("kafka.messages.failed")
            .description("å¤„ç†å¤±è´¥çš„æ¶ˆæ¯æ•°é‡")
            .register(meterRegistry);
        
        // æ¶ˆæ¯å¤„ç†æ—¶é—´
        this.messageProcessingTimer = Timer.builder("kafka.message.processing.time")
            .description("æ¶ˆæ¯å¤„ç†æ—¶é—´")
            .register(meterRegistry);
        
        // æ¶ˆè´¹è€…å»¶è¿Ÿ
        this.consumerLagGauge = Gauge.builder("kafka.consumer.lag")
            .description("æ¶ˆè´¹è€…å»¶è¿Ÿ")
            .register(meterRegistry, this, KafkaMetricsCollector::getConsumerLag);
    }
    
    public void recordMessageProcessed(String topic, String consumerGroup) {
        messageProcessedCounter.increment(
            Tags.of("topic", topic, "consumer_group", consumerGroup));
    }
    
    public void recordMessageFailed(String topic, String consumerGroup, String errorType) {
        messageFailedCounter.increment(
            Tags.of("topic", topic, "consumer_group", consumerGroup, "error_type", errorType));
    }
    
    public void recordProcessingTime(String topic, Duration duration) {
        messageProcessingTimer.record(duration);
    }
    
    private double getConsumerLag() {
        // å®ç°è·å–æ¶ˆè´¹è€…å»¶è¿Ÿçš„é€»è¾‘
        return calculateConsumerLag();
    }
    
    private double calculateConsumerLag() {
        // å®é™…å®ç°ä¸­éœ€è¦æŸ¥è¯¢Kafkaè·å–å»¶è¿Ÿä¿¡æ¯
        return 0.0;
    }
}
```

**å¥åº·æ£€æŸ¥ç«¯ç‚¹**ï¼š

```java
@RestController
@RequestMapping("/health")
public class KafkaHealthController {
    
    private final KafkaAdmin kafkaAdmin;
    private final KafkaMetricsCollector metricsCollector;
    
    @GetMapping("/kafka")
    public ResponseEntity<Map<String, Object>> checkKafkaHealth() {
        Map<String, Object> health = new HashMap<>();
        
        try {
            // æ£€æŸ¥Kafkaè¿æ¥
            boolean isConnected = checkKafkaConnection();
            health.put("status", isConnected ? "UP" : "DOWN");
            health.put("kafka_connection", isConnected);
            
            // æ£€æŸ¥æ¶ˆè´¹è€…å»¶è¿Ÿ
            double consumerLag = metricsCollector.getConsumerLag();
            health.put("consumer_lag", consumerLag);
            health.put("lag_status", consumerLag < 1000 ? "NORMAL" : "HIGH");
            
            // æ£€æŸ¥å¤„ç†é€Ÿç‡
            double processingRate = getProcessingRate();
            health.put("processing_rate", processingRate);
            
            return ResponseEntity.ok(health);
            
        } catch (Exception e) {
            health.put("status", "DOWN");
            health.put("error", e.getMessage());
            return ResponseEntity.status(503).body(health);
        }
    }
    
    private boolean checkKafkaConnection() {
        try {
            kafkaAdmin.describeTopics("test-topic");
            return true;
        } catch (Exception e) {
            return false;
        }
    }
    
    private double getProcessingRate() {
        // å®ç°è·å–å¤„ç†é€Ÿç‡çš„é€»è¾‘
        return 0.0;
    }
}
```

### 7.7 æ€§èƒ½æµ‹è¯•ä¸è°ƒä¼˜å®è·µ

#### 7.7.1 æ€§èƒ½åŸºå‡†æµ‹è¯•

**Kafkaæ€§èƒ½æµ‹è¯•å·¥å…·**ï¼š

```bash
#!/bin/bash
# kafka-benchmark.sh

KAFKA_HOME="/opt/kafka"
BROKER_LIST="localhost:9092"
TOPIC_NAME="benchmark-topic"

echo "=== Kafkaæ€§èƒ½åŸºå‡†æµ‹è¯• ==="

# 1. ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•
echo "1. æµ‹è¯•ç”Ÿäº§è€…æ€§èƒ½..."
kafka-producer-perf-test.sh \
  --topic $TOPIC_NAME \
  --num-records 1000000 \
  --record-size 1024 \
  --throughput 10000 \
  --producer-props bootstrap.servers=$BROKER_LIST \
  --producer-props acks=all \
  --producer-props retries=3 \
  --producer-props batch.size=16384 \
  --producer-props linger.ms=5

# 2. æ¶ˆè´¹è€…æ€§èƒ½æµ‹è¯•
echo "2. æµ‹è¯•æ¶ˆè´¹è€…æ€§èƒ½..."
kafka-consumer-perf-test.sh \
  --topic $TOPIC_NAME \
  --bootstrap-server $BROKER_LIST \
  --messages 1000000 \
  --threads 4 \
  --group test-group

# 3. ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•
echo "3. æµ‹è¯•ç«¯åˆ°ç«¯å»¶è¿Ÿ..."
kafka-producer-perf-test.sh \
  --topic $TOPIC_NAME \
  --num-records 10000 \
  --record-size 1024 \
  --throughput -1 \
  --producer-props bootstrap.servers=$BROKER_LIST \
  --producer-props acks=1 &

sleep 5

kafka-consumer-perf-test.sh \
  --topic $TOPIC_NAME \
  --bootstrap-server $BROKER_LIST \
  --messages 10000 \
  --threads 1 \
  --group latency-test-group

echo "=== æ€§èƒ½æµ‹è¯•å®Œæˆ ==="
```

**è‡ªå®šä¹‰æ€§èƒ½æµ‹è¯•å·¥å…·**ï¼š

```java
@Component
@Slf4j
public class KafkaPerformanceTester {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final MeterRegistry meterRegistry;
    
    public void runProducerBenchmark(String topic, int messageCount, int messageSize) {
        log.info("å¼€å§‹ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•: topic={}, count={}, size={}", 
            topic, messageCount, messageSize);
        
        Timer.Sample sample = Timer.start(meterRegistry);
        
        for (int i = 0; i < messageCount; i++) {
            String message = generateMessage(messageSize);
            kafkaTemplate.send(topic, String.valueOf(i), message);
        }
        
        sample.stop(Timer.builder("kafka.producer.benchmark")
            .register(meterRegistry));
        
        log.info("ç”Ÿäº§è€…æ€§èƒ½æµ‹è¯•å®Œæˆ");
    }
    
    public void runConsumerBenchmark(String topic, String groupId, int expectedMessages) {
        log.info("å¼€å§‹æ¶ˆè´¹è€…æ€§èƒ½æµ‹è¯•: topic={}, group={}, expected={}", 
            topic, groupId, expectedMessages);
        
        AtomicInteger messageCount = new AtomicInteger(0);
        Timer.Sample sample = Timer.start(meterRegistry);
        
        // ä½¿ç”¨KafkaListenerè¿›è¡Œæµ‹è¯•
        // å®é™…å®ç°ä¸­éœ€è¦é…ç½®æµ‹è¯•ä¸“ç”¨çš„æ¶ˆè´¹è€…
        
        log.info("æ¶ˆè´¹è€…æ€§èƒ½æµ‹è¯•å®Œæˆ: processed={}", messageCount.get());
    }
    
    private String generateMessage(int size) {
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < size; i++) {
            sb.append('A');
        }
        return sb.toString();
    }
}
```

#### 7.7.2 æ€§èƒ½è°ƒä¼˜ç­–ç•¥

**ç”Ÿäº§è€…è°ƒä¼˜**ï¼š

```java
@Configuration
public class OptimizedKafkaConfig {
    
    @Bean
    public ProducerFactory<String, Object> optimizedProducerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        
        // åŸºç¡€é…ç½®
        configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        
        // æ€§èƒ½ä¼˜åŒ–é…ç½®
        configProps.put(ProducerConfig.ACKS_CONFIG, "1"); // é™ä½å»¶è¿Ÿ
        configProps.put(ProducerConfig.RETRIES_CONFIG, 3);
        configProps.put(ProducerConfig.BATCH_SIZE_CONFIG, 32768); // å¢åŠ æ‰¹æ¬¡å¤§å°
        configProps.put(ProducerConfig.LINGER_MS_CONFIG, 10); // å¢åŠ ç­‰å¾…æ—¶é—´
        configProps.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 67108864); // å¢åŠ ç¼“å†²åŒº
        configProps.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "snappy"); // å¯ç”¨å‹ç¼©
        configProps.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        configProps.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
        
        // ç½‘ç»œä¼˜åŒ–
        configProps.put(ProducerConfig.SEND_BUFFER_CONFIG, 131072);
        configProps.put(ProducerConfig.RECEIVE_BUFFER_CONFIG, 131072);
        
        return new DefaultKafkaProducerFactory<>(configProps);
    }
    
    @Bean
    public ConsumerFactory<String, Object> optimizedConsumerFactory() {
        Map<String, Object> configProps = new HashMap<>();
        
        // åŸºç¡€é…ç½®
        configProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        configProps.put(ConsumerConfig.GROUP_ID_CONFIG, "optimized-group");
        configProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        configProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        
        // æ€§èƒ½ä¼˜åŒ–é…ç½®
        configProps.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1048576); // å¢åŠ æœ€å°æ‹‰å–å¤§å°
        configProps.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500); // å‡å°‘ç­‰å¾…æ—¶é—´
        configProps.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 1048576); // å¢åŠ åˆ†åŒºæ‹‰å–å¤§å°
        configProps.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        configProps.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);
        configProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "latest");
        configProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // æ‰‹åŠ¨æäº¤
        
        return new DefaultKafkaConsumerFactory<>(configProps);
    }
}
```

**JVMè°ƒä¼˜é…ç½®**ï¼š

```bash
#!/bin/bash
# kafka-jvm-tuning.sh

# è®¾ç½®JVMå‚æ•°
export KAFKA_HEAP_OPTS="-Xmx8g -Xms8g"

# G1GCä¼˜åŒ–å‚æ•°
export KAFKA_JVM_PERFORMANCE_OPTS="
-server
-XX:+UseG1GC
-XX:MaxGCPauseMillis=20
-XX:InitiatingHeapOccupancyPercent=35
-XX:+ExplicitGCInvokesConcurrent
-XX:MaxInlineLevel=15
-XX:+UseStringDeduplication
-XX:+OptimizeStringConcat
-XX:+UseCompressedOops
-XX:+UseCompressedClassPointers
-Djava.awt.headless=true
"

# ç½‘ç»œä¼˜åŒ–å‚æ•°
export KAFKA_OPTS="
-Djava.net.preferIPv4Stack=true
-Dcom.sun.management.jmxremote
-Dcom.sun.management.jmxremote.authenticate=false
-Dcom.sun.management.jmxremote.ssl=false
-Dcom.sun.management.jmxremote.port=9999
"

echo "JVMè°ƒä¼˜å‚æ•°å·²è®¾ç½®"
```

#### 7.7.3 ç³»ç»Ÿçº§ä¼˜åŒ–

**Linuxç³»ç»Ÿä¼˜åŒ–**ï¼š

```bash
#!/bin/bash
# kafka-system-tuning.sh

echo "=== Kafkaç³»ç»Ÿçº§ä¼˜åŒ– ==="

# 1. ç½‘ç»œå‚æ•°ä¼˜åŒ–
echo "1. ä¼˜åŒ–ç½‘ç»œå‚æ•°..."
cat >> /etc/sysctl.conf << EOF
# ç½‘ç»œç¼“å†²åŒº
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 65536 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728

# ç½‘ç»œè¿æ¥ä¼˜åŒ–
net.core.netdev_max_backlog = 5000
net.ipv4.tcp_max_syn_backlog = 4096
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 60
net.ipv4.tcp_keepalive_probes = 3

# æ–‡ä»¶æè¿°ç¬¦
fs.file-max = 1000000
fs.nr_open = 1000000
EOF

# 2. ç£ç›˜I/Oä¼˜åŒ–
echo "2. ä¼˜åŒ–ç£ç›˜I/O..."
cat >> /etc/sysctl.conf << EOF
# è™šæ‹Ÿå†…å­˜ä¼˜åŒ–
vm.swappiness = 1
vm.dirty_ratio = 15
vm.dirty_background_ratio = 5
vm.dirty_expire_centisecs = 3000
vm.dirty_writeback_centisecs = 500
EOF

# 3. åº”ç”¨ç³»ç»Ÿå‚æ•°
sysctl -p

# 4. è®¾ç½®æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
echo "3. è®¾ç½®æ–‡ä»¶æè¿°ç¬¦é™åˆ¶..."
cat >> /etc/security/limits.conf << EOF
kafka soft nofile 1000000
kafka hard nofile 1000000
kafka soft nproc 1000000
kafka hard nproc 1000000
EOF

# 5. ç£ç›˜æŒ‚è½½ä¼˜åŒ–
echo "4. ä¼˜åŒ–ç£ç›˜æŒ‚è½½..."
# ä¸ºKafkaæ•°æ®ç›®å½•æ·»åŠ noatimeé€‰é¡¹
mount -o remount,noatime /data

echo "=== ç³»ç»Ÿä¼˜åŒ–å®Œæˆ ==="
```

#### 7.7.4 ç›‘æ§æŒ‡æ ‡åˆ†æ

**å…³é”®æ€§èƒ½æŒ‡æ ‡**ï¼š

```java
@Component
@Slf4j
public class KafkaPerformanceAnalyzer {
    
    private final MeterRegistry meterRegistry;
    private final KafkaAdmin kafkaAdmin;
    
    public PerformanceReport analyzePerformance() {
        PerformanceReport report = new PerformanceReport();
        
        // 1. ååé‡åˆ†æ
        double producerThroughput = calculateProducerThroughput();
        double consumerThroughput = calculateConsumerThroughput();
        report.setProducerThroughput(producerThroughput);
        report.setConsumerThroughput(consumerThroughput);
        
        // 2. å»¶è¿Ÿåˆ†æ
        double p99Latency = calculateP99Latency();
        double avgLatency = calculateAverageLatency();
        report.setP99Latency(p99Latency);
        report.setAverageLatency(avgLatency);
        
        // 3. èµ„æºä½¿ç”¨ç‡
        double cpuUsage = getCpuUsage();
        double memoryUsage = getMemoryUsage();
        double diskUsage = getDiskUsage();
        report.setCpuUsage(cpuUsage);
        report.setMemoryUsage(memoryUsage);
        report.setDiskUsage(diskUsage);
        
        // 4. æ¶ˆè´¹è€…å»¶è¿Ÿ
        double consumerLag = getConsumerLag();
        report.setConsumerLag(consumerLag);
        
        // 5. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        generatePerformanceReport(report);
        
        return report;
    }
    
    private void generatePerformanceReport(PerformanceReport report) {
        log.info("=== Kafkaæ€§èƒ½åˆ†ææŠ¥å‘Š ===");
        log.info("ç”Ÿäº§è€…ååé‡: {} msg/s", report.getProducerThroughput());
        log.info("æ¶ˆè´¹è€…ååé‡: {} msg/s", report.getConsumerThroughput());
        log.info("P99å»¶è¿Ÿ: {} ms", report.getP99Latency());
        log.info("å¹³å‡å»¶è¿Ÿ: {} ms", report.getAverageLatency());
        log.info("CPUä½¿ç”¨ç‡: {}%", report.getCpuUsage());
        log.info("å†…å­˜ä½¿ç”¨ç‡: {}%", report.getMemoryUsage());
        log.info("ç£ç›˜ä½¿ç”¨ç‡: {}%", report.getDiskUsage());
        log.info("æ¶ˆè´¹è€…å»¶è¿Ÿ: {} ms", report.getConsumerLag());
        
        // æ€§èƒ½å»ºè®®
        providePerformanceRecommendations(report);
    }
    
    private void providePerformanceRecommendations(PerformanceReport report) {
        log.info("=== æ€§èƒ½ä¼˜åŒ–å»ºè®® ===");
        
        if (report.getProducerThroughput() < 10000) {
            log.warn("ç”Ÿäº§è€…ååé‡è¾ƒä½ï¼Œå»ºè®®ï¼š");
            log.warn("- å¢åŠ batch.size");
            log.warn("- å¯ç”¨å‹ç¼©");
            log.warn("- è°ƒæ•´linger.ms");
        }
        
        if (report.getP99Latency() > 100) {
            log.warn("å»¶è¿Ÿè¾ƒé«˜ï¼Œå»ºè®®ï¼š");
            log.warn("- å‡å°‘acksé…ç½®");
            log.warn("- ä¼˜åŒ–ç½‘ç»œå‚æ•°");
            log.warn("- æ£€æŸ¥ç£ç›˜I/O");
        }
        
        if (report.getConsumerLag() > 1000) {
            log.warn("æ¶ˆè´¹è€…å»¶è¿Ÿè¾ƒé«˜ï¼Œå»ºè®®ï¼š");
            log.warn("- å¢åŠ æ¶ˆè´¹è€…å®ä¾‹");
            log.warn("- ä¼˜åŒ–fetché…ç½®");
            log.warn("- æ£€æŸ¥å¤„ç†é€»è¾‘æ€§èƒ½");
        }
    }
}
```

#### 7.7.5 å‹åŠ›æµ‹è¯•åœºæ™¯

**å¤šåœºæ™¯å‹åŠ›æµ‹è¯•**ï¼š

```java
@Component
@Slf4j
public class KafkaStressTester {
    
    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final ExecutorService executorService = Executors.newFixedThreadPool(20);
    
    public void runStressTest() {
        log.info("å¼€å§‹Kafkaå‹åŠ›æµ‹è¯•...");
        
        // åœºæ™¯1ï¼šé«˜å¹¶å‘å†™å…¥
        runHighConcurrencyWriteTest();
        
        // åœºæ™¯2ï¼šå¤§æ¶ˆæ¯å¤„ç†
        runLargeMessageTest();
        
        // åœºæ™¯3ï¼šé•¿æ—¶é—´è¿è¡Œ
        runLongRunningTest();
        
        // åœºæ™¯4ï¼šæ•…éšœæ¢å¤
        runFailureRecoveryTest();
        
        log.info("å‹åŠ›æµ‹è¯•å®Œæˆ");
    }
    
    private void runHighConcurrencyWriteTest() {
        log.info("åœºæ™¯1ï¼šé«˜å¹¶å‘å†™å…¥æµ‹è¯•");
        
        int threadCount = 20;
        int messagesPerThread = 10000;
        
        CountDownLatch latch = new CountDownLatch(threadCount);
        
        for (int i = 0; i < threadCount; i++) {
            final int threadId = i;
            executorService.submit(() -> {
                try {
                    for (int j = 0; j < messagesPerThread; j++) {
                        String message = String.format("Thread-%d-Message-%d", threadId, j);
                        kafkaTemplate.send("stress-test-topic", String.valueOf(j), message);
                    }
                } finally {
                    latch.countDown();
                }
            });
        }
        
        try {
            latch.await(5, TimeUnit.MINUTES);
            log.info("é«˜å¹¶å‘å†™å…¥æµ‹è¯•å®Œæˆ");
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }
    
    private void runLargeMessageTest() {
        log.info("åœºæ™¯2ï¼šå¤§æ¶ˆæ¯å¤„ç†æµ‹è¯•");
        
        int[] messageSizes = {1024, 10240, 102400, 1048576}; // 1KB, 10KB, 100KB, 1MB
        
        for (int size : messageSizes) {
            String largeMessage = generateLargeMessage(size);
            long startTime = System.currentTimeMillis();
            
            kafkaTemplate.send("large-message-topic", "large-message", largeMessage);
            
            long endTime = System.currentTimeMillis();
            log.info("å¤§æ¶ˆæ¯æµ‹è¯•å®Œæˆ: size={} bytes, time={} ms", 
                size, endTime - startTime);
        }
    }
    
    private void runLongRunningTest() {
        log.info("åœºæ™¯3ï¼šé•¿æ—¶é—´è¿è¡Œæµ‹è¯•");
        
        long startTime = System.currentTimeMillis();
        long endTime = startTime + TimeUnit.HOURS.toMillis(1); // è¿è¡Œ1å°æ—¶
        
        int messageCount = 0;
        while (System.currentTimeMillis() < endTime) {
            String message = String.format("LongRunning-Message-%d", messageCount++);
            kafkaTemplate.send("long-running-topic", String.valueOf(messageCount), message);
            
            if (messageCount % 1000 == 0) {
                log.info("é•¿æ—¶é—´è¿è¡Œæµ‹è¯•: å·²å‘é€ {} æ¡æ¶ˆæ¯", messageCount);
            }
            
            try {
                Thread.sleep(10); // 10msé—´éš”
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            }
        }
        
        log.info("é•¿æ—¶é—´è¿è¡Œæµ‹è¯•å®Œæˆ: æ€»å…±å‘é€ {} æ¡æ¶ˆæ¯", messageCount);
    }
    
    private void runFailureRecoveryTest() {
        log.info("åœºæ™¯4ï¼šæ•…éšœæ¢å¤æµ‹è¯•");
        
        // æ¨¡æ‹Ÿç½‘ç»œä¸­æ–­
        simulateNetworkFailure();
        
        // æ¨¡æ‹ŸBrokeré‡å¯
        simulateBrokerRestart();
        
        // æ¨¡æ‹Ÿæ¶ˆè´¹è€…é‡å¯
        simulateConsumerRestart();
        
        log.info("æ•…éšœæ¢å¤æµ‹è¯•å®Œæˆ");
    }
    
    private String generateLargeMessage(int size) {
        StringBuilder sb = new StringBuilder();
        for (int i = 0; i < size; i++) {
            sb.append('A');
        }
        return sb.toString();
    }
    
    private void simulateNetworkFailure() {
        // å®é™…å®ç°ä¸­éœ€è¦æ¨¡æ‹Ÿç½‘ç»œæ•…éšœ
        log.info("æ¨¡æ‹Ÿç½‘ç»œæ•…éšœ...");
    }
    
    private void simulateBrokerRestart() {
        // å®é™…å®ç°ä¸­éœ€è¦é‡å¯Broker
        log.info("æ¨¡æ‹ŸBrokeré‡å¯...");
    }
    
    private void simulateConsumerRestart() {
        // å®é™…å®ç°ä¸­éœ€è¦é‡å¯æ¶ˆè´¹è€…
        log.info("æ¨¡æ‹Ÿæ¶ˆè´¹è€…é‡å¯...");
    }
}
```

## 8. Kafkaæœ€ä½³å®è·µä¸æ€»ç»“

### 8.1 è®¾è®¡åŸåˆ™ä¸æœ€ä½³å®è·µ

**æ¶æ„è®¾è®¡åŸåˆ™**ï¼š
1. **åˆç†è§„åˆ’åˆ†åŒºæ•°**ï¼šæ ¹æ®å¹¶å‘éœ€æ±‚å’Œæ•°æ®é‡è®¾ç½®åˆ†åŒºæ•°
2. **é€‰æ‹©åˆé€‚å‰¯æœ¬å› å­**ï¼šå¹³è¡¡å¯é æ€§å’Œå­˜å‚¨æˆæœ¬
3. **è®¾è®¡åˆç†çš„Topicç»“æ„**ï¼šæŒ‰ä¸šåŠ¡åŸŸå’Œæ•°æ®ç±»å‹åˆ’åˆ†
4. **è€ƒè™‘æ•°æ®ä¿ç•™ç­–ç•¥**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚è®¾ç½®ä¿ç•™æ—¶é—´å’Œå¤§å°

**æ€§èƒ½ä¼˜åŒ–åŸåˆ™**ï¼š
1. **æ‰¹é‡å¤„ç†ä¼˜å…ˆ**ï¼šä½¿ç”¨æ‰¹é‡å‘é€å’Œæ¶ˆè´¹æé«˜ååé‡
2. **å¼‚æ­¥æ“ä½œä¼˜å…ˆ**ï¼šä¼˜å…ˆä½¿ç”¨å¼‚æ­¥APIå‡å°‘å»¶è¿Ÿ
3. **åˆç†ä½¿ç”¨å‹ç¼©**ï¼šé€‰æ‹©åˆé€‚çš„å‹ç¼©ç®—æ³•å¹³è¡¡CPUå’Œç½‘ç»œ
4. **ç›‘æ§é©±åŠ¨ä¼˜åŒ–**ï¼šåŸºäºç›‘æ§æ•°æ®è¿›è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–

**å¯é æ€§ä¿è¯åŸåˆ™**ï¼š
1. **å¤šå‰¯æœ¬å†—ä½™**ï¼šä½¿ç”¨å¤šå‰¯æœ¬ä¿è¯æ•°æ®å®‰å…¨
2. **ä¼˜é›…é™çº§**ï¼šè®¾è®¡é™çº§ç­–ç•¥åº”å¯¹å¼‚å¸¸æƒ…å†µ
3. **å®Œå–„ç›‘æ§**ï¼šå»ºç«‹å®Œæ•´çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»
4. **å®šæœŸæ¼”ç»ƒ**ï¼šå®šæœŸè¿›è¡Œæ•…éšœæ¼”ç»ƒå’Œæ¢å¤æµ‹è¯•

### 7.2 æŠ€æœ¯é€‰å‹å»ºè®®

**Kafkaé€‚ç”¨åœºæ™¯**ï¼š
- é«˜ååé‡çš„å®æ—¶æ•°æ®ç®¡é“
- äº‹ä»¶é©±åŠ¨æ¶æ„çš„æ¶ˆæ¯ä¸­é—´ä»¶
- å¤§æ•°æ®å¹³å°çš„æ•°æ®æ”¶é›†
- å¾®æœåŠ¡é—´çš„å¼‚æ­¥é€šä¿¡
- æ—¥å¿—èšåˆå’Œç›‘æ§æ•°æ®æ”¶é›†

**ä¸å…¶ä»–æ¶ˆæ¯é˜Ÿåˆ—å¯¹æ¯”**ï¼š

| ç‰¹æ€§ | Kafka | RabbitMQ | RocketMQ | ActiveMQ |
|------|-------|----------|----------|----------|
| **ååé‡** | æé«˜ | ä¸­ç­‰ | é«˜ | ä¸­ç­‰ |
| **å»¶è¿Ÿ** | ä½ | ä½ | ä½ | ä¸­ç­‰ |
| **å¯é æ€§** | é«˜ | é«˜ | é«˜ | ä¸­ç­‰ |
| **æ‰©å±•æ€§** | ä¼˜ç§€ | è‰¯å¥½ | ä¼˜ç§€ | ä¸€èˆ¬ |
| **è¿ç»´å¤æ‚åº¦** | ä¸­ç­‰ | ä½ | é«˜ | ä½ |
| **ç”Ÿæ€ç³»ç»Ÿ** | ä¸°å¯Œ | ä¸°å¯Œ | ä¸­ç­‰ | ä¸°å¯Œ |

### 7.3 å­¦ä¹ è·¯å¾„å»ºè®®

**åŸºç¡€é˜¶æ®µ**ï¼š
1. ç†è§£Kafkaæ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„
2. æŒæ¡åŸºæœ¬çš„ç”Ÿäº§è€…å’Œæ¶ˆè´¹è€…API
3. å­¦ä¹ Topicå’Œåˆ†åŒºçš„è®¾è®¡åŸåˆ™
4. äº†è§£å‰¯æœ¬æœºåˆ¶å’Œä¸€è‡´æ€§ä¿è¯

**è¿›é˜¶é˜¶æ®µ**ï¼š
1. æ·±å…¥å­¦ä¹ Kafka Streamsæµå¤„ç†
2. æŒæ¡æ€§èƒ½ä¼˜åŒ–å’Œè°ƒä¼˜æŠ€å·§
3. å­¦ä¹ é›†ç¾¤éƒ¨ç½²å’Œè¿ç»´ç®¡ç†
4. äº†è§£ç›‘æ§å’Œæ•…éšœæ’æŸ¥æ–¹æ³•

**é«˜çº§é˜¶æ®µ**ï¼š
1. ç ”ç©¶Kafkaæºç å’Œå®ç°åŸç†
2. è®¾è®¡å¤§è§„æ¨¡åˆ†å¸ƒå¼æ¶ˆæ¯ç³»ç»Ÿ
3. å‚ä¸å¼€æºç¤¾åŒºè´¡çŒ®ä»£ç 
4. åˆ†äº«ç»éªŒå’Œæœ€ä½³å®è·µ

:::tip Kafkaå­¦ä¹ å»ºè®®
1. **ç†è®ºä¸å®è·µç»“åˆ**ï¼šåœ¨ç†è§£æ¦‚å¿µçš„åŸºç¡€ä¸Šå¤šåšå®é™…é¡¹ç›®
2. **å…³æ³¨æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¼šåˆ†æå’Œä¼˜åŒ–Kafkaæ€§èƒ½
3. **æŒæ¡è¿ç»´æŠ€èƒ½**ï¼šèƒ½å¤Ÿéƒ¨ç½²å’Œç®¡ç†ç”Ÿäº§ç¯å¢ƒçš„Kafkaé›†ç¾¤
4. **è·Ÿè¿›æŠ€æœ¯å‘å±•**ï¼šå…³æ³¨Kafkaæ–°ç‰ˆæœ¬ç‰¹æ€§å’Œç¤¾åŒºåŠ¨æ€
5. **ç§¯ç´¯å®æˆ˜ç»éªŒ**ï¼šé€šè¿‡å®é™…é¡¹ç›®ç§¯ç´¯æ•…éšœæ’æŸ¥å’Œä¼˜åŒ–ç»éªŒ
:::

---

é€šè¿‡æœ¬ç« çš„æ·±å…¥å­¦ä¹ ï¼Œä½ åº”è¯¥å·²ç»å…¨é¢æŒæ¡äº†Apache Kafkaçš„æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„è®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–å’Œå®æˆ˜åº”ç”¨ã€‚Kafkaä½œä¸ºç°ä»£å¤§æ•°æ®å’Œå®æ—¶å¤„ç†æ¶æ„ä¸­çš„æ ¸å¿ƒç»„ä»¶ï¼Œåœ¨æ„å»ºé«˜æ€§èƒ½ã€é«˜å¯é çš„æ•°æ®ç®¡é“å’Œäº‹ä»¶é©±åŠ¨ç³»ç»Ÿä¸­å‘æŒ¥ç€å…³é”®ä½œç”¨ã€‚

åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œåˆç†ä½¿ç”¨Kafkaä¸ä»…èƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡çš„å®æ—¶æ•°æ®æµï¼Œè¿˜èƒ½ç®€åŒ–ç³»ç»Ÿæ¶æ„ï¼Œæé«˜å¼€å‘æ•ˆç‡ã€‚å¸Œæœ›è¿™ä»½è¯¦ç»†çš„KafkaæŒ‡å—èƒ½å¤Ÿå¸®åŠ©ä½ åœ¨æŠ€æœ¯é¢è¯•å’Œå®é™…å·¥ä½œä¸­æ¸¸åˆƒæœ‰ä½™ï¼Œæˆä¸ºKafkaæŠ€æœ¯ä¸“å®¶ï¼