---
sidebar_position: 4
title: æ•°æ®ä»“åº“ä¸ETLæŠ€æœ¯
description: æ·±å…¥ç†è§£æ•°æ®ä»“åº“æ¶æ„ã€ETLæµç¨‹è®¾è®¡ã€æ•°æ®å»ºæ¨¡å’Œæœ€ä½³å®è·µ
authors: [Laby]
last_update:
  date: 2025-08-16
  author: Laby
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TOCInline from '@theme/TOCInline';

# æ•°æ®ä»“åº“ä¸ETLæŠ€æœ¯

æ•°æ®ä»“åº“æ˜¯ä¼ä¸šçº§æ•°æ®ç®¡ç†çš„æ ¸å¿ƒï¼Œå®ƒé›†æˆäº†æ¥è‡ªå¤šä¸ªä¸šåŠ¡ç³»ç»Ÿçš„æ•°æ®ï¼Œä¸ºå†³ç­–æ”¯æŒå’Œå•†ä¸šæ™ºèƒ½æä¾›ç»Ÿä¸€çš„æ•°æ®è§†å›¾ã€‚ETLï¼ˆExtract, Transform, Loadï¼‰æ˜¯æ„å»ºæ•°æ®ä»“åº“çš„å…³é”®æŠ€æœ¯ï¼Œè´Ÿè´£æ•°æ®çš„æŠ½å–ã€è½¬æ¢å’ŒåŠ è½½ã€‚

:::info æœ¬æ–‡å†…å®¹æ¦‚è§ˆ
<TOCInline toc={toc} />
:::

:::tip æ ¸å¿ƒä»·å€¼
**æ•°æ®ä»“åº“ä¸ETL = æ•°æ®é›†æˆ + æ•°æ®è´¨é‡ + ç»Ÿä¸€è§†å›¾ + å†³ç­–æ”¯æŒ + ä¸šåŠ¡æ´å¯Ÿ**
- ğŸš€ **æ•°æ®é›†æˆ**ï¼šæ•´åˆå¤šä¸ªæ•°æ®æºï¼Œæ¶ˆé™¤æ•°æ®å­¤å²›
- ğŸ‘¨â€ğŸ’» **æ•°æ®è´¨é‡**ï¼šé€šè¿‡ETLæµç¨‹ä¿è¯æ•°æ®å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
- ğŸ” **ç»Ÿä¸€è§†å›¾**ï¼šæä¾›æ ‡å‡†åŒ–çš„æ•°æ®æ¨¡å‹å’Œæ¥å£
- ğŸ”— **å†³ç­–æ”¯æŒ**ï¼šæ”¯æŒå¤æ‚çš„åˆ†ææŸ¥è¯¢å’ŒæŠ¥è¡¨ç”Ÿæˆ
- ğŸ“š **ä¸šåŠ¡æ´å¯Ÿ**ï¼šé€šè¿‡æ•°æ®åˆ†æå‘ç°ä¸šåŠ¡ä»·å€¼å’Œè¶‹åŠ¿
:::

## 1. æ•°æ®ä»“åº“åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯æ•°æ®ä»“åº“ï¼Ÿ

æ•°æ®ä»“åº“æ˜¯ä¸€ä¸ªé¢å‘ä¸»é¢˜çš„ã€é›†æˆçš„ã€ç›¸å¯¹ç¨³å®šçš„ã€åæ˜ å†å²å˜åŒ–çš„æ•°æ®é›†åˆï¼Œç”¨äºæ”¯æŒç®¡ç†å†³ç­–ã€‚

```mermaid
graph TD
    A[æ•°æ®ä»“åº“ç‰¹å¾] --> B[é¢å‘ä¸»é¢˜]
    A --> C[é›†æˆæ€§]
    A --> D[éæ˜“å¤±æ€§]
    A --> E[æ—¶å˜æ€§]
    
    B --> B1["æŒ‰ä¸šåŠ¡ä¸»é¢˜ç»„ç»‡"]
    C --> C1["å¤šæºæ•°æ®æ•´åˆ"]
    D --> D1["æ•°æ®ç›¸å¯¹ç¨³å®š"]
    E --> E1["åæ˜ å†å²å˜åŒ–"]
```

#### æ•°æ®ä»“åº“ä¸ä¼ ç»Ÿæ•°æ®åº“çš„åŒºåˆ«
```java title="æ•°æ®ä»“åº“ç‰¹å¾ç¤ºä¾‹"
public class DataWarehouseCharacteristics {
    public static void main(String[] args) {
        // 1. é¢å‘ä¸»é¢˜
        System.out.println("æ•°æ®ä»“åº“æŒ‰ä¸šåŠ¡ä¸»é¢˜ç»„ç»‡ï¼Œå¦‚å®¢æˆ·ã€äº§å“ã€é”€å”®ç­‰");
        
        // 2. é›†æˆæ€§
        System.out.println("æ•´åˆæ¥è‡ªå¤šä¸ªä¸šåŠ¡ç³»ç»Ÿçš„æ•°æ®ï¼Œæ¶ˆé™¤ä¸ä¸€è‡´æ€§");
        
        // 3. éæ˜“å¤±æ€§
        System.out.println("æ•°æ®ä¸€æ—¦è¿›å…¥æ•°æ®ä»“åº“ï¼Œé€šå¸¸ä¸ä¼šä¿®æ”¹æˆ–åˆ é™¤");
        
        // 4. æ—¶å˜æ€§
        System.out.println("æ•°æ®ä»“åº“ä¸­çš„æ•°æ®åæ˜ å†å²å˜åŒ–ï¼Œæ”¯æŒæ—¶é—´åºåˆ—åˆ†æ");
        
        // 5. å†³ç­–æ”¯æŒ
        System.out.println("ä¸»è¦ç”¨äºåˆ†ææŸ¥è¯¢ï¼Œæ”¯æŒå¤æ‚çš„OLAPæ“ä½œ");
    }
}
```

### 1.2 æ•°æ®ä»“åº“æ¶æ„

æ•°æ®ä»“åº“é€šå¸¸é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼š

#### 1.2.1 ç°ä»£æ•°æ®ä»“åº“æ¶æ„æ¼”è¿›
```mermaid
graph TB
    A[æ•°æ®ä»“åº“æ¼”è¿›] --> B[ä¼ ç»Ÿæ•°ä»“]
    A --> C[æ•°æ®æ¹–]
    A --> D[æ¹–ä»“ä¸€ä½“]
    A --> E[æ•°æ®ç½‘æ ¼]
    
    B --> B1["ETL + æ˜Ÿå‹æ¨¡å¼"]
    B --> B2["é¢„å®šä¹‰Schema"]
    B --> B3["æ‰¹å¤„ç†ä¸ºä¸»"]
    
    C --> C1["ELT + åŸå§‹æ•°æ®"]
    C --> C2["Schema on Read"]
    C --> C3["å¤šæ ¼å¼æ”¯æŒ"]
    
    D --> D1["ç»Ÿä¸€å­˜å‚¨"]
    D --> D2["ç»Ÿä¸€è®¡ç®—"]
    D --> D3["ç»Ÿä¸€æ²»ç†"]
    
    E --> E1["å»ä¸­å¿ƒåŒ–"]
    E --> E2["æ•°æ®è‡ªæ²»"]
    E --> E3["é¢†åŸŸé©±åŠ¨"]
```

#### 1.2.2 æ¹–ä»“ä¸€ä½“æ¶æ„è®¾è®¡
```java title="æ¹–ä»“ä¸€ä½“æ¶æ„ç¤ºä¾‹"
public class DataLakehouseArchitecture {
    private final StorageLayer storageLayer;
    private final ProcessingLayer processingLayer;
    private final GovernanceLayer governanceLayer;
    private final QueryEngine queryEngine;
    
    public DataLakehouseArchitecture(StorageLayer storageLayer,
                                   ProcessingLayer processingLayer,
                                   GovernanceLayer governanceLayer,
                                   QueryEngine queryEngine) {
        this.storageLayer = storageLayer;
        this.processingLayer = processingLayer;
        this.governanceLayer = governanceLayer;
        this.queryEngine = queryEngine;
    }
    
    public void ingestData(DataSource source) {
        // 1. åŸå§‹æ•°æ®å­˜å‚¨åˆ°æ•°æ®æ¹–
        String rawPath = storageLayer.storeRaw(source.getData(), source.getFormat());
        
        // 2. å…ƒæ•°æ®æ³¨å†Œ
        Metadata metadata = new Metadata();
        metadata.setSource(source.getName());
        metadata.setIngestTime(LocalDateTime.now());
        metadata.setRawPath(rawPath);
        metadata.setSchema(source.getSchema());
        metadata.setFormat(source.getFormat());
        metadata.setSize(source.getData().size());
        
        governanceLayer.registerMetadata(metadata);
        
        // 3. æ•°æ®è´¨é‡æ£€æŸ¥
        DataQualityReport qualityReport = governanceLayer.checkQuality(source.getData());
        
        if (qualityReport.isValid()) {
            // 4. æ•°æ®è½¬æ¢å’Œä¼˜åŒ–
            String processedPath = processingLayer.process(source.getData(), source.getSchema());
            
            // 5. åˆ›å»ºDeltaè¡¨
            createDeltaTable(processedPath, source.getSchema());
            
            // 6. æ›´æ–°å…ƒæ•°æ®
            metadata.setProcessedPath(processedPath);
            metadata.setStatus("PROCESSED");
            governanceLayer.updateMetadata(metadata);
            
            // 7. åˆ›å»ºç‰©åŒ–è§†å›¾
            createMaterializedViews(processedPath);
            
        } else {
            // 8. æ•°æ®è´¨é‡é—®é¢˜å¤„ç†
            governanceLayer.handleQualityIssues(qualityReport);
            metadata.setStatus("FAILED");
            metadata.setQualityIssues(qualityReport.getIssues());
            governanceLayer.updateMetadata(metadata);
        }
    }
    
    private void createDeltaTable(String dataPath, StructType schema) {
        // ä½¿ç”¨Delta Lakeåˆ›å»ºè¡¨
        DeltaTable deltaTable = DeltaTable.createIfNotExists()
            .tableName("raw_data")
            .addColumn("id", DataTypes.StringType)
            .addColumn("data", DataTypes.StringType)
            .addColumn("timestamp", DataTypes.TimestampType)
            .location(dataPath)
            .execute();
        
        System.out.println("Delta table created at: " + dataPath);
    }
    
    private void createMaterializedViews(String tablePath) {
        // åˆ›å»ºå¸¸ç”¨æŸ¥è¯¢çš„ç‰©åŒ–è§†å›¾
        String viewSQL = "CREATE MATERIALIZED VIEW user_summary AS " +
                        "SELECT user_id, COUNT(*) as event_count, " +
                        "AVG(amount) as avg_amount " +
                        "FROM raw_data " +
                        "GROUP BY user_id";
        
        queryEngine.executeSQL(viewSQL);
        System.out.println("Materialized view created");
    }
}
```

```mermaid
graph TB
    A[æ•°æ®ä»“åº“æ¶æ„] --> B[æ•°æ®æºå±‚]
    A --> C[æ•°æ®æŠ½å–å±‚]
    A --> D[æ•°æ®å­˜å‚¨å±‚]
    A --> E[æ•°æ®æœåŠ¡å±‚]
    A --> F[æ•°æ®åº”ç”¨å±‚]
    
    B --> B1["ä¸šåŠ¡ç³»ç»Ÿ"]
    B --> B2["å¤–éƒ¨æ•°æ®"]
    B --> B3["æ–‡ä»¶æ•°æ®"]
    
    C --> C1["ETLå·¥å…·"]
    C --> C2["æ•°æ®æ¸…æ´—"]
    C --> C3["æ•°æ®è½¬æ¢"]
    
    D --> D1["ODSæ“ä½œæ•°æ®å­˜å‚¨"]
    D --> D2["DWæ•°æ®ä»“åº“"]
    D --> D3["DMæ•°æ®é›†å¸‚"]
    
    E --> E1["æŸ¥è¯¢æœåŠ¡"]
    E --> E2["APIæ¥å£"]
    
    F --> F1["æŠ¥è¡¨ç³»ç»Ÿ"]
    F --> F2["åˆ†æå·¥å…·"]
    F --> F3["æ•°æ®æŒ–æ˜"]
```

## 2. ETLæµç¨‹è®¾è®¡

### 2.1 ETLåŸºæœ¬æµç¨‹

ETLæ˜¯æ•°æ®ä»“åº“å»ºè®¾çš„æ ¸å¿ƒç¯èŠ‚ï¼š

```mermaid
graph LR
    A[æ•°æ®æº] --> B[ExtractæŠ½å–]
    B --> C[Transformè½¬æ¢]
    C --> D[LoadåŠ è½½]
    D --> E[ç›®æ ‡ç³»ç»Ÿ]
    
    B --> B1["å…¨é‡æŠ½å–"]
    B --> B2["å¢é‡æŠ½å–"]
    
    C --> C1["æ•°æ®æ¸…æ´—"]
    C --> C2["æ•°æ®è½¬æ¢"]
    C --> C3["æ•°æ®èšåˆ"]
    
    D --> D1["æ‰¹é‡åŠ è½½"]
    D --> D2["å®æ—¶åŠ è½½"]
```

### 2.2 ETLå®ç°ç¤ºä¾‹

<Tabs>
  <TabItem value="extract" label="æ•°æ®æŠ½å–" default>
  ```java title="æ•°æ®æŠ½å–ç¤ºä¾‹"
  public class DataExtractor {
      public List<Customer> extractCustomers(String sourceType) {
          List<Customer> customers = new ArrayList<>();
          
          switch (sourceType) {
              case "database":
                  customers = extractFromDatabase();
                  break;
              case "file":
                  customers = extractFromFile();
                  break;
              case "api":
                  customers = extractFromAPI();
                  break;
              default:
                  throw new IllegalArgumentException("Unsupported source type");
          }
          
          return customers;
      }
      
      private List<Customer> extractFromDatabase() {
          // ä»æ•°æ®åº“æŠ½å–å®¢æˆ·æ•°æ®
          String sql = "SELECT id, name, email, phone FROM customers";
          // æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›ç»“æœ
          return executeQuery(sql);
      }
      
      private List<Customer> extractFromFile() {
          // ä»æ–‡ä»¶æŠ½å–å®¢æˆ·æ•°æ®
          List<Customer> customers = new ArrayList<>();
          try (BufferedReader reader = new BufferedReader(
                  new FileReader("customers.csv"))) {
              String line;
              while ((line = reader.readLine()) != null) {
                  String[] fields = line.split(",");
                  customers.add(new Customer(fields[0], fields[1], fields[2], fields[3]));
              }
          } catch (IOException e) {
              throw new RuntimeException("Failed to read file", e);
          }
          return customers;
      }
  }
  ```
  </TabItem>
  <TabItem value="transform" label="æ•°æ®è½¬æ¢">
  ```java title="æ•°æ®è½¬æ¢ç¤ºä¾‹"
  public class DataTransformer {
      public List<Customer> transformCustomers(List<Customer> rawCustomers) {
          return rawCustomers.stream()
              .filter(this::validateCustomer)           // æ•°æ®éªŒè¯
              .map(this::standardizeCustomer)           // æ•°æ®æ ‡å‡†åŒ–
              .map(this::enrichCustomer)                // æ•°æ®ä¸°å¯Œ
              .collect(Collectors.toList());
      }
      
      private boolean validateCustomer(Customer customer) {
          // éªŒè¯å®¢æˆ·æ•°æ®å®Œæ•´æ€§
          return customer.getId() != null && 
                 customer.getName() != null && 
                 customer.getEmail() != null &&
                 isValidEmail(customer.getEmail());
      }
      
      private Customer standardizeCustomer(Customer customer) {
          // æ ‡å‡†åŒ–å®¢æˆ·æ•°æ®
          return new Customer(
              customer.getId(),
              customer.getName().trim().toLowerCase(),
              customer.getEmail().toLowerCase(),
              standardizePhone(customer.getPhone())
          );
      }
      
      private Customer enrichCustomer(Customer customer) {
          // ä¸°å¯Œå®¢æˆ·æ•°æ®
          String region = determineRegion(customer.getPhone());
          String segment = determineSegment(customer);
          
          return new EnrichedCustomer(customer, region, segment);
      }
      
      private boolean isValidEmail(String email) {
          return email.matches("^[A-Za-z0-9+_.-]+@(.+)$");
      }
  }
  ```
  </TabItem>
  <TabItem value="load" label="æ•°æ®åŠ è½½">
  ```java title="æ•°æ®åŠ è½½ç¤ºä¾‹"
  public class DataLoader {
      public void loadCustomers(List<Customer> customers, String targetType) {
          switch (targetType) {
              case "warehouse":
                  loadToWarehouse(customers);
                  break;
              case "mart":
                  loadToDataMart(customers);
                  break;
              case "cache":
                  loadToCache(customers);
                  break;
              default:
                  throw new IllegalArgumentException("Unsupported target type");
          }
      }
      
      private void loadToWarehouse(List<Customer> customers) {
          // æ‰¹é‡åŠ è½½åˆ°æ•°æ®ä»“åº“
          try (Connection conn = getWarehouseConnection()) {
              conn.setAutoCommit(false);
              
              String sql = "INSERT INTO dim_customers (id, name, email, phone, created_date) " +
                          "VALUES (?, ?, ?, ?, ?) " +
                          "ON DUPLICATE KEY UPDATE " +
                          "name = VALUES(name), email = VALUES(email), " +
                          "phone = VALUES(phone), updated_date = NOW()";
              
              try (PreparedStatement stmt = conn.prepareStatement(sql)) {
                  for (Customer customer : customers) {
                      stmt.setString(1, customer.getId());
                      stmt.setString(2, customer.getName());
                      stmt.setString(3, customer.getEmail());
                      stmt.setString(4, customer.getPhone());
                      stmt.setTimestamp(5, new Timestamp(System.currentTimeMillis()));
                      stmt.addBatch();
                  }
                  stmt.executeBatch();
                  conn.commit();
              }
          } catch (SQLException e) {
              throw new RuntimeException("Failed to load data to warehouse", e);
          }
      }
  }
  ```
  </TabItem>
</Tabs>

## 3. æ•°æ®å»ºæ¨¡

### 3.1 ç»´åº¦å»ºæ¨¡

ç»´åº¦å»ºæ¨¡æ˜¯æ•°æ®ä»“åº“è®¾è®¡çš„æ ¸å¿ƒæ–¹æ³•ï¼š

#### 3.1.1 ç°ä»£ç»´åº¦å»ºæ¨¡æ¨¡å¼
```mermaid
graph TB
    A[ç»´åº¦å»ºæ¨¡æ¨¡å¼] --> B[æ˜Ÿå‹æ¨¡å¼]
    A --> C[é›ªèŠ±æ¨¡å¼]
    A --> D[æ˜Ÿåº§æ¨¡å¼]
    A --> E[æ•°æ®ä»“åº“æ€»çº¿]
    
    B --> B1["ç®€å•ç›´è§‚"]
    B --> B2["æŸ¥è¯¢æ€§èƒ½å¥½"]
    B --> B3["ç»´æŠ¤ç®€å•"]
    
    C --> C1["è§„èŒƒåŒ–è®¾è®¡"]
    C --> C2["å­˜å‚¨ç©ºé—´å°‘"]
    C --> C3["æŸ¥è¯¢å¤æ‚"]
    
    D --> D1["å¤šäº‹å®è¡¨"]
    D --> D2["å…±äº«ç»´åº¦"]
    D --> D3["å¤æ‚ä¸šåŠ¡"]
    
    E --> E1["ä¸€è‡´æ€§ç»´åº¦"]
    E --> E2["æ ‡å‡†äº‹å®"]
    E --> E3["ä¼ä¸šçº§è®¾è®¡"]
```

#### 3.1.2 æ•°æ®ä»“åº“æ€»çº¿æ¶æ„
```java title="æ•°æ®ä»“åº“æ€»çº¿æ¶æ„ç¤ºä¾‹"
public class DataWarehouseBusArchitecture {
    private final List<ConformedDimension> conformedDimensions;
    private final List<StandardFact> standardFacts;
    private final DataModelValidator validator;
    
    public DataWarehouseBusArchitecture() {
        this.conformedDimensions = new ArrayList<>();
        this.standardFacts = new ArrayList<>();
        this.validator = new DataModelValidator();
    }
    
    public void addConformedDimension(ConformedDimension dimension) {
        // éªŒè¯ä¸€è‡´æ€§ç»´åº¦
        if (validator.validateConformedDimension(dimension)) {
            conformedDimensions.add(dimension);
            System.out.println("Conformed dimension added: " + dimension.getName());
        } else {
            throw new IllegalArgumentException("Invalid conformed dimension: " + dimension.getName());
        }
    }
    
    public void addStandardFact(StandardFact fact) {
        // éªŒè¯æ ‡å‡†äº‹å®
        if (validator.validateStandardFact(fact)) {
            standardFacts.add(fact);
            System.out.println("Standard fact added: " + fact.getName());
        } else {
            throw new IllegalArgumentException("Invalid standard fact: " + fact.getName());
        }
    }
    
    public void buildDataMarts() {
        // åŸºäºä¸€è‡´æ€§ç»´åº¦å’Œæ ‡å‡†äº‹å®æ„å»ºæ•°æ®é›†å¸‚
        for (StandardFact fact : standardFacts) {
            DataMart dataMart = new DataMart(fact.getName());
            
            // æ·»åŠ ç›¸å…³ç»´åº¦
            for (ConformedDimension dimension : conformedDimensions) {
                if (fact.usesDimension(dimension)) {
                    dataMart.addDimension(dimension);
                }
            }
            
            // æ„å»ºæ•°æ®é›†å¸‚
            dataMart.build();
            System.out.println("Data mart built: " + dataMart.getName());
        }
    }
}

// ä¸€è‡´æ€§ç»´åº¦
public class ConformedDimension {
    private String name;
    private String businessKey;
    private List<Attribute> attributes;
    private List<Hierarchy> hierarchies;
    private SCDType scdType;
    
    public ConformedDimension(String name, String businessKey) {
        this.name = name;
        this.businessKey = businessKey;
        this.attributes = new ArrayList<>();
        this.hierarchies = new ArrayList<>();
    }
    
    public void addAttribute(Attribute attribute) {
        attributes.add(attribute);
    }
    
    public void addHierarchy(Hierarchy hierarchy) {
        hierarchies.add(hierarchy);
    }
    
    public void setSCDType(SCDType scdType) {
        this.scdType = scdType;
    }
    
    // getters...
}

// æ ‡å‡†äº‹å®
public class StandardFact {
    private String name;
    private List<Measure> measures;
    private List<String> dimensionKeys;
    private Grain grain;
    
    public StandardFact(String name) {
        this.name = name;
        this.measures = new ArrayList<>();
        this.dimensionKeys = new ArrayList<>();
    }
    
    public void addMeasure(Measure measure) {
        measures.add(measure);
    }
    
    public void addDimensionKey(String dimensionKey) {
        dimensionKeys.add(dimensionKey);
    }
    
    public void setGrain(Grain grain) {
        this.grain = grain;
    }
    
    public boolean usesDimension(ConformedDimension dimension) {
        return dimensionKeys.contains(dimension.getBusinessKey());
    }
    
    // getters...
}
```

```mermaid
graph TB
    A[ç»´åº¦å»ºæ¨¡] --> B[äº‹å®è¡¨]
    A --> C[ç»´åº¦è¡¨]
    A --> D[ç¼“æ…¢å˜åŒ–ç»´åº¦]
    
    B --> B1["åº¦é‡å€¼"]
    B --> B2["å¤–é”®"]
    
    C --> C1["æè¿°æ€§å±æ€§"]
    C --> C2["å±‚æ¬¡ç»“æ„"]
    
    D --> D1["ç±»å‹1: è¦†ç›–"]
    D --> D2["ç±»å‹2: æ–°å¢è®°å½•"]
    D --> D3["ç±»å‹3: æ–°å¢å­—æ®µ"]
```

### 3.2 æ˜Ÿå‹æ¨¡å¼ç¤ºä¾‹

<div className="code-with-callout">

```java title="æ˜Ÿå‹æ¨¡å¼æ•°æ®æ¨¡å‹ç¤ºä¾‹"
public class StarSchemaModel {
    // äº‹å®è¡¨ - é”€å”®äº‹å®
    public static class SalesFact {
        private String factId;
        private String customerId;      // å®¢æˆ·ç»´åº¦å¤–é”®
        private String productId;       // äº§å“ç»´åº¦å¤–é”®
        private String timeId;          // æ—¶é—´ç»´åº¦å¤–é”®
        private String storeId;         // å•†åº—ç»´åº¦å¤–é”®
        private BigDecimal quantity;    // æ•°é‡åº¦é‡
        private BigDecimal amount;      // é‡‘é¢åº¦é‡
        private BigDecimal cost;        // æˆæœ¬åº¦é‡
        
        // æ„é€ å‡½æ•°ã€getterã€setter...
    }
    
    // ç»´åº¦è¡¨ - å®¢æˆ·ç»´åº¦
    public static class CustomerDimension {
        private String customerId;
        private String customerName;
        private String customerType;
        private String region;
        private String city;
        private String country;
        private String segment;
        private Date effectiveDate;
        private Date expiryDate;
        private boolean isCurrent;
        
        // æ„é€ å‡½æ•°ã€getterã€setter...
    }
    
    // ç»´åº¦è¡¨ - äº§å“ç»´åº¦
    public static class ProductDimension {
        private String productId;
        private String productName;
        private String category;
        private String subcategory;
        private String brand;
        private String color;
        private String size;
        private BigDecimal unitPrice;
        
        // æ„é€ å‡½æ•°ã€getterã€setter...
    }
    
    // ç»´åº¦è¡¨ - æ—¶é—´ç»´åº¦
    public static class TimeDimension {
        private String timeId;
        private Date fullDate;
        private int year;
        private int quarter;
        private int month;
        private int day;
        private String dayOfWeek;
        private boolean isWeekend;
        private boolean isHoliday;
        
        // æ„é€ å‡½æ•°ã€getterã€setter...
    }
}
```

:::info æ˜Ÿå‹æ¨¡å¼ä¼˜åŠ¿
æ˜Ÿå‹æ¨¡å¼ç®€å•ç›´è§‚ï¼ŒæŸ¥è¯¢æ€§èƒ½å¥½ï¼Œé€‚åˆOLAPåˆ†æï¼Œæ˜¯æ•°æ®ä»“åº“è®¾è®¡ä¸­æœ€å¸¸ç”¨çš„æ¨¡å¼ã€‚
:::
</div>

## 4. æ•°æ®è´¨é‡ä¿è¯

### 4.1 æ•°æ®è´¨é‡ç»´åº¦

æ•°æ®è´¨é‡åŒ…å«å¤šä¸ªç»´åº¦ï¼š

#### 4.1.1 æ•°æ®è´¨é‡è¯„ä¼°æ¡†æ¶
```mermaid
graph TB
    A[æ•°æ®è´¨é‡è¯„ä¼°] --> B[æ•°æ®å®Œæ•´æ€§]
    A --> C[æ•°æ®å‡†ç¡®æ€§]
    A --> D[æ•°æ®ä¸€è‡´æ€§]
    A --> E[æ•°æ®åŠæ—¶æ€§]
    A --> F[æ•°æ®æœ‰æ•ˆæ€§]
    A --> G[æ•°æ®å”¯ä¸€æ€§]
    
    B --> B1["ç©ºå€¼æ£€æŸ¥"]
    B --> B2["å¿…å¡«å­—æ®µéªŒè¯"]
    B --> B3["æ•°æ®é•¿åº¦æ£€æŸ¥"]
    
    C --> C1["ä¸šåŠ¡è§„åˆ™éªŒè¯"]
    C --> C2["èŒƒå›´æ£€æŸ¥"]
    C --> C3["æ ¼å¼éªŒè¯"]
    
    D --> D1["è·¨ç³»ç»Ÿä¸€è‡´æ€§"]
    D --> D2["å†å²æ•°æ®ä¸€è‡´æ€§"]
    D --> D3["å¼•ç”¨å®Œæ•´æ€§"]
    
    E --> E1["æ•°æ®æ–°é²œåº¦"]
    E --> E2["æ›´æ–°é¢‘ç‡"]
    
    F --> F1["æ•°æ®ç±»å‹æ£€æŸ¥"]
    F --> F2["æ ¼å¼è§„èŒƒ"]
    
    G --> G1["é‡å¤æ•°æ®æ£€æµ‹"]
    G --> G2["ä¸»é”®å”¯ä¸€æ€§"]
```

#### 4.1.2 æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿ
```java title="æ•°æ®è´¨é‡ç›‘æ§ç³»ç»Ÿç¤ºä¾‹"
public class DataQualityMonitoringSystem {
    private final List<QualityRule> qualityRules;
    private final QualityMetricsCollector metricsCollector;
    private final AlertSystem alertSystem;
    private final QualityDashboard dashboard;
    
    public DataQualityMonitoringSystem() {
        this.qualityRules = new ArrayList<>();
        this.metricsCollector = new QualityMetricsCollector();
        this.alertSystem = new AlertSystem();
        this.dashboard = new QualityDashboard();
    }
    
    public void addQualityRule(QualityRule rule) {
        qualityRules.add(rule);
        System.out.println("Quality rule added: " + rule.getName());
    }
    
    public DataQualityReport monitorDataQuality(Dataset<Row> data, String datasetName) {
        DataQualityReport report = new DataQualityReport(datasetName);
        
        // 1. æ‰§è¡Œæ‰€æœ‰è´¨é‡è§„åˆ™
        for (QualityRule rule : qualityRules) {
            QualityCheckResult result = rule.execute(data);
            report.addCheckResult(result);
            
            // 2. æ”¶é›†è´¨é‡æŒ‡æ ‡
            metricsCollector.collectMetrics(result);
            
            // 3. æ£€æŸ¥å‘Šè­¦æ¡ä»¶
            if (result.getSeverity() == Severity.CRITICAL) {
                alertSystem.sendAlert(new Alert(
                    AlertType.DATA_QUALITY,
                    Severity.CRITICAL,
                    "Critical data quality issue in " + datasetName,
                    result.getDetails()
                ));
            }
        }
        
        // 4. æ›´æ–°ä»ªè¡¨æ¿
        dashboard.updateMetrics(report);
        
        // 5. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        generateQualityReport(report);
        
        return report;
    }
    
    private void generateQualityReport(DataQualityReport report) {
        // ç”Ÿæˆè¯¦ç»†çš„è´¨é‡æŠ¥å‘Š
        StringBuilder reportBuilder = new StringBuilder();
        reportBuilder.append("=== Data Quality Report ===\n");
        reportBuilder.append("Dataset: ").append(report.getDatasetName()).append("\n");
        reportBuilder.append("Timestamp: ").append(LocalDateTime.now()).append("\n");
        reportBuilder.append("Overall Score: ").append(report.getOverallScore()).append("\n\n");
        
        for (QualityCheckResult result : report.getCheckResults()) {
            reportBuilder.append("Rule: ").append(result.getRuleName()).append("\n");
            reportBuilder.append("Status: ").append(result.getStatus()).append("\n");
            reportBuilder.append("Severity: ").append(result.getSeverity()).append("\n");
            reportBuilder.append("Details: ").append(result.getDetails()).append("\n\n");
        }
        
        System.out.println(reportBuilder.toString());
    }
}

// æ•°æ®è´¨é‡è§„åˆ™æ¥å£
public interface QualityRule {
    String getName();
    QualityCheckResult execute(Dataset<Row> data);
}

// å®Œæ•´æ€§æ£€æŸ¥è§„åˆ™
public class CompletenessRule implements QualityRule {
    private final String columnName;
    private final double threshold;
    
    public CompletenessRule(String columnName, double threshold) {
        this.columnName = columnName;
        this.threshold = threshold;
    }
    
    @Override
    public String getName() {
        return "Completeness Check for " + columnName;
    }
    
    @Override
    public QualityCheckResult execute(Dataset<Row> data) {
        long totalRows = data.count();
        long nonNullRows = data.filter(col(columnName).isNotNull()).count();
        double completeness = (double) nonNullRows / totalRows;
        
        boolean passed = completeness >= threshold;
        Severity severity = passed ? Severity.INFO : 
                           (completeness >= threshold * 0.8 ? Severity.WARNING : Severity.CRITICAL);
        
        return new QualityCheckResult(
            getName(),
            passed ? Status.PASSED : Status.FAILED,
            severity,
            String.format("Completeness: %.2f%% (threshold: %.2f%%)", 
                         completeness * 100, threshold * 100)
        );
    }
}

// ä¸€è‡´æ€§æ£€æŸ¥è§„åˆ™
public class ConsistencyRule implements QualityRule {
    private final String columnName;
    private final String referenceColumn;
    private final String referenceTable;
    
    public ConsistencyRule(String columnName, String referenceColumn, String referenceTable) {
        this.columnName = columnName;
        this.referenceColumn = referenceColumn;
        this.referenceTable = referenceTable;
    }
    
    @Override
    public String getName() {
        return "Referential Integrity Check: " + columnName + " -> " + referenceTable + "." + referenceColumn;
    }
    
    @Override
    public QualityCheckResult execute(Dataset<Row> data) {
        // æ£€æŸ¥å¼•ç”¨å®Œæ•´æ€§
        Dataset<Row> referenceData = spark.table(referenceTable);
        
        long totalRows = data.count();
        long validRows = data.join(referenceData, 
            data.col(columnName).equalTo(referenceData.col(referenceColumn)), "left_anti")
            .count();
        
        double consistency = (double) (totalRows - validRows) / totalRows;
        boolean passed = consistency >= 0.95; // 95%ä¸€è‡´æ€§é˜ˆå€¼
        
        return new QualityCheckResult(
            getName(),
            passed ? Status.PASSED : Status.FAILED,
            passed ? Severity.INFO : Severity.CRITICAL,
            String.format("Consistency: %.2f%%", consistency * 100)
        );
    }
}
```

| è´¨é‡ç»´åº¦ | æè¿° | æ£€æŸ¥æ–¹æ³• |
|----------|------|----------|
| **å®Œæ•´æ€§** | æ•°æ®æ˜¯å¦å®Œæ•´ï¼Œæ— ç¼ºå¤± | ç©ºå€¼æ£€æŸ¥ã€å¿…å¡«å­—æ®µéªŒè¯ |
| **å‡†ç¡®æ€§** | æ•°æ®æ˜¯å¦æ­£ç¡®ã€çœŸå® | ä¸šåŠ¡è§„åˆ™éªŒè¯ã€èŒƒå›´æ£€æŸ¥ |
| **ä¸€è‡´æ€§** | æ•°æ®åœ¨ä¸åŒç³»ç»Ÿä¸­æ˜¯å¦ä¸€è‡´ | è·¨ç³»ç»Ÿæ•°æ®å¯¹æ¯” |
| **åŠæ—¶æ€§** | æ•°æ®æ˜¯å¦åŠæ—¶æ›´æ–° | æ•°æ®æ–°é²œåº¦æ£€æŸ¥ |
| **æœ‰æ•ˆæ€§** | æ•°æ®æ ¼å¼æ˜¯å¦ç¬¦åˆè§„èŒƒ | æ ¼å¼éªŒè¯ã€ç±»å‹æ£€æŸ¥ |

### 4.2 æ•°æ®è´¨é‡æ£€æŸ¥å®ç°

```java title="æ•°æ®è´¨é‡æ£€æŸ¥ç¤ºä¾‹"
public class DataQualityChecker {
    public DataQualityReport checkDataQuality(List<Customer> customers) {
        DataQualityReport report = new DataQualityReport();
        
        // 1. å®Œæ•´æ€§æ£€æŸ¥
        long nullNameCount = customers.stream()
            .filter(c -> c.getName() == null || c.getName().trim().isEmpty())
            .count();
        report.addIssue("å®Œæ•´æ€§", "å§“åç¼ºå¤±", nullNameCount);
        
        // 2. å‡†ç¡®æ€§æ£€æŸ¥
        long invalidEmailCount = customers.stream()
            .filter(c -> !isValidEmail(c.getEmail()))
            .count();
        report.addIssue("å‡†ç¡®æ€§", "é‚®ç®±æ ¼å¼æ— æ•ˆ", invalidEmailCount);
        
        // 3. ä¸€è‡´æ€§æ£€æŸ¥
        long duplicateIdCount = customers.stream()
            .collect(Collectors.groupingBy(Customer::getId))
            .values().stream()
            .filter(list -> list.size() > 1)
            .count();
        report.addIssue("ä¸€è‡´æ€§", "å®¢æˆ·IDé‡å¤", duplicateIdCount);
        
        // 4. æœ‰æ•ˆæ€§æ£€æŸ¥
        long invalidPhoneCount = customers.stream()
            .filter(c -> !isValidPhone(c.getPhone()))
            .count();
        report.addIssue("æœ‰æ•ˆæ€§", "ç”µè¯å·ç æ ¼å¼æ— æ•ˆ", invalidPhoneCount);
        
        return report;
    }
    
    private boolean isValidEmail(String email) {
        return email != null && email.matches("^[A-Za-z0-9+_.-]+@(.+)$");
    }
    
    private boolean isValidPhone(String phone) {
        return phone != null && phone.matches("^\\+?[1-9]\\d{1,14}$");
    }
}
```

## 5. ETLå·¥å…·å’Œæ¡†æ¶

### 5.1 ä¸»æµETLå·¥å…·

<div className="card">
<div className="card__header">
<h4>ETLå·¥å…·å¯¹æ¯”</h4>
</div>
<div className="card__body">
<ol>
<li><strong>Apache NiFi</strong>ï¼šå¼€æºæ•°æ®æµå·¥å…·ï¼Œæ”¯æŒå®æ—¶æ•°æ®å¤„ç†</li>
<li><strong>Apache Airflow</strong>ï¼šPythonç¼–å†™çš„ä»»åŠ¡è°ƒåº¦å¹³å°</li>
<li><strong>Talend</strong>ï¼šä¼ä¸šçº§æ•°æ®é›†æˆå¹³å°ï¼Œæä¾›å›¾å½¢åŒ–ç•Œé¢</li>
<li><strong>Informatica</strong>ï¼šå•†ä¸šETLå·¥å…·ï¼ŒåŠŸèƒ½å¼ºå¤§ä½†æˆæœ¬è¾ƒé«˜</li>
<li><strong>DataStage</strong>ï¼šIBMçš„æ•°æ®é›†æˆå¹³å°</li>
</ol>
</div>
</div>

### 5.2 è‡ªå®šä¹‰ETLæ¡†æ¶

```java title="è‡ªå®šä¹‰ETLæ¡†æ¶ç¤ºä¾‹"
public class ETLFramework {
    public void executeETL(ETLJob job) {
        try {
            // 1. æ‰§è¡ŒæŠ½å–
            log.info("Starting extraction phase...");
            List<RawData> rawData = job.getExtractor().extract();
            log.info("Extraction completed. Records: " + rawData.size());
            
            // 2. æ‰§è¡Œè½¬æ¢
            log.info("Starting transformation phase...");
            List<TransformedData> transformedData = job.getTransformer().transform(rawData);
            log.info("Transformation completed. Records: " + transformedData.size());
            
            // 3. æ‰§è¡ŒåŠ è½½
            log.info("Starting loading phase...");
            job.getLoader().load(transformedData);
            log.info("Loading completed successfully");
            
            // 4. æ›´æ–°å…ƒæ•°æ®
            updateMetadata(job, rawData.size(), transformedData.size());
            
        } catch (Exception e) {
            log.error("ETL job failed", e);
            handleError(job, e);
            throw new ETLException("ETL job execution failed", e);
        }
    }
    
    private void updateMetadata(ETLJob job, int extractedCount, int transformedCount) {
        ETLMetadata metadata = new ETLMetadata();
        metadata.setJobId(job.getJobId());
        metadata.setExecutionTime(LocalDateTime.now());
        metadata.setExtractedRecords(extractedCount);
        metadata.setTransformedRecords(transformedCount);
        metadata.setStatus("SUCCESS");
        
        metadataRepository.save(metadata);
    }
    
    private void handleError(ETLJob job, Exception e) {
        ETLMetadata metadata = new ETLMetadata();
        metadata.setJobId(job.getJobId());
        metadata.setExecutionTime(LocalDateTime.now());
        metadata.setStatus("FAILED");
        metadata.setErrorMessage(e.getMessage());
        
        metadataRepository.save(metadata);
    }
}
```

## 6. å®æ—¶ETLå’Œæµå¤„ç†

### 6.1 å®æ—¶ETLæ¶æ„

ç°ä»£æ•°æ®ä»“åº“éœ€è¦æ”¯æŒå®æ—¶æ•°æ®å¤„ç†ï¼š

#### 6.1.1 å®æ—¶ETLæŠ€æœ¯æ ˆå¯¹æ¯”
```mermaid
graph TB
    A[å®æ—¶ETLæŠ€æœ¯æ ˆ] --> B[æµå¤„ç†å¼•æ“]
    A --> C[æ¶ˆæ¯é˜Ÿåˆ—]
    A --> D[å®æ—¶å­˜å‚¨]
    A --> E[å®æ—¶æŸ¥è¯¢]
    
    B --> B1["Apache Flink"]
    B --> B2["Spark Streaming"]
    B --> B3["Kafka Streams"]
    
    C --> C1["Apache Kafka"]
    C --> C2["Apache Pulsar"]
    C --> C3["RabbitMQ"]
    
    D --> D1["ClickHouse"]
    D --> D2["Apache Druid"]
    D --> D3["Redis"]
    
    E --> E1["å®æ—¶ä»ªè¡¨æ¿"]
    E --> E2["æµå¼API"]
    E --> E3["å‘Šè­¦ç³»ç»Ÿ"]
```

#### 6.1.2 å®æ—¶ETLæ¶æ„è®¾è®¡
```java title="å®æ—¶ETLæ¶æ„è®¾è®¡ç¤ºä¾‹"
public class RealTimeETLArchitecture {
    private final StreamProcessor streamProcessor;
    private final MessageQueue messageQueue;
    private final RealTimeStorage realTimeStorage;
    private final DataQualityChecker qualityChecker;
    private final MonitoringSystem monitoringSystem;
    
    public RealTimeETLArchitecture(StreamProcessor streamProcessor,
                                  MessageQueue messageQueue,
                                  RealTimeStorage realTimeStorage,
                                  DataQualityChecker qualityChecker,
                                  MonitoringSystem monitoringSystem) {
        this.streamProcessor = streamProcessor;
        this.messageQueue = messageQueue;
        this.realTimeStorage = realTimeStorage;
        this.qualityChecker = qualityChecker;
        this.monitoringSystem = monitoringSystem;
    }
    
    public void buildRealTimeETL() {
        // 1. é…ç½®æ•°æ®æºè¿æ¥
        configureDataSources();
        
        // 2. è®¾ç½®æµå¤„ç†ç®¡é“
        setupStreamingPipeline();
        
        // 3. é…ç½®æ•°æ®è´¨é‡æ£€æŸ¥
        setupQualityChecks();
        
        // 4. è®¾ç½®ç›‘æ§å’Œå‘Šè­¦
        setupMonitoring();
        
        // 5. å¯åŠ¨å®æ—¶ETL
        startRealTimeETL();
    }
    
    private void configureDataSources() {
        // é…ç½®å¤šç§æ•°æ®æº
        DataSourceConfig dbConfig = new DataSourceConfig("database", "jdbc:mysql://localhost:3306/source_db");
        DataSourceConfig apiConfig = new DataSourceConfig("api", "https://api.example.com/events");
        DataSourceConfig fileConfig = new DataSourceConfig("file", "/data/streaming/");
        
        messageQueue.configureSource(dbConfig);
        messageQueue.configureSource(apiConfig);
        messageQueue.configureSource(fileConfig);
        
        System.out.println("Data sources configured");
    }
    
    private void setupStreamingPipeline() {
        // è®¾ç½®Flinkæµå¤„ç†ç®¡é“
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // è®¾ç½®æ£€æŸ¥ç‚¹
        env.enableCheckpointing(60000); // æ¯åˆ†é’Ÿæ£€æŸ¥ç‚¹
        env.getCheckpointConfig().setCheckpointTimeout(30000);
        env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);
        
        // åˆ›å»ºæ•°æ®æµ
        DataStream<Event> eventStream = env
            .addSource(new KafkaSource<>())
            .name("event-source");
        
        // æ•°æ®è½¬æ¢å’Œæ¸…æ´—
        DataStream<ProcessedEvent> processedStream = eventStream
            .map(new EventProcessor())
            .filter(new EventFilter())
            .keyBy(ProcessedEvent::getUserId);
        
        // çª—å£èšåˆ
        DataStream<UserProfile> userProfileStream = processedStream
            .window(TumblingEventTimeWindows.of(Time.minutes(5)))
            .aggregate(new UserProfileAggregator());
        
        // è¾“å‡ºåˆ°å®æ—¶å­˜å‚¨
        userProfileStream.addSink(new ClickHouseSink());
        
        // ä¿å­˜æ‰§è¡Œè®¡åˆ’
        streamProcessor.setExecutionPlan(env);
        
        System.out.println("Streaming pipeline configured");
    }
    
    private void setupQualityChecks() {
        // é…ç½®å®æ—¶æ•°æ®è´¨é‡æ£€æŸ¥
        qualityChecker.addRule(new CompletenessRule("userId", 0.99));
        qualityChecker.addRule(new ConsistencyRule("userId", "users", "id"));
        qualityChecker.addRule(new ValidityRule("email", "^[A-Za-z0-9+_.-]+@(.+)$"));
        
        // è®¾ç½®è´¨é‡æ£€æŸ¥çª—å£
        qualityChecker.setCheckWindow(Time.minutes(1));
        qualityChecker.setAlertThreshold(0.95);
        
        System.out.println("Quality checks configured");
    }
    
    private void setupMonitoring() {
        // é…ç½®ç›‘æ§æŒ‡æ ‡
        monitoringSystem.addMetric("events_processed_per_second", MetricType.COUNTER);
        monitoringSystem.addMetric("processing_latency_ms", MetricType.HISTOGRAM);
        monitoringSystem.addMetric("error_rate", MetricType.GAUGE);
        monitoringSystem.addMetric("data_quality_score", MetricType.GAUGE);
        
        // è®¾ç½®å‘Šè­¦è§„åˆ™
        monitoringSystem.addAlertRule("error_rate > 0.05", Severity.CRITICAL);
        monitoringSystem.addAlertRule("processing_latency_ms > 1000", Severity.WARNING);
        monitoringSystem.addAlertRule("data_quality_score < 0.9", Severity.WARNING);
        
        System.out.println("Monitoring configured");
    }
    
    private void startRealTimeETL() {
        try {
            // å¯åŠ¨æµå¤„ç†ä½œä¸š
            streamProcessor.start();
            
            // å¯åŠ¨ç›‘æ§
            monitoringSystem.start();
            
            // å¯åŠ¨è´¨é‡æ£€æŸ¥
            qualityChecker.start();
            
            System.out.println("Real-time ETL started successfully");
            
        } catch (Exception e) {
            System.err.println("Failed to start real-time ETL: " + e.getMessage());
            throw new RuntimeException("Real-time ETL startup failed", e);
        }
    }
}

// äº‹ä»¶å¤„ç†å™¨
public class EventProcessor implements MapFunction<Event, ProcessedEvent> {
    @Override
    public ProcessedEvent map(Event event) throws Exception {
        // æ•°æ®æ¸…æ´—å’Œè½¬æ¢
        ProcessedEvent processed = new ProcessedEvent();
        processed.setUserId(cleanUserId(event.getUserId()));
        processed.setEventType(normalizeEventType(event.getEventType()));
        processed.setTimestamp(parseTimestamp(event.getTimestamp()));
        processed.setAmount(validateAmount(event.getAmount()));
        
        return processed;
    }
    
    private String cleanUserId(String userId) {
        return userId != null ? userId.trim().toLowerCase() : null;
    }
    
    private String normalizeEventType(String eventType) {
        if (eventType == null) return "unknown";
        switch (eventType.toLowerCase()) {
            case "view": return "VIEW";
            case "click": return "CLICK";
            case "purchase": return "PURCHASE";
            default: return "OTHER";
        }
    }
    
    private LocalDateTime parseTimestamp(String timestamp) {
        try {
            return LocalDateTime.parse(timestamp, DateTimeFormatter.ISO_LOCAL_DATE_TIME);
        } catch (Exception e) {
            return LocalDateTime.now();
        }
    }
    
    private Double validateAmount(Double amount) {
        return amount != null && amount >= 0 ? amount : 0.0;
    }
}
```

```mermaid
graph LR
    A[å®æ—¶æ•°æ®æº] --> B[æ¶ˆæ¯é˜Ÿåˆ—]
    B --> C[æµå¤„ç†å¼•æ“]
    C --> D[å®æ—¶æ•°æ®ä»“åº“]
    D --> E[å®æ—¶åˆ†æ]
    
    A --> A1["æ•°æ®åº“CDC"]
    A --> A2["IoTè®¾å¤‡"]
    A --> A3["ç”¨æˆ·è¡Œä¸º"]
    
    B --> B1["Kafka"]
    B --> B2["Pulsar"]
    
    C --> C1["Flink"]
    C --> C2["Spark Streaming"]
    
    D --> D1["ClickHouse"]
    D --> D2["Apache Druid"]
```

### 6.2 å®æ—¶ETLå®ç°

<Tabs>
  <TabItem value="flink" label="Flinkå®æ—¶ETL" default>
  ```java title="Flinkå®æ—¶ETLç¤ºä¾‹"
  public class FlinkRealTimeETL {
      public void buildRealTimeETL(StreamExecutionEnvironment env) {
          // 1. åˆ›å»ºKafkaæ¶ˆè´¹è€…
          FlinkKafkaConsumer<String> consumer = new FlinkKafkaConsumer<>(
              "user-events",
              new SimpleStringSchema(),
              getKafkaProperties()
          );
          
          // 2. æ•°æ®æµå¤„ç†
          DataStream<UserEvent> userEvents = env
              .addSource(consumer)
              .map(this::parseUserEvent)
              .filter(this::validateEvent)
              .keyBy(UserEvent::getUserId)
              .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
              .aggregate(new UserEventAggregator());
          
          // 3. è¾“å‡ºåˆ°å®æ—¶æ•°æ®ä»“åº“
          userEvents.addSink(new ClickHouseSink());
          
          // 4. æ‰§è¡Œæµå¤„ç†ä½œä¸š
          env.execute("Real-time User Event ETL");
      }
      
      private UserEvent parseUserEvent(String json) {
          try {
              return objectMapper.readValue(json, UserEvent.class);
          } catch (Exception e) {
              log.error("Failed to parse user event", e);
              return null;
          }
      }
      
      private boolean validateEvent(UserEvent event) {
          return event != null && 
                 event.getUserId() != null && 
                 event.getEventType() != null;
      }
  }
  ```
  </TabItem>
  <TabItem value="kafka" label="Kafka Streams">
  ```java title="Kafka Streams ETLç¤ºä¾‹"
  public class KafkaStreamsETL {
      public void buildKafkaStreamsETL() {
          Properties props = new Properties();
          props.put(StreamsConfig.APPLICATION_ID_CONFIG, "user-events-etl");
          props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
          props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());
          props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
          
          StreamsBuilder builder = new StreamsBuilder();
          
          // 1. ä»è¾“å…¥ä¸»é¢˜è¯»å–æ•°æ®
          KStream<String, String> inputStream = builder.stream("user-events-input");
          
          // 2. æ•°æ®è½¬æ¢å’Œæ¸…æ´—
          KStream<String, UserEvent> processedStream = inputStream
              .filter((key, value) -> value != null && !value.isEmpty())
              .mapValues(this::parseUserEvent)
              .filter((key, event) -> event != null && validateEvent(event));
          
          // 3. èšåˆå¤„ç†
          KTable<String, UserProfile> userProfiles = processedStream
              .groupBy((key, event) -> event.getUserId())
              .aggregate(
                  UserProfile::new,
                  (userId, event, profile) -> profile.update(event),
                  Materialized.as("user-profiles-store")
              );
          
          // 4. è¾“å‡ºåˆ°ç›®æ ‡ä¸»é¢˜
          userProfiles.toStream().to("user-profiles-output");
          
          // 5. æ„å»ºå’Œå¯åŠ¨æµå¤„ç†åº”ç”¨
          KafkaStreams streams = new KafkaStreams(builder.build(), props);
          streams.start();
      }
  }
  ```
  </TabItem>
</Tabs>

## 7. æ•°æ®ä»“åº“æ€§èƒ½ä¼˜åŒ–

### 7.1 æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–

æ•°æ®ä»“åº“æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š

#### 7.1.1 æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯æ ˆ
```mermaid
graph TB
    A[æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯] --> B[ç´¢å¼•ä¼˜åŒ–]
    A --> C[åˆ†åŒºç­–ç•¥]
    A --> D[ç‰©åŒ–è§†å›¾]
    A --> E[æŸ¥è¯¢é‡å†™]
    A --> F[å¹¶è¡Œå¤„ç†]
    A --> G[ç¼“å­˜ç­–ç•¥]
    
    B --> B1["B+æ ‘ç´¢å¼•"]
    B --> B2["ä½å›¾ç´¢å¼•"]
    B --> B3["å“ˆå¸Œç´¢å¼•"]
    
    C --> C1["æ—¶é—´åˆ†åŒº"]
    C --> C2["èŒƒå›´åˆ†åŒº"]
    C --> C3["åˆ—è¡¨åˆ†åŒº"]
    
    D --> D1["é¢„è®¡ç®—è§†å›¾"]
    D --> D2["å¢é‡æ›´æ–°"]
    D --> D3["æ™ºèƒ½åˆ·æ–°"]
    
    E --> E1["è°“è¯ä¸‹æ¨"]
    E --> E2["åˆ—è£å‰ª"]
    E --> E3["è¿æ¥é‡æ’åº"]
    
    F --> F1["å¤šæ ¸å¹¶è¡Œ"]
    F --> F2["åˆ†å¸ƒå¼æŸ¥è¯¢"]
    
    G --> G1["æŸ¥è¯¢ç»“æœç¼“å­˜"]
    G --> G2["çƒ­ç‚¹æ•°æ®ç¼“å­˜"]
```

#### 7.1.2 é«˜çº§æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥
```java title="é«˜çº§æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥ç¤ºä¾‹"
public class AdvancedQueryOptimization {
    private final QueryOptimizer optimizer;
    private final StatisticsCollector statisticsCollector;
    private final CacheManager cacheManager;
    
    public AdvancedQueryOptimization(QueryOptimizer optimizer,
                                   StatisticsCollector statisticsCollector,
                                   CacheManager cacheManager) {
        this.optimizer = optimizer;
        this.statisticsCollector = statisticsCollector;
        this.cacheManager = cacheManager;
    }
    
    public void optimizeQuery(Query query) {
        // 1. æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        collectStatistics(query);
        
        // 2. åˆ†ææŸ¥è¯¢æ¨¡å¼
        analyzeQueryPattern(query);
        
        // 3. åº”ç”¨ä¼˜åŒ–è§„åˆ™
        applyOptimizationRules(query);
        
        // 4. æ£€æŸ¥ç¼“å­˜å‘½ä¸­
        checkCacheHit(query);
        
        // 5. ç”Ÿæˆæ‰§è¡Œè®¡åˆ’
        generateExecutionPlan(query);
    }
    
    private void collectStatistics(Query query) {
        // æ”¶é›†è¡¨å’Œåˆ—çš„ç»Ÿè®¡ä¿¡æ¯
        for (String tableName : query.getReferencedTables()) {
            TableStatistics tableStats = statisticsCollector.collectTableStatistics(tableName);
            ColumnStatistics columnStats = statisticsCollector.collectColumnStatistics(tableName);
            
            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            optimizer.updateStatistics(tableName, tableStats, columnStats);
        }
        
        System.out.println("Statistics collected for query optimization");
    }
    
    private void analyzeQueryPattern(Query query) {
        // åˆ†ææŸ¥è¯¢ç‰¹å¾
        QueryPattern pattern = new QueryPattern();
        pattern.setJoinType(query.getJoinType());
        pattern.setFilterConditions(query.getFilterConditions());
        pattern.setAggregationFunctions(query.getAggregationFunctions());
        pattern.setSortColumns(query.getSortColumns());
        
        // è¯†åˆ«æŸ¥è¯¢æ¨¡å¼
        QueryPatternType patternType = optimizer.identifyPattern(pattern);
        System.out.println("Query pattern identified: " + patternType);
    }
    
    private void applyOptimizationRules(Query query) {
        // åº”ç”¨å„ç§ä¼˜åŒ–è§„åˆ™
        
        // 1. è°“è¯ä¸‹æ¨
        if (canPushDownPredicates(query)) {
            optimizer.pushDownPredicates(query);
            System.out.println("Predicates pushed down");
        }
        
        // 2. åˆ—è£å‰ª
        if (canPruneColumns(query)) {
            optimizer.pruneColumns(query);
            System.out.println("Columns pruned");
        }
        
        // 3. è¿æ¥é‡æ’åº
        if (canReorderJoins(query)) {
            optimizer.reorderJoins(query);
            System.out.println("Joins reordered");
        }
        
        // 4. åˆ†åŒºè£å‰ª
        if (canPrunePartitions(query)) {
            optimizer.prunePartitions(query);
            System.out.println("Partitions pruned");
        }
    }
    
    private boolean canPushDownPredicates(Query query) {
        // æ£€æŸ¥æ˜¯å¦å¯ä»¥ä¸‹æ¨è°“è¯
        return query.getFilterConditions().stream()
            .anyMatch(condition -> condition.canBePushedDown());
    }
    
    private boolean canPruneColumns(Query query) {
        // æ£€æŸ¥æ˜¯å¦å¯ä»¥è£å‰ªåˆ—
        Set<String> referencedColumns = query.getReferencedColumns();
        Set<String> allColumns = query.getAllColumns();
        return allColumns.size() > referencedColumns.size();
    }
    
    private boolean canReorderJoins(Query query) {
        // æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡æ’åºè¿æ¥
        return query.getJoinCount() > 2;
    }
    
    private boolean canPrunePartitions(Query query) {
        // æ£€æŸ¥æ˜¯å¦å¯ä»¥è£å‰ªåˆ†åŒº
        return query.hasPartitionFilter();
    }
    
    private void checkCacheHit(Query query) {
        // æ£€æŸ¥æŸ¥è¯¢ç¼“å­˜
        String cacheKey = generateCacheKey(query);
        CachedResult cachedResult = cacheManager.get(cacheKey);
        
        if (cachedResult != null && !cachedResult.isExpired()) {
            System.out.println("Query cache hit, using cached result");
            query.setCachedResult(cachedResult);
        } else {
            System.out.println("Query cache miss, executing query");
        }
    }
    
    private String generateCacheKey(Query query) {
        // ç”Ÿæˆç¼“å­˜é”®
        StringBuilder keyBuilder = new StringBuilder();
        keyBuilder.append(query.getSQL().hashCode());
        keyBuilder.append("_");
        keyBuilder.append(query.getParameters().hashCode());
        return keyBuilder.toString();
    }
    
    private void generateExecutionPlan(Query query) {
        // ç”Ÿæˆä¼˜åŒ–çš„æ‰§è¡Œè®¡åˆ’
        ExecutionPlan plan = optimizer.generatePlan(query);
        
        // è®¾ç½®å¹¶è¡Œåº¦
        plan.setParallelism(calculateOptimalParallelism(query));
        
        // è®¾ç½®å†…å­˜é…ç½®
        plan.setMemoryConfig(calculateMemoryConfig(query));
        
        // è®¾ç½®ç¼“å­˜ç­–ç•¥
        plan.setCacheStrategy(selectCacheStrategy(query));
        
        System.out.println("Optimized execution plan generated");
        System.out.println("Parallelism: " + plan.getParallelism());
        System.out.println("Memory: " + plan.getMemoryConfig());
        System.out.println("Cache strategy: " + plan.getCacheStrategy());
    }
    
    private int calculateOptimalParallelism(Query query) {
        // æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦è®¡ç®—æœ€ä¼˜å¹¶è¡Œåº¦
        int baseParallelism = Runtime.getRuntime().availableProcessors();
        double complexityFactor = query.getComplexityScore();
        
        if (complexityFactor > 0.8) {
            return baseParallelism * 2; // å¤æ‚æŸ¥è¯¢ä½¿ç”¨æ›´å¤šå¹¶è¡Œåº¦
        } else if (complexityFactor < 0.3) {
            return Math.max(1, baseParallelism / 2); // ç®€å•æŸ¥è¯¢å‡å°‘å¹¶è¡Œåº¦
        } else {
            return baseParallelism;
        }
    }
    
    private MemoryConfig calculateMemoryConfig(Query query) {
        // æ ¹æ®æŸ¥è¯¢éœ€æ±‚è®¡ç®—å†…å­˜é…ç½®
        MemoryConfig config = new MemoryConfig();
        
        if (query.hasLargeJoins()) {
            config.setJoinBufferSize("2GB");
            config.setSortBufferSize("1GB");
        } else if (query.hasAggregations()) {
            config.setAggregationBufferSize("1GB");
        } else {
            config.setDefaultBufferSize("512MB");
        }
        
        return config;
    }
    
    private CacheStrategy selectCacheStrategy(Query query) {
        // æ ¹æ®æŸ¥è¯¢ç‰¹å¾é€‰æ‹©ç¼“å­˜ç­–ç•¥
        if (query.isFrequentlyExecuted()) {
            return CacheStrategy.AGGRESSIVE; // é¢‘ç¹æŸ¥è¯¢ä½¿ç”¨æ¿€è¿›ç¼“å­˜
        } else if (query.isReadOnly()) {
            return CacheStrategy.MODERATE; // åªè¯»æŸ¥è¯¢ä½¿ç”¨ä¸­ç­‰ç¼“å­˜
        } else {
            return CacheStrategy.CONSERVATIVE; // å…¶ä»–æŸ¥è¯¢ä½¿ç”¨ä¿å®ˆç¼“å­˜
        }
    }
}
```

<div className="card">
<div className="card__body">
<ol>
<li><strong>ç´¢å¼•ä¼˜åŒ–</strong>ï¼šåˆç†è®¾è®¡ç´¢å¼•ï¼Œæ”¯æŒæŸ¥è¯¢æ¨¡å¼</li>
<li><strong>åˆ†åŒºç­–ç•¥</strong>ï¼šæŒ‰æ—¶é—´ã€åœ°åŒºç­‰ç»´åº¦åˆ†åŒº</li>
<li><strong>ç‰©åŒ–è§†å›¾</strong>ï¼šé¢„è®¡ç®—å¸¸ç”¨æŸ¥è¯¢ç»“æœ</li>
<li><strong>æŸ¥è¯¢é‡å†™</strong>ï¼šä¼˜åŒ–å™¨è‡ªåŠ¨é‡å†™æŸ¥è¯¢</li>
<li><strong>å¹¶è¡Œå¤„ç†</strong>ï¼šåˆ©ç”¨å¤šæ ¸CPUå¹¶è¡Œæ‰§è¡Œ</li>
</ol>
</div>
</div>

### 7.2 åˆ†åŒºç­–ç•¥ç¤ºä¾‹

```java title="åˆ†åŒºç­–ç•¥ç¤ºä¾‹"
public class PartitioningStrategy {
    public void createPartitionedTable(Connection conn) throws SQLException {
        // 1. æŒ‰æ—¶é—´åˆ†åŒº
        String timePartitionSQL = 
            "CREATE TABLE sales_fact_partitioned (" +
            "  sale_id VARCHAR(50)," +
            "  customer_id VARCHAR(50)," +
            "  product_id VARCHAR(50)," +
            "  sale_date DATE," +
            "  amount DECIMAL(10,2)" +
            ") PARTITION BY RANGE (YEAR(sale_date)) (" +
            "  PARTITION p2020 VALUES LESS THAN (2021)," +
            "  PARTITION p2021 VALUES LESS THAN (2022)," +
            "  PARTITION p2022 VALUES LESS THAN (2023)," +
            "  PARTITION p2023 VALUES LESS THAN (2024)," +
            "  PARTITION p_future VALUES LESS THAN MAXVALUE" +
            ")";
        
        // 2. æŒ‰åœ°åŒºåˆ†åŒº
        String regionPartitionSQL = 
            "CREATE TABLE customer_dim_partitioned (" +
            "  customer_id VARCHAR(50)," +
            "  customer_name VARCHAR(100)," +
            "  region VARCHAR(50)," +
            "  city VARCHAR(50)" +
            ") PARTITION BY LIST (region) (" +
            "  PARTITION p_north VALUES IN ('North', 'Northeast')," +
            "  PARTITION p_south VALUES IN ('South', 'Southeast')," +
            "  PARTITION p_east VALUES IN ('East', 'Northeast')," +
            "  PARTITION p_west VALUES IN ('West', 'Northwest')" +
            ")";
        
        try (Statement stmt = conn.createStatement()) {
            stmt.execute(timePartitionSQL);
            stmt.execute(regionPartitionSQL);
        }
    }
}
```

## 8. æœ€ä½³å®è·µ

### 8.1 è®¾è®¡æœ€ä½³å®è·µ

<div className="card">
<div className="card__body">
<ol>
<li><strong>æ•°æ®å»ºæ¨¡</strong>ï¼šé‡‡ç”¨ç»´åº¦å»ºæ¨¡ï¼Œè®¾è®¡æ¸…æ™°çš„äº‹å®è¡¨å’Œç»´åº¦è¡¨</li>
<li><strong>ETLè®¾è®¡</strong>ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒå¢é‡å¤„ç†å’Œé”™è¯¯æ¢å¤</li>
<li><strong>æ€§èƒ½ä¼˜åŒ–</strong>ï¼šåˆç†ä½¿ç”¨åˆ†åŒºã€ç´¢å¼•å’Œç‰©åŒ–è§†å›¾</li>
<li><strong>æ•°æ®è´¨é‡</strong>ï¼šå»ºç«‹å®Œå–„çš„æ•°æ®è´¨é‡æ£€æŸ¥æœºåˆ¶</li>
<li><strong>ç›‘æ§å‘Šè­¦</strong>ï¼šå®æ—¶ç›‘æ§ETLä½œä¸šæ‰§è¡ŒçŠ¶æ€</li>
</ol>
</div>
</div>

### 8.2 å®æ–½å»ºè®®

1. **åˆ†é˜¶æ®µå®æ–½**ï¼šå…ˆå»ºè®¾æ ¸å¿ƒä¸»é¢˜åŸŸï¼Œå†é€æ­¥æ‰©å±•
2. **æ•°æ®æ ‡å‡†åŒ–**ï¼šå»ºç«‹ç»Ÿä¸€çš„æ•°æ®æ ‡å‡†å’Œå‘½åè§„èŒƒ
3. **ç‰ˆæœ¬ç®¡ç†**ï¼šå¯¹æ•°æ®æ¨¡å‹å’ŒETLæµç¨‹è¿›è¡Œç‰ˆæœ¬æ§åˆ¶
4. **æ–‡æ¡£ç®¡ç†**ï¼šç»´æŠ¤å®Œæ•´çš„æŠ€æœ¯æ–‡æ¡£å’Œä¸šåŠ¡æ–‡æ¡£
5. **åŸ¹è®­æ”¯æŒ**ï¼šä¸ºä¸šåŠ¡ç”¨æˆ·æä¾›æ•°æ®ä½¿ç”¨åŸ¹è®­

## 9. æ€»ç»“

æ•°æ®ä»“åº“ä¸ETLæŠ€æœ¯æ˜¯ä¼ä¸šæ•°æ®ç®¡ç†çš„åŸºç¡€ï¼Œå®ƒä»¬ä¸ºä¼ä¸šæä¾›äº†ç»Ÿä¸€çš„æ•°æ®è§†å›¾å’Œå¼ºå¤§çš„åˆ†æèƒ½åŠ›ã€‚é€šè¿‡åˆç†çš„è®¾è®¡å’Œå®æ–½ï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é çš„æ•°æ®ä»“åº“ç³»ç»Ÿã€‚

### å­¦ä¹ å»ºè®®

1. **ç†è§£æ¦‚å¿µ**ï¼šæ·±å…¥ç†è§£æ•°æ®ä»“åº“çš„åŸºæœ¬æ¦‚å¿µå’Œæ¶æ„
2. **æŒæ¡å»ºæ¨¡**ï¼šå­¦ä¹ ç»´åº¦å»ºæ¨¡æ–¹æ³•å’Œæœ€ä½³å®è·µ
3. **å®è·µETL**ï¼šé€šè¿‡å®é™…é¡¹ç›®æŒæ¡ETLæµç¨‹è®¾è®¡
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¹ æŸ¥è¯¢ä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜æŠ€æœ¯
5. **å·¥å…·ä½¿ç”¨**ï¼šç†Ÿæ‚‰ä¸»æµETLå·¥å…·å’Œæ¡†æ¶

æ•°æ®ä»“åº“å»ºè®¾æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸æ–­ä¼˜åŒ–å’Œå®Œå–„ï¼Œä»¥é€‚åº”ä¸šåŠ¡å‘å±•çš„éœ€è¦ã€‚ 