---
sidebar_position: 2
title: Hadoopç”Ÿæ€ç³»ç»Ÿè¯¦è§£
description: æ·±å…¥ç†è§£Hadoopç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ã€æ¶æ„è®¾è®¡ã€éƒ¨ç½²é…ç½®å’Œæœ€ä½³å®è·µ
authors: [Laby]
last_update:
  date: 2025-08-16
  author: Laby
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TOCInline from '@theme/TOCInline';

# Hadoopç”Ÿæ€ç³»ç»Ÿè¯¦è§£

Hadoopæ˜¯Apacheè½¯ä»¶åŸºé‡‘ä¼šå¼€å‘çš„å¼€æºåˆ†å¸ƒå¼è®¡ç®—å¹³å°ï¼Œå®ƒæä¾›äº†å¯é ã€å¯æ‰©å±•çš„åˆ†å¸ƒå¼è®¡ç®—å’Œå­˜å‚¨èƒ½åŠ›ã€‚Hadoopç”Ÿæ€ç³»ç»ŸåŒ…å«äº†å¤šä¸ªç›¸äº’åä½œçš„ç»„ä»¶ï¼Œå½¢æˆäº†ä¸€ä¸ªå®Œæ•´çš„å¤§æ•°æ®å¤„ç†è§£å†³æ–¹æ¡ˆã€‚

:::info æœ¬æ–‡å†…å®¹æ¦‚è§ˆ
<TOCInline toc={toc} />
:::

:::tip æ ¸å¿ƒä»·å€¼
**Hadoopç”Ÿæ€ç³»ç»Ÿ = åˆ†å¸ƒå¼å­˜å‚¨ + åˆ†å¸ƒå¼è®¡ç®— + èµ„æºç®¡ç† + æ•°æ®ä»“åº“ + å·¥ä½œæµè°ƒåº¦ + ç›‘æ§ç®¡ç†**
- ğŸš€ **åˆ†å¸ƒå¼å­˜å‚¨**ï¼šHDFSæä¾›é«˜å¯é ã€é«˜ååé‡çš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ
- ğŸ‘¨â€ğŸ’» **åˆ†å¸ƒå¼è®¡ç®—**ï¼šMapReduceæä¾›ç®€å•æ˜“ç”¨çš„åˆ†å¸ƒå¼è®¡ç®—æ¨¡å‹
- ğŸ” **èµ„æºç®¡ç†**ï¼šYARNç»Ÿä¸€ç®¡ç†é›†ç¾¤èµ„æºï¼Œæ”¯æŒå¤šç§è®¡ç®—æ¡†æ¶
- ğŸ”— **æ•°æ®ä»“åº“**ï¼šHiveæä¾›SQLæŸ¥è¯¢èƒ½åŠ›ï¼Œé™ä½å¤§æ•°æ®ä½¿ç”¨é—¨æ§›
- ğŸ“š **å·¥ä½œæµè°ƒåº¦**ï¼šOozieåè°ƒå¤æ‚çš„æ•°æ®å¤„ç†å·¥ä½œæµ
:::

## 1. Hadoopæ ¸å¿ƒæ¶æ„

### 1.1 æ•´ä½“æ¶æ„

Hadoopé‡‡ç”¨ä¸»ä»ï¼ˆMaster-Slaveï¼‰æ¶æ„ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

#### 1.1.1 æ¶æ„æ¼”è¿›å†ç¨‹
```mermaid
timeline
    title Hadoopæ¶æ„æ¼”è¿›
    Hadoop 1.x : åªæœ‰HDFSå’ŒMapReduceï¼Œå•ç‚¹æ•…éšœ
    Hadoop 2.x : å¼•å…¥YARNï¼Œæ”¯æŒå¤šç§è®¡ç®—æ¡†æ¶
    Hadoop 3.x : æ”¯æŒå®¹å™¨åŒ–ã€GPUã€çº åˆ ç 
    Hadoop 4.x : äº‘åŸç”Ÿã€Kubernetesé›†æˆ
```

#### 1.1.2 é«˜å¯ç”¨æ¶æ„è®¾è®¡
```mermaid
graph TB
    A[Hadoopé›†ç¾¤] --> B[NameNode HA]
    A --> C[ResourceManager HA]
    A --> D[ZooKeeperé›†ç¾¤]
    
    B --> B1["Active NameNode"]
    B --> B2["Standby NameNode"]
    B --> B3["JournalNodeé›†ç¾¤"]
    
    C --> C1["Active ResourceManager"]
    C --> C2["Standby ResourceManager"]
    
    D --> D1["ZK1"]
    D --> D2["ZK2"]
    D --> D3["ZK3"]
```

```mermaid
graph TB
    A[Hadoopé›†ç¾¤] --> B[NameNode]
    A --> C[DataNode]
    A --> D[ResourceManager]
    A --> E[NodeManager]
    A --> F[ApplicationMaster]
    
    B --> B1["å…ƒæ•°æ®ç®¡ç†"]
    B --> B2["å‘½åç©ºé—´ç®¡ç†"]
    
    C --> C1["æ•°æ®å­˜å‚¨"]
    C --> C2["æ•°æ®å¤åˆ¶"]
    
    D --> D1["èµ„æºåˆ†é…"]
    D --> D2["ä»»åŠ¡è°ƒåº¦"]
    
    E --> E1["æœ¬åœ°èµ„æºç®¡ç†"]
    E --> E2["ä»»åŠ¡æ‰§è¡Œ"]
    
    F --> F1["åº”ç”¨ç®¡ç†"]
    F --> F2["ä»»åŠ¡åè°ƒ"]
```

### 1.2 æ ¸å¿ƒç»„ä»¶å…³ç³»

<div className="card">
<div className="card__header">
<h4>Hadoopæ ¸å¿ƒç»„ä»¶èŒè´£</h4>
</div>
<div className="card__body">
<ol>
<li><strong>HDFS</strong>ï¼šåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼Œæä¾›æ•°æ®å­˜å‚¨</li>
<li><strong>YARN</strong>ï¼šèµ„æºç®¡ç†å’Œä»»åŠ¡è°ƒåº¦å¹³å°</li>
<li><strong>MapReduce</strong>ï¼šåˆ†å¸ƒå¼è®¡ç®—ç¼–ç¨‹æ¨¡å‹</li>
<li><strong>Common</strong>ï¼šå…¬å…±å·¥å…·å’Œåº“</li>
</ol>
</div>
</div>

#### 1.2.1 ç»„ä»¶äº¤äº’æµç¨‹
```mermaid
sequenceDiagram
    participant Client
    participant NameNode
    participant DataNode
    participant ResourceManager
    participant NodeManager
    participant ApplicationMaster
    
    Client->>NameNode: åˆ›å»ºæ–‡ä»¶è¯·æ±‚
    NameNode->>Client: è¿”å›DataNodeåˆ—è¡¨
    Client->>DataNode: å†™å…¥æ•°æ®
    DataNode->>DataNode: æ•°æ®å¤åˆ¶
    DataNode->>NameNode: ç¡®è®¤å†™å…¥
    
    Client->>ResourceManager: æäº¤åº”ç”¨
    ResourceManager->>NodeManager: åˆ†é…èµ„æº
    NodeManager->>ApplicationMaster: å¯åŠ¨å®¹å™¨
    ApplicationMaster->>ResourceManager: ç”³è¯·æ›´å¤šèµ„æº
```

#### 1.2.2 é›†ç¾¤è§„æ¨¡è§„åˆ’
```java title="é›†ç¾¤è§„æ¨¡è§„åˆ’ç¤ºä¾‹"
public class ClusterPlanning {
    public ClusterSpecification planCluster(WorkloadRequirements requirements) {
        ClusterSpecification spec = new ClusterSpecification();
        
        // 1. å­˜å‚¨å®¹é‡è§„åˆ’
        long totalStorage = requirements.getDataVolume() * 3; // 3å‰¯æœ¬
        int dataNodes = (int) Math.ceil(totalStorage / (4L * 1024 * 1024 * 1024)); // 4TB/èŠ‚ç‚¹
        spec.setDataNodeCount(dataNodes);
        
        // 2. è®¡ç®—èµ„æºè§„åˆ’
        int totalCores = requirements.getCpuCores();
        int totalMemory = requirements.getMemoryGB();
        int computeNodes = Math.max(
            (int) Math.ceil(totalCores / 16.0), // 16æ ¸/èŠ‚ç‚¹
            (int) Math.ceil(totalMemory / 64.0)  // 64GB/èŠ‚ç‚¹
        );
        spec.setComputeNodeCount(computeNodes);
        
        // 3. ç½‘ç»œå¸¦å®½è§„åˆ’
        double networkBandwidth = requirements.getDataThroughput() * 1.5; // 1.5å€å†—ä½™
        spec.setNetworkBandwidth(networkBandwidth);
        
        // 4. é«˜å¯ç”¨é…ç½®
        spec.setNameNodeCount(2); // ä¸»å¤‡NameNode
        spec.setResourceManagerCount(2); // ä¸»å¤‡ResourceManager
        spec.setZooKeeperCount(3); // 3èŠ‚ç‚¹ZKé›†ç¾¤
        
        return spec;
    }
    
    public static class ClusterSpecification {
        private int dataNodeCount;
        private int computeNodeCount;
        private double networkBandwidth;
        private int nameNodeCount;
        private int resourceManagerCount;
        private int zooKeeperCount;
        
        // getters and setters...
    }
}
```

## 2. HDFSåˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ

### 2.1 HDFSæ¶æ„è®¾è®¡

HDFSé‡‡ç”¨ä¸»ä»æ¶æ„ï¼ŒåŒ…å«NameNodeå’ŒDataNodeï¼š

#### 2.1.1 HDFS 3.xæ–°ç‰¹æ€§
```mermaid
graph TB
    A[HDFS 3.xæ–°ç‰¹æ€§] --> B[çº åˆ ç ]
    A --> C[å®¹å™¨åŒ–æ”¯æŒ]
    A --> D[GPUæ”¯æŒ]
    A --> E[å¯¹è±¡å­˜å‚¨]
    
    B --> B1["RS-6-3ç¼–ç "]
    B --> B2["èŠ‚çœ50%å­˜å‚¨ç©ºé—´"]
    
    C --> C1["Dockerå®¹å™¨"]
    C --> C2["Kubernetesé›†æˆ"]
    
    D --> D1["GPUåŠ é€Ÿè®¡ç®—"]
    D --> D2["æ·±åº¦å­¦ä¹ æ”¯æŒ"]
    
    E --> E1["S3å…¼å®¹æ¥å£"]
    E --> E2["äº‘å­˜å‚¨é›†æˆ"]
```

#### 2.1.2 æ•°æ®å—ç®¡ç†ç­–ç•¥
```java title="HDFSæ•°æ®å—ç®¡ç†ç¤ºä¾‹"
public class HDFSBlockManagement {
    private final Configuration conf;
    private final FileSystem fs;
    
    public HDFSBlockManagement(Configuration conf) throws IOException {
        this.conf = conf;
        this.fs = FileSystem.get(conf);
    }
    
    public void optimizeBlockPlacement(String path) throws IOException {
        // 1. è·å–æ–‡ä»¶å—ä¿¡æ¯
        FileStatus fileStatus = fs.getFileStatus(new Path(path));
        BlockLocation[] blockLocations = fs.getFileBlockLocations(fileStatus, 0, fileStatus.getLen());
        
        // 2. åˆ†æå—åˆ†å¸ƒ
        Map<String, Integer> rackBlockCount = new HashMap<>();
        for (BlockLocation block : blockLocations) {
            String[] hosts = block.getHosts();
            String[] racks = block.getTopologyPaths();
            
            for (String rack : racks) {
                rackBlockCount.merge(rack, 1, Integer::sum);
            }
        }
        
        // 3. æ£€æŸ¥æ•°æ®å€¾æ–œ
        int avgBlocksPerRack = blockLocations.length / rackBlockCount.size();
        for (Map.Entry<String, Integer> entry : rackBlockCount.entrySet()) {
            if (entry.getValue() > avgBlocksPerRack * 1.5) {
                System.out.println("Rack " + entry.getKey() + " has too many blocks: " + entry.getValue());
            }
        }
        
        // 4. å»ºè®®é‡æ–°å¹³è¡¡
        if (needsRebalancing(rackBlockCount)) {
            System.out.println("Recommend running hdfs balancer");
        }
    }
    
    private boolean needsRebalancing(Map<String, Integer> rackBlockCount) {
        if (rackBlockCount.size() < 2) return false;
        
        int min = rackBlockCount.values().stream().mapToInt(Integer::intValue).min().orElse(0);
        int max = rackBlockCount.values().stream().mapToInt(Integer::intValue).max().orElse(0);
        
        return (double) max / min > 1.5; // æœ€å¤§æœ€å°æ¯”ä¾‹è¶…è¿‡1.5
    }
    
    public void enableErasureCoding(String path) throws IOException {
        // å¯ç”¨çº åˆ ç 
        fs.setErasureCodingPolicy(new Path(path), "RS-6-3-1024k");
        System.out.println("Erasure coding enabled for: " + path);
    }
}
```

```mermaid
graph LR
    A[Client] --> B[NameNode]
    A --> C[DataNode1]
    A --> D[DataNode2]
    A --> E[DataNode3]
    
    B --> B1["å…ƒæ•°æ®ç®¡ç†"]
    B --> B2["å‘½åç©ºé—´"]
    
    C --> C1["æ•°æ®å—1"]
    C --> C2["æ•°æ®å—2"]
    
    D --> D1["æ•°æ®å—2"]
    D --> D2["æ•°æ®å—3"]
    
    E --> E1["æ•°æ®å—1"]
    E --> E2["æ•°æ®å—3"]
```

#### HDFSæ ¸å¿ƒç‰¹æ€§
```java title="HDFSç‰¹æ€§ç¤ºä¾‹"
public class HDFSFeatures {
    public static void main(String[] args) {
        // 1. é«˜å®¹é”™æ€§
        System.out.println("HDFSé€šè¿‡æ•°æ®å¤åˆ¶æä¾›é«˜å®¹é”™æ€§");
        
        // 2. é«˜ååé‡
        System.out.println("HDFSè®¾è®¡ç”¨äºæ‰¹å¤„ç†ï¼Œæä¾›é«˜ååé‡");
        
        // 3. å¤§æ–‡ä»¶æ”¯æŒ
        System.out.println("HDFSé€‚åˆå­˜å‚¨å¤§æ–‡ä»¶ï¼Œé€šå¸¸GBåˆ°TBçº§åˆ«");
        
        // 4. æµå¼æ•°æ®è®¿é—®
        System.out.println("HDFSæ”¯æŒä¸€æ¬¡å†™å…¥ï¼Œå¤šæ¬¡è¯»å–çš„è®¿é—®æ¨¡å¼");
        
        // 5. ç¡¬ä»¶å®¹é”™
        System.out.println("HDFSè¿è¡Œåœ¨æ™®é€šç¡¬ä»¶ä¸Šï¼Œé€šè¿‡è½¯ä»¶æä¾›å®¹é”™èƒ½åŠ›");
    }
}
```

### 2.2 HDFSæ–‡ä»¶æ“ä½œ

<Tabs>
  <TabItem value="read" label="æ–‡ä»¶è¯»å–" default>
  ```java title="HDFSæ–‡ä»¶è¯»å–ç¤ºä¾‹"
  public class HDFSReader {
      public String readFile(String filePath) throws IOException {
          Configuration conf = new Configuration();
          FileSystem fs = FileSystem.get(conf);
          
          try (FSDataInputStream in = fs.open(new Path(filePath))) {
              BufferedReader reader = new BufferedReader(
                  new InputStreamReader(in)
              );
              return reader.lines().collect(Collectors.joining("\n"));
          }
      }
  }
  ```
  </TabItem>
  <TabItem value="write" label="æ–‡ä»¶å†™å…¥">
  ```java title="HDFSæ–‡ä»¶å†™å…¥ç¤ºä¾‹"
  public class HDFSWriter {
      public void writeFile(String filePath, String content) throws IOException {
          Configuration conf = new Configuration();
          FileSystem fs = FileSystem.get(conf);
          
          try (FSDataOutputStream out = fs.create(new Path(filePath))) {
              out.writeBytes(content);
          }
      }
  }
  ```
  </TabItem>
  <TabItem value="delete" label="æ–‡ä»¶åˆ é™¤">
  ```java title="HDFSæ–‡ä»¶åˆ é™¤ç¤ºä¾‹"
  public class HDFSDeleter {
      public boolean deleteFile(String filePath) throws IOException {
          Configuration conf = new Configuration();
          FileSystem fs = FileSystem.get(conf);
          
          return fs.delete(new Path(filePath), false);
      }
  }
  ```
  </TabItem>
</Tabs>

## 3. YARNèµ„æºç®¡ç†

### 3.1 YARNæ¶æ„

YARNï¼ˆYet Another Resource Negotiatorï¼‰æ˜¯Hadoop 2.0å¼•å…¥çš„èµ„æºç®¡ç†å¹³å°ï¼š

#### 3.1.1 YARN 3.xæ–°ç‰¹æ€§
```mermaid
graph TB
    A[YARN 3.xæ–°ç‰¹æ€§] --> B[å®¹å™¨åŒ–æ”¯æŒ]
    A --> C[GPUèµ„æºç®¡ç†]
    A --> D[å¼¹æ€§ä¼¸ç¼©]
    A --> E[å¤šç§Ÿæˆ·æ”¯æŒ]
    
    B --> B1["Dockerå®¹å™¨"]
    B --> B2["Kubernetesé›†æˆ"]
    
    C --> C1["GPUèµ„æºåˆ†é…"]
    C --> C2["æ·±åº¦å­¦ä¹ æ”¯æŒ"]
    
    D --> D1["è‡ªåŠ¨æ‰©ç¼©å®¹"]
    D --> D2["èµ„æºå¼¹æ€§è°ƒæ•´"]
    
    E --> E1["é˜Ÿåˆ—éš”ç¦»"]
    E --> E2["èµ„æºé…é¢ç®¡ç†"]
```

#### 3.1.2 èµ„æºè°ƒåº¦å™¨å¯¹æ¯”
```java title="YARNè°ƒåº¦å™¨å¯¹æ¯”ç¤ºä¾‹"
public class YARNSchedulerComparison {
    public void compareSchedulers() {
        // 1. FIFOè°ƒåº¦å™¨ - å…ˆè¿›å…ˆå‡º
        System.out.println("=== FIFO Scheduler ===");
        System.out.println("ä¼˜ç‚¹: ç®€å•ã€å…¬å¹³");
        System.out.println("ç¼ºç‚¹: å°ä½œä¸šå¯èƒ½è¢«å¤§ä½œä¸šé˜»å¡");
        System.out.println("é€‚ç”¨: å•ç”¨æˆ·ç¯å¢ƒ");
        
        // 2. Capacityè°ƒåº¦å™¨ - å®¹é‡è°ƒåº¦
        System.out.println("\n=== Capacity Scheduler ===");
        System.out.println("ä¼˜ç‚¹: èµ„æºéš”ç¦»ã€å¤šç§Ÿæˆ·æ”¯æŒ");
        System.out.println("ç¼ºç‚¹: é…ç½®å¤æ‚");
        System.out.println("é€‚ç”¨: å¤šç”¨æˆ·ã€å¤šé˜Ÿåˆ—ç¯å¢ƒ");
        
        // 3. Fairè°ƒåº¦å™¨ - å…¬å¹³è°ƒåº¦
        System.out.println("\n=== Fair Scheduler ===");
        System.out.println("ä¼˜ç‚¹: åŠ¨æ€èµ„æºåˆ†é…ã€å“åº”æ—¶é—´çŸ­");
        System.out.println("ç¼ºç‚¹: èµ„æºåˆ©ç”¨ç‡å¯èƒ½ä¸é«˜");
        System.out.println("é€‚ç”¨: äº¤äº’å¼æŸ¥è¯¢ã€å®æ—¶åº”ç”¨");
    }
    
    public void configureCapacityScheduler() {
        // Capacityè°ƒåº¦å™¨é…ç½®ç¤ºä¾‹
        Properties props = new Properties();
        
        // é˜Ÿåˆ—é…ç½®
        props.setProperty("yarn.scheduler.capacity.root.queues", "default,prod,dev");
        props.setProperty("yarn.scheduler.capacity.root.default.capacity", "20");
        props.setProperty("yarn.scheduler.capacity.root.prod.capacity", "60");
        props.setProperty("yarn.scheduler.capacity.root.dev.capacity", "20");
        
        // ç”¨æˆ·é™åˆ¶
        props.setProperty("yarn.scheduler.capacity.root.default.maximum-applications", "100");
        props.setProperty("yarn.scheduler.capacity.root.prod.maximum-applications", "200");
        props.setProperty("yarn.scheduler.capacity.root.dev.maximum-applications", "50");
        
        // èµ„æºé™åˆ¶
        props.setProperty("yarn.scheduler.capacity.root.default.maximum-allocation-mb", "8192");
        props.setProperty("yarn.scheduler.capacity.root.prod.maximum-allocation-mb", "16384");
        props.setProperty("yarn.scheduler.capacity.root.dev.maximum-allocation-mb", "4096");
        
        System.out.println("Capacity Scheduler configured with production and development queues");
    }
}
```

```mermaid
graph TB
    A[YARNæ¶æ„] --> B[ResourceManager]
    A --> C[NodeManager]
    A --> D[ApplicationMaster]
    A --> E[Container]
    
    B --> B1["å…¨å±€èµ„æºç®¡ç†"]
    B --> B2["åº”ç”¨è°ƒåº¦"]
    
    C --> C1["æœ¬åœ°èµ„æºç®¡ç†"]
    C --> C2["Containerç”Ÿå‘½å‘¨æœŸç®¡ç†"]
    
    D --> D1["åº”ç”¨èµ„æºç”³è¯·"]
    D --> D2["ä»»åŠ¡åè°ƒ"]
    
    E --> E1["èµ„æºéš”ç¦»"]
    E --> E2["ä»»åŠ¡æ‰§è¡Œç¯å¢ƒ"]
```

### 3.2 YARNå·¥ä½œæµç¨‹

<div className="code-with-callout">

```java title="YARNåº”ç”¨æäº¤ç¤ºä¾‹"
public class YARNApplication {
    public void submitApplication() throws Exception {
        // 1. åˆ›å»ºYARNå®¢æˆ·ç«¯
        YarnClient yarnClient = YarnClient.createYarnClient();
        yarnClient.init(conf);
        yarnClient.start();
        
        // 2. åˆ›å»ºåº”ç”¨
        YarnClientApplication app = yarnClient.createApplication();
        GetNewApplicationResponse appResponse = app.getNewApplicationResponse();
        
        // 3. è®¾ç½®åº”ç”¨ä¸Šä¸‹æ–‡
        ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();
        appContext.setApplicationName("MyYARNApp");
        appContext.setApplicationType("MAPREDUCE");
        
        // 4. æäº¤åº”ç”¨
        yarnClient.submitApplication(appContext);
    }
}
```

:::info YARNä¼˜åŠ¿
YARNå°†èµ„æºç®¡ç†å’Œä»»åŠ¡è°ƒåº¦åˆ†ç¦»ï¼Œæ”¯æŒå¤šç§è®¡ç®—æ¡†æ¶ï¼ˆMapReduceã€Sparkã€Flinkç­‰ï¼‰ï¼Œæé«˜äº†é›†ç¾¤èµ„æºåˆ©ç”¨ç‡ã€‚
:::
</div>

## 4. MapReduceç¼–ç¨‹æ¨¡å‹

### 4.1 MapReduceåŸç†

MapReduceæ˜¯ä¸€ç§ç¼–ç¨‹æ¨¡å‹ï¼Œç”¨äºå¤§è§„æ¨¡æ•°æ®é›†çš„å¹¶è¡Œè®¡ç®—ï¼š

#### 4.1.1 MapReduce 2.0æ¶æ„
```mermaid
graph TB
    A[MapReduce 2.0] --> B[YARNèµ„æºç®¡ç†]
    A --> C[ApplicationMaster]
    A --> D[Map Task]
    A --> E[Reduce Task]
    
    B --> B1["èµ„æºåˆ†é…"]
    B --> B2["ä»»åŠ¡è°ƒåº¦"]
    
    C --> C1["ä½œä¸šç®¡ç†"]
    C --> C2["ä»»åŠ¡åè°ƒ"]
    
    D --> D1["Mapé˜¶æ®µ"]
    D --> D2["æœ¬åœ°èšåˆ"]
    
    E --> E1["Shuffleé˜¶æ®µ"]
    E --> E2["Reduceé˜¶æ®µ"]
```

#### 4.1.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
```java title="MapReduceæ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹"
public class MapReduceOptimization {
    public void optimizeJob(Job job) throws IOException {
        Configuration conf = job.getConfiguration();
        
        // 1. è¾“å…¥åˆ†ç‰‡ä¼˜åŒ–
        conf.set("mapreduce.input.fileinputformat.split.minsize", "134217728"); // 128MB
        conf.set("mapreduce.input.fileinputformat.split.maxsize", "268435456"); // 256MB
        
        // 2. Mapç«¯ä¼˜åŒ–
        conf.set("mapreduce.map.memory.mb", "4096"); // 4GBå†…å­˜
        conf.set("mapreduce.map.java.opts", "-Xmx3072m"); // 3GBå †å†…å­˜
        conf.set("mapreduce.map.output.compress", "true"); // å¯ç”¨å‹ç¼©
        conf.set("mapreduce.map.output.compress.codec", "org.apache.hadoop.io.compress.SnappyCodec");
        
        // 3. Reduceç«¯ä¼˜åŒ–
        conf.set("mapreduce.reduce.memory.mb", "8192"); // 8GBå†…å­˜
        conf.set("mapreduce.reduce.java.opts", "-Xmx6144m"); // 6GBå †å†…å­˜
        conf.set("mapreduce.reduce.shuffle.parallelcopies", "5"); // å¹¶è¡Œå¤åˆ¶æ•°
        
        // 4. Shuffleä¼˜åŒ–
        conf.set("mapreduce.reduce.shuffle.input.buffer.percent", "0.7"); // 70%å†…å­˜ç”¨äºshuffle
        conf.set("mapreduce.reduce.shuffle.merge.percent", "0.66"); // 66%æ•°æ®æ—¶å¼€å§‹åˆå¹¶
        
        // 5. æ¨æµ‹æ‰§è¡Œ
        conf.set("mapreduce.map.speculative", "true");
        conf.set("mapreduce.reduce.speculative", "true");
        
        // 6. ä»»åŠ¡æ•°é‡ä¼˜åŒ–
        conf.set("mapreduce.job.maps", "200"); // Mapä»»åŠ¡æ•°
        conf.set("mapreduce.job.reduces", "50"); // Reduceä»»åŠ¡æ•°
        
        System.out.println("MapReduce job optimized for performance");
    }
    
    public void configureCombiner(Job job) {
        // é…ç½®Combinerå‡å°‘ç½‘ç»œä¼ è¾“
        job.setCombinerClass(WordCountCombiner.class);
        System.out.println("Combiner configured to reduce network traffic");
    }
    
    public void configurePartitioner(Job job) {
        // è‡ªå®šä¹‰åˆ†åŒºå™¨é¿å…æ•°æ®å€¾æ–œ
        job.setPartitionerClass(CustomPartitioner.class);
        System.out.println("Custom partitioner configured to balance data distribution");
    }
}

// è‡ªå®šä¹‰åˆ†åŒºå™¨ç¤ºä¾‹
public static class CustomPartitioner extends Partitioner<Text, IntWritable> {
    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        // ä½¿ç”¨å“ˆå¸Œåˆ†åŒºé¿å…æ•°æ®å€¾æ–œ
        String word = key.toString();
        return Math.abs(word.hashCode()) % numPartitions;
    }
}
```

```mermaid
graph LR
    A[è¾“å…¥æ•°æ®] --> B[Mapé˜¶æ®µ]
    B --> C[Shuffleé˜¶æ®µ]
    C --> D[Reduceé˜¶æ®µ]
    D --> E[è¾“å‡ºç»“æœ]
    
    B --> B1["å¹¶è¡Œå¤„ç†"]
    B --> B2["é”®å€¼å¯¹ç”Ÿæˆ"]
    
    C --> C1["æ•°æ®åˆ†ç»„"]
    C --> C2["æ•°æ®æ’åº"]
    
    D --> D1["èšåˆè®¡ç®—"]
    D --> D2["ç»“æœè¾“å‡º"]
```

### 4.2 MapReduceç¼–ç¨‹ç¤ºä¾‹

<Tabs>
  <TabItem value="wordcount" label="WordCountç¤ºä¾‹" default>
  ```java title="WordCount MapReduceç¤ºä¾‹"
  public class WordCount {
      public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
          private final static IntWritable one = new IntWritable(1);
          private Text word = new Text();
          
          public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
              StringTokenizer itr = new StringTokenizer(value.toString());
              while (itr.hasMoreTokens()) {
                  word.set(itr.nextToken());
                  context.write(word, one);
              }
          }
      }
      
      public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
          private IntWritable result = new IntWritable();
          
          public void reduce(Text key, Iterable<IntWritable> values, Context context) 
              throws IOException, InterruptedException {
              int sum = 0;
              for (IntWritable val : values) {
                  sum += val.get();
              }
              result.set(sum);
              context.write(key, result);
          }
      }
  }
  ```
  </TabItem>
  <TabItem value="custom" label="è‡ªå®šä¹‰MapReduce">
  ```java title="è‡ªå®šä¹‰MapReduceç¤ºä¾‹"
  public class CustomMapReduce {
      public static class CustomMapper extends Mapper<LongWritable, Text, Text, LongWritable> {
          public void map(LongWritable key, Text value, Context context) 
              throws IOException, InterruptedException {
              // è‡ªå®šä¹‰æ˜ å°„é€»è¾‘
              String line = value.toString();
              String[] fields = line.split(",");
              
              if (fields.length >= 2) {
                  String category = fields[0];
                  long amount = Long.parseLong(fields[1]);
                  context.write(new Text(category), new LongWritable(amount));
              }
          }
      }
      
      public static class CustomReducer extends Reducer<Text, LongWritable, Text, LongWritable> {
          public void reduce(Text key, Iterable<LongWritable> values, Context context) 
              throws IOException, InterruptedException {
              // è‡ªå®šä¹‰å½’çº¦é€»è¾‘
              long total = 0;
              for (LongWritable value : values) {
                  total += value.get();
              }
              context.write(key, new LongWritable(total));
          }
      }
  }
  ```
  </TabItem>
</Tabs>

## 5. Hadoopç”Ÿæ€ç³»ç»Ÿç»„ä»¶

### 5.1 æ•°æ®å­˜å‚¨ç»„ä»¶

| ç»„ä»¶ | ç”¨é€” | ç‰¹ç‚¹ |
|------|------|------|
| **HBase** | åˆ†å¸ƒå¼NoSQLæ•°æ®åº“ | å¼ºä¸€è‡´æ€§ã€å®æ—¶è¯»å†™ã€åˆ—å¼å­˜å‚¨ |
| **Cassandra** | åˆ†å¸ƒå¼NoSQLæ•°æ®åº“ | é«˜å¯ç”¨æ€§ã€çº¿æ€§æ‰©å±•ã€æœ€ç»ˆä¸€è‡´æ€§ |
| **MongoDB** | æ–‡æ¡£æ•°æ®åº“ | çµæ´»çš„æ•°æ®æ¨¡å‹ã€ä¸°å¯Œçš„æŸ¥è¯¢è¯­è¨€ |

### 5.2 æ•°æ®å¤„ç†ç»„ä»¶

<div className="card">
<div className="card__header">
<h4>æ•°æ®å¤„ç†ç»„ä»¶å¯¹æ¯”</h4>
</div>
<div className="card__body">
<ol>
<li><strong>Hive</strong>ï¼šæ•°æ®ä»“åº“ï¼Œæä¾›SQLæŸ¥è¯¢èƒ½åŠ›</li>
<li><strong>Pig</strong>ï¼šæ•°æ®æµè¯­è¨€ï¼Œé€‚åˆETLå¤„ç†</li>
<li><strong>Sqoop</strong>ï¼šå…³ç³»å‹æ•°æ®åº“ä¸Hadoopä¹‹é—´çš„æ•°æ®ä¼ è¾“</li>
<li><strong>Flume</strong>ï¼šåˆ†å¸ƒå¼æ—¥å¿—æ”¶é›†ç³»ç»Ÿ</li>
</ol>
</div>
</div>

### 5.3 å·¥ä½œæµè°ƒåº¦ç»„ä»¶

```java title="Oozieå·¥ä½œæµç¤ºä¾‹"
public class OozieWorkflow {
    public void createWorkflow() {
        // åˆ›å»ºå·¥ä½œæµå®šä¹‰
        WorkflowApp app = new WorkflowApp();
        app.setName("DataProcessingWorkflow");
        
        // æ·»åŠ MapReduceä½œä¸š
        MapReduceAction mrAction = new MapReduceAction();
        mrAction.setName("WordCount");
        mrAction.setJobTracker("${jobTracker}");
        mrAction.setNameNode("${nameNode}");
        
        // è®¾ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
        mrAction.setConfigProperty("mapred.input.dir", "/input");
        mrAction.setConfigProperty("mapred.output.dir", "/output");
        
        // æ·»åŠ åˆ°å·¥ä½œæµ
        app.addAction(mrAction);
    }
}
```

## 6. Hadoopéƒ¨ç½²å’Œé…ç½®

### 6.1 é›†ç¾¤è§„åˆ’

```mermaid
graph TB
    A[é›†ç¾¤è§„åˆ’] --> B[ç¡¬ä»¶é…ç½®]
    A --> C[ç½‘ç»œé…ç½®]
    A --> D[è½¯ä»¶é…ç½®]
    A --> E[å®‰å…¨é…ç½®]
    
    B --> B1["CPU: 8-16æ ¸"]
    B --> B2["å†…å­˜: 32-128GB"]
    B --> B3["å­˜å‚¨: 4-12TB"]
    
    C --> C1["åƒå…†/ä¸‡å…†ç½‘ç»œ"]
    C --> C2["ç½‘ç»œéš”ç¦»"]
    
    D --> D1["æ“ä½œç³»ç»Ÿé€‰æ‹©"]
    D --> D2["Javaç‰ˆæœ¬"]
    
    E --> E1["Kerberosè®¤è¯"]
    E --> E2["æƒé™æ§åˆ¶"]
```

### 6.2 é…ç½®æ–‡ä»¶ç¤ºä¾‹

<Tabs>
  <TabItem value="core" label="core-site.xml" default>
  ```xml
  <configuration>
      <property>
          <name>fs.defaultFS</name>
          <value>hdfs://namenode:9000</value>
      </property>
      <property>
          <name>hadoop.tmp.dir</name>
          <value>/opt/hadoop/data</value>
      </property>
  </configuration>
  ```
  </TabItem>
  <TabItem value="hdfs" label="hdfs-site.xml">
  ```xml
  <configuration>
      <property>
          <name>dfs.replication</name>
          <value>3</value>
      </property>
      <property>
          <name>dfs.namenode.name.dir</name>
          <value>/opt/hadoop/data/namenode</value>
      </property>
      <property>
          <name>dfs.datanode.data.dir</name>
          <value>/opt/hadoop/data/datanode</value>
      </property>
  </configuration>
  ```
  </TabItem>
  <TabItem value="yarn" label="yarn-site.xml">
  ```xml
  <configuration>
      <property>
          <name>yarn.nodemanager.aux-services</name>
          <value>mapreduce_shuffle</value>
      </property>
      <property>
          <name>yarn.resourcemanager.hostname</name>
          <value>resourcemanager</value>
      </property>
  </configuration>
  ```
  </TabItem>
</Tabs>

## 7. æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§

### 7.1 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

<div className="card">
<div className="card__body">
<ol>
<li><strong>HDFSä¼˜åŒ–</strong>ï¼šè°ƒæ•´å—å¤§å°ã€å‰¯æœ¬æ•°é‡ã€å‹ç¼©ç®—æ³•</li>
<li><strong>MapReduceä¼˜åŒ–</strong>ï¼šåˆç†è®¾ç½®Map/Reduceæ•°é‡ã€å†…å­˜é…ç½®</li>
<li><strong>YARNä¼˜åŒ–</strong>ï¼šè°ƒæ•´èµ„æºåˆ†é…ç­–ç•¥ã€é˜Ÿåˆ—é…ç½®</li>
<li><strong>ç½‘ç»œä¼˜åŒ–</strong>ï¼šä½¿ç”¨ä¸“ç”¨ç½‘ç»œã€è°ƒæ•´ç½‘ç»œå‚æ•°</li>
</ol>
</div>
</div>

### 7.2 ç›‘æ§å·¥å…·

```java title="ç›‘æ§æŒ‡æ ‡æ”¶é›†ç¤ºä¾‹"
public class HadoopMonitoring {
    public void collectMetrics() {
        // æ”¶é›†HDFSæŒ‡æ ‡
        HdfsMetrics hdfsMetrics = new HdfsMetrics();
        hdfsMetrics.collectNameNodeMetrics();
        hdfsMetrics.collectDataNodeMetrics();
        
        // æ”¶é›†YARNæŒ‡æ ‡
        YarnMetrics yarnMetrics = new YarnMetrics();
        yarnMetrics.collectResourceManagerMetrics();
        yarnMetrics.collectNodeManagerMetrics();
        
        // æ”¶é›†MapReduceæŒ‡æ ‡
        MapReduceMetrics mrMetrics = new MapReduceMetrics();
        mrMetrics.collectJobMetrics();
        mrMetrics.collectTaskMetrics();
    }
}
```

## 8. æœ€ä½³å®è·µ

### 8.1 å¼€å‘æœ€ä½³å®è·µ

<Tabs>
  <TabItem value="design" label="è®¾è®¡åŸåˆ™" default>
  ```java
  // 1. åˆç†è®¾è®¡é”®å€¼å¯¹
  public class KeyValueDesign {
      // å¥½çš„è®¾è®¡ï¼šå¤åˆé”®
      public static class CompositeKey implements WritableComparable<CompositeKey> {
          private String category;
          private String subcategory;
          // å®ç°æ–¹æ³•...
      }
  }
  ```
  </TabItem>
  <TabItem value="performance" label="æ€§èƒ½ä¼˜åŒ–">
  ```java
  // 2. ä½¿ç”¨Combinerå‡å°‘ç½‘ç»œä¼ è¾“
  public class WordCountCombiner extends Reducer<Text, IntWritable, Text, IntWritable> {
      public void reduce(Text key, Iterable<IntWritable> values, Context context) 
          throws IOException, InterruptedException {
          int sum = 0;
          for (IntWritable val : values) {
              sum += val.get();
          }
          context.write(key, new IntWritable(sum));
      }
  }
  ```
  </TabItem>
  <TabItem value="error" label="é”™è¯¯å¤„ç†">
  ```java
  // 3. å¼‚å¸¸å¤„ç†å’Œå®¹é”™
  public class RobustMapper extends Mapper<LongWritable, Text, Text, IntWritable> {
      public void map(LongWritable key, Text value, Context context) 
          throws IOException, InterruptedException {
          try {
              // å¤„ç†é€»è¾‘
              processRecord(value, context);
          } catch (Exception e) {
              // è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
              context.getCounter("Errors", "ParseErrors").increment(1);
          }
      }
  }
  ```
  </TabItem>
</Tabs>

### 8.2 è¿ç»´æœ€ä½³å®è·µ

1. **å®šæœŸå¤‡ä»½**ï¼šå®šæœŸå¤‡ä»½HDFSå…ƒæ•°æ®å’Œé‡è¦æ•°æ®
2. **ç›‘æ§å‘Šè­¦**ï¼šè®¾ç½®å…³é”®æŒ‡æ ‡çš„ç›‘æ§å’Œå‘Šè­¦
3. **å®¹é‡è§„åˆ’**ï¼šæå‰è§„åˆ’å­˜å‚¨å’Œè®¡ç®—èµ„æº
4. **ç‰ˆæœ¬ç®¡ç†**ï¼šè°¨æ…å‡çº§ï¼Œä¿æŒç‰ˆæœ¬ä¸€è‡´æ€§
5. **å®‰å…¨åŠ å›º**ï¼šå¯ç”¨Kerberosè®¤è¯ï¼Œæ§åˆ¶è®¿é—®æƒé™

## 9. æ€»ç»“

Hadoopç”Ÿæ€ç³»ç»Ÿä¸ºå¤§æ•°æ®å¤„ç†æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬å­˜å‚¨ã€è®¡ç®—ã€èµ„æºç®¡ç†ç­‰å„ä¸ªæ–¹é¢ã€‚é€šè¿‡åˆç†ä½¿ç”¨Hadoopç»„ä»¶ï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é çš„å¤§æ•°æ®å¤„ç†å¹³å°ã€‚

### å­¦ä¹ å»ºè®®

1. **ç†è§£æ¶æ„**ï¼šæ·±å…¥ç†è§£HDFSã€YARNã€MapReduceçš„æ¶æ„è®¾è®¡
2. **å®è·µç¼–ç¨‹**ï¼šé€šè¿‡å®é™…é¡¹ç›®æŒæ¡MapReduceç¼–ç¨‹
3. **å­¦ä¹ ç”Ÿæ€**ï¼šäº†è§£Hadoopç”Ÿæ€ç³»ç»Ÿä¸­å„ä¸ªç»„ä»¶çš„ç”¨é€”
4. **æ€§èƒ½è°ƒä¼˜**ï¼šå­¦ä¹ æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§æ–¹æ³•
5. **æœ€ä½³å®è·µ**ï¼šæŒæ¡å¼€å‘å’Œè¿ç»´çš„æœ€ä½³å®è·µ

Hadoopç”Ÿæ€ç³»ç»Ÿæ˜¯å¤§æ•°æ®æŠ€æœ¯çš„åŸºç¡€ï¼ŒæŒæ¡å®ƒå°†ä¸ºå­¦ä¹ å…¶ä»–å¤§æ•°æ®æŠ€æœ¯å¥ å®šåšå®çš„åŸºç¡€ã€‚ 