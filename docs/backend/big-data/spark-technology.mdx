---
sidebar_position: 3
title: SparkæŠ€æœ¯è¯¦è§£
description: æ·±å…¥ç†è§£Apache Sparkçš„æ ¸å¿ƒæ¦‚å¿µã€æ¶æ„è®¾è®¡ã€ç¼–ç¨‹æ¨¡å‹å’Œæ€§èƒ½ä¼˜åŒ–
authors: [Laby]
last_update:
  date: 2025-08-16
  author: Laby
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import TOCInline from '@theme/TOCInline';

# SparkæŠ€æœ¯è¯¦è§£

Apache Sparkæ˜¯ä¸€ä¸ªå¿«é€Ÿã€é€šç”¨çš„å¤§è§„æ¨¡æ•°æ®å¤„ç†å¼•æ“ï¼Œå®ƒæä¾›äº†å†…å­˜è®¡ç®—èƒ½åŠ›ï¼Œä½¿å¾—å¤§æ•°æ®å¤„ç†é€Ÿåº¦æ¯”ä¼ ç»ŸMapReduceå¿«10-100å€ã€‚Sparkæ”¯æŒæ‰¹å¤„ç†ã€äº¤äº’å¼æŸ¥è¯¢ã€æµå¤„ç†å’Œæœºå™¨å­¦ä¹ ç­‰å¤šç§è®¡ç®—èŒƒå¼ã€‚

:::info æœ¬æ–‡å†…å®¹æ¦‚è§ˆ
<TOCInline toc={toc} />
:::

:::tip æ ¸å¿ƒä»·å€¼
**Apache Spark = å†…å­˜è®¡ç®— + ç»Ÿä¸€å¹³å° + é«˜æ€§èƒ½ + æ˜“ç”¨æ€§ + ç”Ÿæ€ç³»ç»Ÿ**
- ğŸš€ **å†…å­˜è®¡ç®—**ï¼šæ•°æ®å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè®¡ç®—é€Ÿåº¦å¤§å¹…æå‡
- ğŸ‘¨â€ğŸ’» **ç»Ÿä¸€å¹³å°**ï¼šæ”¯æŒæ‰¹å¤„ç†ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ç­‰å¤šç§è®¡ç®—æ¨¡å¼
- ğŸ” **é«˜æ€§èƒ½**ï¼šæ¯”MapReduceå¿«10-100å€ï¼Œæ”¯æŒå¤æ‚çš„è¿­ä»£ç®—æ³•
- ğŸ”— **æ˜“ç”¨æ€§**ï¼šæä¾›Javaã€Scalaã€Pythonã€Rç­‰å¤šç§API
- ğŸ“š **ç”Ÿæ€ç³»ç»Ÿ**ï¼šSpark SQLã€Spark Streamingã€MLlibã€GraphXç­‰ç»„ä»¶
:::

## 1. Sparkæ ¸å¿ƒæ¦‚å¿µ

### 1.1 Sparkæ¶æ„

Sparké‡‡ç”¨ä¸»ä»æ¶æ„ï¼Œä¸»è¦åŒ…å«ä»¥ä¸‹ç»„ä»¶ï¼š

```mermaid
graph TB
    A[Sparké›†ç¾¤] --> B[Driver Program]
    A --> C[Cluster Manager]
    A --> D[Worker Node]
    
    B --> B1["åº”ç”¨é€»è¾‘"]
    B --> B2["ä»»åŠ¡è°ƒåº¦"]
    
    C --> C1["èµ„æºåˆ†é…"]
    C --> C2["ä»»åŠ¡åˆ†å‘"]
    
    D --> D1[Executor]
    D --> D2[Task]
    
    D1 --> D3["å†…å­˜ç®¡ç†"]
    D1 --> D4["ä»»åŠ¡æ‰§è¡Œ"]
```

### 1.2 Sparkæ ¸å¿ƒæŠ½è±¡

<div className="card">
<div className="card__header">
<h4>Sparkæ ¸å¿ƒæŠ½è±¡</h4>
</div>
<div className="card__body">
<ol>
<li><strong>RDD</strong>ï¼šå¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼Œä¸å¯å˜çš„åˆ†å¸ƒå¼å¯¹è±¡é›†åˆ</li>
<li><strong>DataFrame</strong>ï¼šåŸºäºRDDçš„åˆ†å¸ƒå¼æ•°æ®è¡¨ï¼Œç±»ä¼¼å…³ç³»å‹æ•°æ®åº“è¡¨</li>
<li><strong>Dataset</strong>ï¼šç±»å‹å®‰å…¨çš„DataFrameï¼Œç»“åˆäº†RDDå’ŒDataFrameçš„ä¼˜ç‚¹</li>
<li><strong>SparkContext</strong>ï¼šSparkåº”ç”¨çš„å…¥å£ç‚¹ï¼Œè´Ÿè´£ä¸é›†ç¾¤é€šä¿¡</li>
</ol>
</div>
</div>

#### 1.2.1 Spark 3.xæ–°ç‰¹æ€§
```mermaid
graph TB
    A[Spark 3.xæ–°ç‰¹æ€§] --> B[è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ]
    A --> C[åŠ¨æ€åˆ†åŒºè£å‰ª]
    A --> D[GPUåŠ é€Ÿ]
    A --> E[KubernetesåŸç”Ÿæ”¯æŒ]
    
    B --> B1["AQEä¼˜åŒ–å™¨"]
    B --> B2["åŠ¨æ€åˆå¹¶åˆ†åŒº"]
    
    C --> C1["åˆ†åŒºè£å‰ªä¼˜åŒ–"]
    C --> C2["å‡å°‘æ•°æ®æ‰«æ"]
    
    D --> D1["GPUå†…å­˜ç®¡ç†"]
    D --> D2["æ·±åº¦å­¦ä¹ åŠ é€Ÿ"]
    
    E --> E1["K8sè°ƒåº¦å™¨"]
    E --> E2["å®¹å™¨åŒ–éƒ¨ç½²"]
```

#### 1.2.2 RDDä¾èµ–å…³ç³»ç®¡ç†
```java title="RDDä¾èµ–å…³ç³»ç¤ºä¾‹"
public class RDDDependencyExample {
    public void demonstrateDependencies(JavaSparkContext sc) {
        // 1. çª„ä¾èµ– - ä¸€å¯¹ä¸€ä¾èµ–
        JavaRDD<String> lines = sc.textFile("input.txt");
        JavaRDD<Integer> lengths = lines.map(String::length); // çª„ä¾èµ–
        
        // 2. å®½ä¾èµ– - Shuffleä¾èµ–
        JavaPairRDD<String, Integer> pairs = lines.mapToPair(line -> 
            new Tuple2<>(line.split(" ")[0], 1));
        JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b); // å®½ä¾èµ–
        
        // 3. æ£€æŸ¥ä¾èµ–ç±»å‹
        System.out.println("Lines RDD dependencies: " + lines.dependencies());
        System.out.println("Lengths RDD dependencies: " + lengths.dependencies());
        System.out.println("Counts RDD dependencies: " + counts.dependencies());
        
        // 4. ä¼˜åŒ–å»ºè®®
        if (hasWideDependency(counts)) {
            System.out.println("Warning: Wide dependency detected. Consider repartitioning.");
        }
    }
    
    private boolean hasWideDependency(JavaPairRDD<String, Integer> rdd) {
        return rdd.dependencies().stream()
            .anyMatch(dep -> dep instanceof ShuffleDependency);
    }
}
```

#### RDDç‰¹æ€§
```java title="RDDç‰¹æ€§ç¤ºä¾‹"
public class RDDFeatures {
    public static void main(String[] args) {
        // 1. å¼¹æ€§ï¼ˆResilientï¼‰
        System.out.println("RDDå…·æœ‰å®¹é”™èƒ½åŠ›ï¼Œå¯ä»¥ä»å¤±è´¥ä¸­æ¢å¤");
        
        // 2. åˆ†å¸ƒå¼ï¼ˆDistributedï¼‰
        System.out.println("RDDæ•°æ®åˆ†å¸ƒåœ¨é›†ç¾¤çš„å¤šä¸ªèŠ‚ç‚¹ä¸Š");
        
        // 3. æ•°æ®é›†ï¼ˆDatasetï¼‰
        System.out.println("RDDæ˜¯æ•°æ®é›†åˆï¼Œæ”¯æŒå¤šç§æ•°æ®ç±»å‹");
        
        // 4. ä¸å¯å˜æ€§ï¼ˆImmutableï¼‰
        System.out.println("RDDä¸€æ—¦åˆ›å»ºå°±ä¸èƒ½ä¿®æ”¹ï¼Œåªèƒ½é€šè¿‡è½¬æ¢ç”Ÿæˆæ–°çš„RDD");
        
        // 5. å»¶è¿Ÿè®¡ç®—ï¼ˆLazy Evaluationï¼‰
        System.out.println("RDDè½¬æ¢æ“ä½œæ˜¯å»¶è¿Ÿçš„ï¼Œåªæœ‰é‡åˆ°åŠ¨ä½œæ“ä½œæ—¶æ‰æ‰§è¡Œ");
    }
}
```

## 2. Sparkç¼–ç¨‹æ¨¡å‹

### 2.1 RDDæ“ä½œç±»å‹

Spark RDDæ”¯æŒä¸¤ç§ç±»å‹çš„æ“ä½œï¼š

```mermaid
graph LR
    A[RDDæ“ä½œ] --> B[è½¬æ¢æ“ä½œ]
    A --> C[åŠ¨ä½œæ“ä½œ]
    
    B --> B1["map, filter, flatMap"]
    B --> B2["groupBy, reduceByKey"]
    B --> B3["è¿”å›æ–°çš„RDD"]
    
    C --> C1["collect, count, save"]
    C --> C2["è§¦å‘è®¡ç®—æ‰§è¡Œ"]
    C --> C3["è¿”å›ç»“æœæˆ–ä¿å­˜æ•°æ®"]
```

### 2.2 åŸºæœ¬RDDæ“ä½œ

<Tabs>
  <TabItem value="transformations" label="è½¬æ¢æ“ä½œ" default>
  ```java title="RDDè½¬æ¢æ“ä½œç¤ºä¾‹"
  public class RDDTransformations {
      public void demonstrateTransformations(JavaRDD<String> lines) {
          // 1. map - ä¸€å¯¹ä¸€è½¬æ¢
          JavaRDD<Integer> lengths = lines.map(String::length);
          
          // 2. filter - è¿‡æ»¤æ•°æ®
          JavaRDD<String> longLines = lines.filter(line -> line.length() > 100);
          
          // 3. flatMap - ä¸€å¯¹å¤šè½¬æ¢
          JavaRDD<String> words = lines.flatMap(line -> 
              Arrays.asList(line.split(" ")).iterator());
          
          // 4. distinct - å»é‡
          JavaRDD<String> uniqueWords = words.distinct();
          
          // 5. sample - é‡‡æ ·
          JavaRDD<String> sampledLines = lines.sample(false, 0.1);
      }
  }
  ```
  </TabItem>
  <TabItem value="actions" label="åŠ¨ä½œæ“ä½œ">
  ```java title="RDDåŠ¨ä½œæ“ä½œç¤ºä¾‹"
  public class RDDActions {
      public void demonstrateActions(JavaRDD<String> lines) {
          // 1. collect - æ”¶é›†æ‰€æœ‰æ•°æ®åˆ°Driver
          List<String> allLines = lines.collect();
          
          // 2. count - è®¡ç®—å…ƒç´ ä¸ªæ•°
          long lineCount = lines.count();
          
          // 3. take - å–å‰Nä¸ªå…ƒç´ 
          List<String> firstLines = lines.take(10);
          
          // 4. reduce - å½’çº¦æ“ä½œ
          String longestLine = lines.reduce((a, b) -> 
              a.length() > b.length() ? a : b);
          
          // 5. foreach - å¯¹æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œ
          lines.foreach(line -> System.out.println("Processing: " + line));
      }
  }
  ```
  </TabItem>
  <TabItem value="keyvalue" label="é”®å€¼å¯¹æ“ä½œ">
  ```java title="é”®å€¼å¯¹RDDæ“ä½œç¤ºä¾‹"
  public class KeyValueRDDOperations {
      public void demonstrateKeyValueOperations(JavaPairRDD<String, Integer> pairs) {
          // 1. reduceByKey - æŒ‰é”®å½’çº¦
          JavaPairRDD<String, Integer> sums = pairs.reduceByKey((a, b) -> a + b);
          
          // 2. groupByKey - æŒ‰é”®åˆ†ç»„
          JavaPairRDD<String, Iterable<Integer>> groups = pairs.groupByKey();
          
          // 3. sortByKey - æŒ‰é”®æ’åº
          JavaPairRDD<String, Integer> sorted = pairs.sortByKey();
          
          // 4. join - è¿æ¥æ“ä½œ
          JavaPairRDD<String, Tuple2<Integer, String>> joined = 
              pairs.join(otherPairs);
          
          // 5. cogroup - ååŒåˆ†ç»„
          JavaPairRDD<String, Tuple2<Iterable<Integer>, Iterable<String>>> cogrouped = 
              pairs.cogroup(otherPairs);
      }
  }
  ```
  </TabItem>
</Tabs>

## 3. Spark SQLå’ŒDataFrame

### 3.1 DataFrameæ¦‚å¿µ

DataFrameæ˜¯Sparkä¸­å¤„ç†ç»“æ„åŒ–æ•°æ®çš„æ ¸å¿ƒæŠ½è±¡ï¼š

#### 3.1.1 Catalystä¼˜åŒ–å™¨å·¥ä½œåŸç†
```mermaid
graph TB
    A[SQLæŸ¥è¯¢] --> B[è§£æé˜¶æ®µ]
    B --> C[åˆ†æé˜¶æ®µ]
    C --> D[ä¼˜åŒ–é˜¶æ®µ]
    D --> E[ç‰©ç†è®¡åˆ’]
    E --> F[ä»£ç ç”Ÿæˆ]
    
    B --> B1["è¯­æ³•æ ‘æ„å»º"]
    C --> C1["è¯­ä¹‰åˆ†æ"]
    C --> C2["ç±»å‹æ£€æŸ¥"]
    
    D --> D1["å¸¸é‡æŠ˜å "]
    D --> D2["è°“è¯ä¸‹æ¨"]
    D --> D3["åˆ—è£å‰ª"]
    D --> D4["åˆ†åŒºè£å‰ª"]
    
    E --> E1["ç‰©ç†ç®—å­é€‰æ‹©"]
    E --> E2["å¹¶è¡Œåº¦è®¾ç½®"]
    
    F --> F1["Javaå­—èŠ‚ç ç”Ÿæˆ"]
    F --> F2["æ€§èƒ½ä¼˜åŒ–"]
```

#### 3.1.2 è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ(AQE)
```java title="AQEä¼˜åŒ–ç¤ºä¾‹"
public class AQEExample {
    public void demonstrateAQE(SparkSession spark) {
        // å¯ç”¨AQE
        spark.conf().set("spark.sql.adaptive.enabled", "true");
        spark.conf().set("spark.sql.adaptive.coalescePartitions.enabled", "true");
        spark.conf().set("spark.sql.adaptive.skewJoin.enabled", "true");
        spark.conf().set("spark.sql.adaptive.localShuffleReader.enabled", "true");
        
        // åˆ›å»ºæµ‹è¯•æ•°æ®
        Dataset<Row> users = spark.createDataFrame(Arrays.asList(
            RowFactory.create("user1", "Alice", 25, "Engineer"),
            RowFactory.create("user2", "Bob", 30, "Manager"),
            RowFactory.create("user3", "Charlie", 35, "Director")
        ), new StructType()
            .add("id", DataTypes.StringType)
            .add("name", DataTypes.StringType)
            .add("age", DataTypes.IntegerType)
            .add("job", DataTypes.StringType));
        
        Dataset<Row> orders = spark.createDataFrame(Arrays.asList(
            RowFactory.create("order1", "user1", 100.0),
            RowFactory.create("order2", "user2", 200.0),
            RowFactory.create("order3", "user1", 150.0)
        ), new StructType()
            .add("orderId", DataTypes.StringType)
            .add("userId", DataTypes.StringType)
            .add("amount", DataTypes.DoubleType));
        
        // å¤æ‚æŸ¥è¯¢ - AQEä¼šè‡ªåŠ¨ä¼˜åŒ–
        Dataset<Row> result = users.join(orders, users.col("id").equalTo(orders.col("userId")))
            .groupBy("job")
            .agg(functions.avg("amount").as("avg_amount"))
            .filter(col("avg_amount").gt(100));
        
        result.explain(true); // æ˜¾ç¤ºä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’
        result.show();
    }
}
```

```mermaid
graph TB
    A[DataFrame] --> B[ç»“æ„åŒ–æ•°æ®]
    A --> C[Schemaä¿¡æ¯]
    A --> D[ä¼˜åŒ–æ‰§è¡Œ]
    
    B --> B1["è¡Œå’Œåˆ—ç»„ç»‡"]
    B --> B2["ç±»å‹å®‰å…¨"]
    
    C --> C1["åˆ—åå’Œç±»å‹"]
    C --> C2["å…ƒæ•°æ®ç®¡ç†"]
    
    D --> D1["Catalystä¼˜åŒ–å™¨"]
    D --> D2["ä»£ç ç”Ÿæˆ"]
```

### 3.2 DataFrameæ“ä½œç¤ºä¾‹

<div className="code-with-callout">

```java title="DataFrameæ“ä½œç¤ºä¾‹"
public class DataFrameOperations {
    public void demonstrateDataFrameOperations(SparkSession spark) {
        // 1. åˆ›å»ºDataFrame
        List<Row> data = Arrays.asList(
            RowFactory.create("Alice", 25, "Engineer"),
            RowFactory.create("Bob", 30, "Manager"),
            RowFactory.create("Charlie", 35, "Director")
        );
        
        StructType schema = new StructType()
            .add("name", DataTypes.StringType)
            .add("age", DataTypes.IntegerType)
            .add("job", DataTypes.StringType);
        
        Dataset<Row> df = spark.createDataFrame(data, schema);
        
        // 2. æ˜¾ç¤ºæ•°æ®
        df.show();
        
        // 3. è¿‡æ»¤æ•°æ®
        Dataset<Row> youngPeople = df.filter(col("age").lt(30));
        
        // 4. é€‰æ‹©åˆ—
        Dataset<Row> namesAndAges = df.select("name", "age");
        
        // 5. åˆ†ç»„èšåˆ
        Dataset<Row> jobCounts = df.groupBy("job").count();
        
        // 6. SQLæŸ¥è¯¢
        df.createOrReplaceTempView("people");
        Dataset<Row> sqlResult = spark.sql(
            "SELECT job, AVG(age) as avg_age FROM people GROUP BY job"
        );
    }
}
```

:::info DataFrameä¼˜åŠ¿
DataFrameæä¾›äº†ç±»ä¼¼SQLçš„æŸ¥è¯¢æ¥å£ï¼Œæ”¯æŒä¼˜åŒ–æ‰§è¡Œï¼Œæ¯”RDDæ“ä½œæ€§èƒ½æ›´å¥½ï¼Œç‰¹åˆ«é€‚åˆç»“æ„åŒ–æ•°æ®å¤„ç†ã€‚
:::
</div>

## 4. Spark Streaming

### 4.1 æµå¤„ç†æ¶æ„

Spark Streamingå°†æµå¼è®¡ç®—åˆ†è§£ä¸ºä¸€ç³»åˆ—å°æ‰¹é‡çš„æ‰¹å¤„ç†ä½œä¸šï¼š

#### 4.1.1 ç»“æ„åŒ–æµå¤„ç†(Structured Streaming)
```mermaid
graph TB
    A[æ•°æ®æº] --> B[ç»“æ„åŒ–æµ]
    B --> C[æŸ¥è¯¢å¤„ç†]
    C --> D[ç»“æœè¾“å‡º]
    
    A --> A1["Kafka"]
    A --> A2["æ–‡ä»¶æµ"]
    A --> A3["Socket"]
    
    B --> B1["æ— é™DataFrame"]
    B --> B2["äº‹ä»¶æ—¶é—´å¤„ç†"]
    
    C --> C1["çª—å£èšåˆ"]
    C --> C2["æ°´å°å¤„ç†"]
    C --> C3["çŠ¶æ€ç®¡ç†"]
    
    D --> D1["æ–‡ä»¶è¾“å‡º"]
    D --> D2["æ•°æ®åº“å†™å…¥"]
    D --> D3["æ§åˆ¶å°è¾“å‡º"]
```

#### 4.1.2 ç»“æ„åŒ–æµå¤„ç†ç¤ºä¾‹
```java title="ç»“æ„åŒ–æµå¤„ç†ç¤ºä¾‹"
public class StructuredStreamingExample {
    public void buildStructuredStreaming(SparkSession spark) {
        // 1. ä»Kafkaè¯»å–æµæ•°æ®
        Dataset<Row> streamDF = spark
            .readStream()
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "user-events")
            .option("startingOffsets", "latest")
            .load();
        
        // 2. è§£æJSONæ•°æ®
        Dataset<Row> parsedDF = streamDF
            .selectExpr("CAST(value AS STRING) as json")
            .select(functions.from_json(col("json"), getUserEventSchema()).as("data"))
            .select("data.*");
        
        // 3. äº‹ä»¶æ—¶é—´å¤„ç†å’Œæ°´å°
        Dataset<Row> withWatermark = parsedDF
            .withWatermark("timestamp", "10 minutes")
            .groupBy(
                functions.window(col("timestamp"), "5 minutes"),
                col("userId")
            )
            .agg(
                functions.count("*").as("event_count"),
                functions.avg("amount").as("avg_amount")
            );
        
        // 4. è¾“å‡ºåˆ°æ§åˆ¶å°
        StreamingQuery query = withWatermark
            .writeStream()
            .outputMode("append")
            .format("console")
            .option("truncate", false)
            .start();
        
        // 5. ç­‰å¾…æŸ¥è¯¢ç»ˆæ­¢
        query.awaitTermination();
    }
    
    private StructType getUserEventSchema() {
        return new StructType()
            .add("userId", DataTypes.StringType)
            .add("eventType", DataTypes.StringType)
            .add("amount", DataTypes.DoubleType)
            .add("timestamp", DataTypes.TimestampType);
    }
}

// æœ‰çŠ¶æ€æµå¤„ç†ç¤ºä¾‹
public class StatefulStreamingExample {
    public void buildStatefulStreaming(SparkSession spark) {
        // 1. ä»Kafkaè¯»å–ç”¨æˆ·è¡Œä¸ºæµ
        Dataset<Row> userBehaviorStream = spark
            .readStream()
            .format("kafka")
            .option("kafka.bootstrap.servers", "localhost:9092")
            .option("subscribe", "user-behavior")
            .load();
        
        // 2. è§£æç”¨æˆ·è¡Œä¸º
        Dataset<Row> behaviorDF = userBehaviorStream
            .selectExpr("CAST(value AS STRING) as json")
            .select(functions.from_json(col("json"), getBehaviorSchema()).as("data"))
            .select("data.*");
        
        // 3. æœ‰çŠ¶æ€èšåˆ - ç”¨æˆ·ä¼šè¯ç»Ÿè®¡
        Dataset<Row> sessionStats = behaviorDF
            .withWatermark("timestamp", "1 hour")
            .groupByKey((MapFunction<Row, String>) row -> row.getAs("userId"), Encoders.STRING())
            .flatMapGroupsWithState(
                new UserSessionAggregator(),
                OutputMode.Append(),
                Encoders.bean(UserSession.class),
                Encoders.bean(UserSession.class),
                GroupStateTimeout.ProcessingTimeTimeout()
            );
        
        // 4. è¾“å‡ºç»“æœ
        StreamingQuery query = sessionStats
            .writeStream()
            .outputMode("append")
            .format("console")
            .start();
        
        query.awaitTermination();
    }
}

// ç”¨æˆ·ä¼šè¯èšåˆå™¨
class UserSessionAggregator implements FlatMapGroupsWithStateFunction<String, Row, UserSession, UserSession> {
    @Override
    public Iterator<UserSession> call(String userId, Iterator<Row> events, GroupState<UserSession> state) {
        List<UserSession> results = new ArrayList<>();
        
        // è·å–å½“å‰çŠ¶æ€
        UserSession currentSession = state.exists() ? state.get() : new UserSession(userId);
        
        // å¤„ç†äº‹ä»¶
        while (events.hasNext()) {
            Row event = events.next();
            currentSession.updateSession(event);
        }
        
        // æ›´æ–°çŠ¶æ€
        state.update(currentSession);
        
        // å¦‚æœä¼šè¯å®Œæˆï¼Œè¾“å‡ºç»“æœ
        if (currentSession.isCompleted()) {
            results.add(currentSession);
            state.remove();
        }
        
        return results.iterator();
    }
}
```

```mermaid
graph LR
    A[æ•°æ®æµ] --> B[æ•°æ®æ¥æ”¶å™¨]
    B --> C[å¾®æ‰¹å¤„ç†]
    C --> D[RDDè½¬æ¢]
    D --> E[ç»“æœè¾“å‡º]
    
    B --> B1["Kafka/Flumeç­‰"]
    C --> C1["æ—¶é—´çª—å£"]
    D --> D1["RDDæ“ä½œ"]
    E --> E1["æ–‡ä»¶/æ•°æ®åº“"]
```

### 4.2 æµå¤„ç†ç¤ºä¾‹

<Tabs>
  <TabItem value="basic" label="åŸºç¡€æµå¤„ç†" default>
  ```java title="åŸºç¡€æµå¤„ç†ç¤ºä¾‹"
  public class BasicStreaming {
      public void processStream(SparkSession spark) {
          // åˆ›å»ºStreamingContext
          JavaStreamingContext ssc = new JavaStreamingContext(
              spark.sparkContext(), Durations.seconds(5));
          
          // åˆ›å»ºDStream
          JavaReceiverInputDStream<String> lines = ssc.socketTextStream(
              "localhost", 9999);
          
          // å¤„ç†æ•°æ®æµ
          JavaDStream<String> words = lines.flatMap(line -> 
              Arrays.asList(line.split(" ")).iterator());
          
          JavaPairDStream<String, Integer> wordCounts = words
              .mapToPair(word -> new Tuple2<>(word, 1))
              .reduceByKey((a, b) -> a + b);
          
          // è¾“å‡ºç»“æœ
          wordCounts.print();
          
          // å¯åŠ¨æµå¤„ç†
          ssc.start();
          ssc.awaitTermination();
      }
  }
  ```
  </TabItem>
  <TabItem value="kafka" label="Kafkaé›†æˆ">
  ```java title="Kafkaæµå¤„ç†ç¤ºä¾‹"
  public class KafkaStreaming {
      public void processKafkaStream(SparkSession spark) {
          JavaStreamingContext ssc = new JavaStreamingContext(
              spark.sparkContext(), Durations.seconds(5));
          
          // Kafkaé…ç½®
          Map<String, Object> kafkaParams = new HashMap<>();
          kafkaParams.put("bootstrap.servers", "localhost:9092");
          kafkaParams.put("key.deserializer", StringDeserializer.class);
          kafkaParams.put("value.deserializer", StringDeserializer.class);
          kafkaParams.put("group.id", "spark-streaming-group");
          kafkaParams.put("auto.offset.reset", "latest");
          
          // åˆ›å»ºKafka DStream
          JavaInputDStream<ConsumerRecord<String, String>> stream = 
              KafkaUtils.createDirectStream(ssc, 
                  LocationStrategies.PreferConsistent(),
                  ConsumerStrategies.Subscribe(
                      Arrays.asList("input-topic"), kafkaParams));
          
          // å¤„ç†æ¶ˆæ¯
          JavaDStream<String> lines = stream.map(record -> record.value());
          
          // è¯é¢‘ç»Ÿè®¡
          JavaPairDStream<String, Integer> wordCounts = lines
              .flatMap(line -> Arrays.asList(line.split(" ")).iterator())
              .mapToPair(word -> new Tuple2<>(word, 1))
              .reduceByKey((a, b) -> a + b);
          
          wordCounts.print();
          
          ssc.start();
          ssc.awaitTermination();
      }
  }
  ```
  </TabItem>
</Tabs>

## 5. Spark MLlibæœºå™¨å­¦ä¹ 

### 5.1 MLlibç»„ä»¶

MLlibæ˜¯Sparkçš„æœºå™¨å­¦ä¹ åº“ï¼Œæä¾›äº†ä¸°å¯Œçš„ç®—æ³•å’Œå·¥å…·ï¼š

#### 5.1.1 MLlibç®—æ³•åˆ†ç±»
```mermaid
graph TB
    A[MLlibç®—æ³•] --> B[åˆ†ç±»ç®—æ³•]
    A --> C[å›å½’ç®—æ³•]
    A --> D[èšç±»ç®—æ³•]
    A --> E[æ¨èç®—æ³•]
    A --> F[ç‰¹å¾å·¥ç¨‹]
    
    B --> B1["é€»è¾‘å›å½’"]
    B --> B2["å†³ç­–æ ‘"]
    B --> B3["éšæœºæ£®æ—"]
    B --> B4["SVM"]
    
    C --> C1["çº¿æ€§å›å½’"]
    C --> C2["å²­å›å½’"]
    C --> C3["Lassoå›å½’"]
    
    D --> D1["K-means"]
    D --> D2["LDA"]
    D --> D3["é«˜æ–¯æ··åˆ"]
    
    E --> E1["ALS"]
    E --> E2["ååŒè¿‡æ»¤"]
    
    F --> F1["TF-IDF"]
    F --> F2["Word2Vec"]
    F --> F3["ç‰¹å¾é€‰æ‹©"]
```

#### 5.1.2 ç‰¹å¾å·¥ç¨‹Pipelineç¤ºä¾‹
```java title="ç‰¹å¾å·¥ç¨‹Pipelineç¤ºä¾‹"
public class FeatureEngineeringPipeline {
    public PipelineModel buildFeaturePipeline(SparkSession spark) {
        // 1. å­—ç¬¦ä¸²ç´¢å¼•åŒ–
        StringIndexer stringIndexer = new StringIndexer()
            .setInputCol("category")
            .setOutputCol("categoryIndex")
            .setHandleInvalid("skip");
        
        // 2. ç‹¬çƒ­ç¼–ç 
        OneHotEncoder oneHotEncoder = new OneHotEncoder()
            .setInputCol("categoryIndex")
            .setOutputCol("categoryOneHot");
        
        // 3. æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–
        StandardScaler standardScaler = new StandardScaler()
            .setInputCol("numericalFeatures")
            .setOutputCol("scaledFeatures")
            .setWithStd(true)
            .setWithMean(true);
        
        // 4. ç‰¹å¾å‘é‡ç»„è£…
        VectorAssembler assembler = new VectorAssembler()
            .setInputCols(new String[]{"categoryOneHot", "scaledFeatures", "otherFeatures"})
            .setOutputCol("features");
        
        // 5. ç‰¹å¾é€‰æ‹©
        ChiSqSelector featureSelector = new ChiSqSelector()
            .setNumTopFeatures(20)
            .setFeaturesCol("features")
            .setLabelCol("label")
            .setOutputCol("selectedFeatures");
        
        // 6. æ„å»ºPipeline
        Pipeline pipeline = new Pipeline()
            .setStages(new PipelineStage[]{
                stringIndexer, oneHotEncoder, standardScaler, assembler, featureSelector
            });
        
        return pipeline;
    }
}

// å®Œæ•´çš„æœºå™¨å­¦ä¹ Pipelineç¤ºä¾‹
public class CompleteMLPipeline {
    public void buildCompletePipeline(SparkSession spark, Dataset<Row> trainingData) {
        // 1. ç‰¹å¾å·¥ç¨‹Pipeline
        FeatureEngineeringPipeline featurePipeline = new FeatureEngineeringPipeline();
        PipelineModel featureModel = featurePipeline.buildFeaturePipeline(spark);
        
        // 2. æœºå™¨å­¦ä¹ ç®—æ³•
        RandomForestClassifier classifier = new RandomForestClassifier()
            .setLabelCol("label")
            .setFeaturesCol("selectedFeatures")
            .setNumTrees(100)
            .setMaxDepth(10)
            .setMaxBins(32)
            .setSeed(42);
        
        // 3. æ¨¡å‹è¯„ä¼°å™¨
        MulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator()
            .setLabelCol("label")
            .setPredictionCol("prediction")
            .setMetricName("accuracy");
        
        // 4. äº¤å‰éªŒè¯
        CrossValidator crossValidator = new CrossValidator()
            .setEstimator(new Pipeline().setStages(new PipelineStage[]{
                featureModel, classifier
            }))
            .setEvaluator(evaluator)
            .setNumFolds(5)
            .setParallelism(2);
        
        // 5. è®­ç»ƒæ¨¡å‹
        CrossValidatorModel cvModel = crossValidator.fit(trainingData);
        
        // 6. æ¨¡å‹è¯„ä¼°
        Dataset<Row> predictions = cvModel.transform(trainingData);
        double accuracy = evaluator.evaluate(predictions);
        System.out.println("Cross-validation accuracy: " + accuracy);
        
        // 7. ä¿å­˜æ¨¡å‹
        cvModel.save("models/random_forest_model");
        
        // 8. ç‰¹å¾é‡è¦æ€§åˆ†æ
        PipelineModel bestModel = (PipelineModel) cvModel.bestModel();
        RandomForestClassificationModel rfModel = (RandomForestClassificationModel) 
            bestModel.stages()[bestModel.stages().length - 1];
        
        System.out.println("Feature importances:");
        double[] importances = rfModel.featureImportances().toArray();
        for (int i = 0; i < importances.length; i++) {
            System.out.println("Feature " + i + ": " + importances[i]);
        }
    }
}
```

```mermaid
graph TB
    A[MLlib] --> B[ç‰¹å¾å·¥ç¨‹]
    A --> C[æœºå™¨å­¦ä¹ ç®—æ³•]
    A --> D[æ¨¡å‹è¯„ä¼°]
    A --> E[ç®¡é“ç®¡ç†]
    
    B --> B1["ç‰¹å¾æå–"]
    B --> B2["ç‰¹å¾è½¬æ¢"]
    B --> B3["ç‰¹å¾é€‰æ‹©"]
    
    C --> C1["åˆ†ç±»ç®—æ³•"]
    C --> C2["å›å½’ç®—æ³•"]
    C --> C3["èšç±»ç®—æ³•"]
    
    D --> D1["äº¤å‰éªŒè¯"]
    D --> D2["æ¨¡å‹é€‰æ‹©"]
    
    E --> E1["Pipeline"]
    E --> E2["å·¥ä½œæµç®¡ç†"]
```

### 5.2 æœºå™¨å­¦ä¹ ç¤ºä¾‹

<div className="code-with-callout">

```java title="æœºå™¨å­¦ä¹ Pipelineç¤ºä¾‹"
public class MLPipelineExample {
    public void buildMLPipeline(SparkSession spark, Dataset<Row> data) {
        // 1. ç‰¹å¾å·¥ç¨‹
        StringIndexer indexer = new StringIndexer()
            .setInputCol("category")
            .setOutputCol("categoryIndex");
        
        VectorAssembler assembler = new VectorAssembler()
            .setInputCols(new String[]{"feature1", "feature2", "categoryIndex"})
            .setOutputCol("features");
        
        // 2. æœºå™¨å­¦ä¹ ç®—æ³•
        RandomForestClassifier classifier = new RandomForestClassifier()
            .setLabelCol("label")
            .setFeaturesCol("features")
            .setNumTrees(10);
        
        // 3. æ„å»ºPipeline
        Pipeline pipeline = new Pipeline()
            .setStages(new PipelineStage[]{indexer, assembler, classifier});
        
        // 4. è®­ç»ƒæ¨¡å‹
        PipelineModel model = pipeline.fit(data);
        
        // 5. é¢„æµ‹
        Dataset<Row> predictions = model.transform(data);
        predictions.show();
        
        // 6. æ¨¡å‹è¯„ä¼°
        MulticlassClassificationEvaluator evaluator = 
            new MulticlassClassificationEvaluator()
                .setLabelCol("label")
                .setPredictionCol("prediction")
                .setMetricName("accuracy");
        
        double accuracy = evaluator.evaluate(predictions);
        System.out.println("Accuracy: " + accuracy);
    }
}
```

:::info MLlibä¼˜åŠ¿
MLlibæä¾›äº†åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†ï¼Œä¸Sparkç”Ÿæ€ç³»ç»Ÿæ— ç¼é›†æˆã€‚
:::
</div>

## 6. Sparkæ€§èƒ½ä¼˜åŒ–

### 6.1 å†…å­˜ç®¡ç†

Sparkå†…å­˜ç®¡ç†æ˜¯æ€§èƒ½ä¼˜åŒ–çš„å…³é”®ï¼š

#### 6.1.1 å†…å­˜ç®¡ç†æ¶æ„
```mermaid
graph TB
    A[Sparkå†…å­˜] --> B[æ‰§è¡Œå†…å­˜]
    A --> C[å­˜å‚¨å†…å­˜]
    A --> D[ç”¨æˆ·å†…å­˜]
    A --> E[é¢„ç•™å†…å­˜]
    
    B --> B1["è®¡ç®—å’Œshuffle"]
    B --> B2["åŠ¨æ€åˆ†é…"]
    B --> B3["å¯è¢«å­˜å‚¨å†…å­˜å ç”¨"]
    
    C --> C1["ç¼“å­˜å’Œå¹¿æ’­"]
    C --> C2["LRUæ·˜æ±°ç­–ç•¥"]
    C --> C3["å¯è¢«æ‰§è¡Œå†…å­˜å ç”¨"]
    
    D --> D1["ç”¨æˆ·ä»£ç å’Œæ•°æ®ç»“æ„"]
    D --> D2["å›ºå®šå¤§å°"]
    
    E --> E1["ç³»ç»Ÿé¢„ç•™"]
    E --> E2["é˜²æ­¢OOM"]
```

#### 6.1.2 å†…å­˜è°ƒä¼˜ç­–ç•¥
```java title="å†…å­˜è°ƒä¼˜ç¤ºä¾‹"
public class SparkMemoryOptimization {
    public void optimizeMemory(SparkConf conf) {
        // 1. æ‰§è¡Œå†…å­˜é…ç½®
        conf.set("spark.executor.memory", "8g");
        conf.set("spark.executor.memoryOverhead", "2g"); // å †å¤–å†…å­˜
        conf.set("spark.memory.fraction", "0.8"); // æ‰§è¡Œå’Œå­˜å‚¨å†…å­˜å æ¯”
        conf.set("spark.memory.storageFraction", "0.3"); // å­˜å‚¨å†…å­˜å æ¯”
        
        // 2. åºåˆ—åŒ–é…ç½®
        conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");
        conf.set("spark.kryo.registrationRequired", "false");
        conf.set("spark.kryo.registrator", "com.example.MyKryoRegistrator");
        
        // 3. å‹ç¼©é…ç½®
        conf.set("spark.sql.inMemoryColumnarStorage.compressed", "true");
        conf.set("spark.sql.inMemoryColumnarStorage.batchSize", "10000");
        
        // 4. å¹¿æ’­å˜é‡é…ç½®
        conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760"); // 10MB
        
        // 5. åŠ¨æ€åˆ†é…é…ç½®
        conf.set("spark.dynamicAllocation.enabled", "true");
        conf.set("spark.dynamicAllocation.minExecutors", "2");
        conf.set("spark.dynamicAllocation.maxExecutors", "20");
        conf.set("spark.dynamicAllocation.initialExecutors", "5");
        
        System.out.println("Memory optimization configured");
    }
    
    public void optimizeDataStructures(JavaRDD<String> data) {
        // 1. ä½¿ç”¨å¹¿æ’­å˜é‡å‡å°‘æ•°æ®ä¼ è¾“
        List<String> stopWords = Arrays.asList("the", "a", "an", "and", "or", "but");
        Broadcast<List<String>> stopWordsBroadcast = data.context().broadcast(stopWords, 
            ClassTag$.MODULE$.apply(List.class));
        
        // 2. ä½¿ç”¨ç´¯åŠ å™¨è¿›è¡Œè®¡æ•°
        Accumulator<Integer> totalWords = data.context().accumulator(0, "TotalWords");
        Accumulator<Integer> filteredWords = data.context().accumulator(0, "FilteredWords");
        
        // 3. ä¼˜åŒ–RDDæ“ä½œ
        JavaRDD<String> optimizedData = data
            .mapPartitions(iterator -> {
                List<String> batch = new ArrayList<>();
                while (iterator.hasNext()) {
                    String line = iterator.next();
                    if (!stopWordsBroadcast.value().contains(line.toLowerCase())) {
                        batch.add(line);
                        filteredWords.add(1);
                    }
                    totalWords.add(1);
                }
                return batch.iterator();
            })
            .cache(); // ç¼“å­˜ä¸­é—´ç»“æœ
        
        // 4. ä½¿ç”¨mapPartitionså‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
        JavaRDD<String> processedData = optimizedData.mapPartitions(iterator -> {
            List<String> results = new ArrayList<>();
            while (iterator.hasNext()) {
                String word = iterator.next();
                results.add(word.toUpperCase());
            }
            return results.iterator();
        });
        
        System.out.println("Total words: " + totalWords.value());
        System.out.println("Filtered words: " + filteredWords.value());
    }
}

// è‡ªå®šä¹‰Kryoåºåˆ—åŒ–æ³¨å†Œå™¨
public class MyKryoRegistrator implements KryoRegistrator {
    @Override
    public void registerClasses(Kryo kryo) {
        // æ³¨å†Œè‡ªå®šä¹‰ç±»
        kryo.register(User.class);
        kryo.register(Product.class);
        kryo.register(Order.class);
        
        // æ³¨å†Œé›†åˆç±»
        kryo.register(ArrayList.class);
        kryo.register(HashMap.class);
        kryo.register(HashSet.class);
    }
}
```

```mermaid
graph TB
    A[Sparkå†…å­˜] --> B[æ‰§è¡Œå†…å­˜]
    A --> C[å­˜å‚¨å†…å­˜]
    A --> D[ç”¨æˆ·å†…å­˜]
    
    B --> B1["è®¡ç®—å’Œshuffle"]
    C --> C1["ç¼“å­˜å’Œå¹¿æ’­"]
    D --> D1["ç”¨æˆ·ä»£ç å’Œæ•°æ®ç»“æ„"]
    
    B1 --> B2["åŠ¨æ€åˆ†é…"]
    C1 --> C2["LRUæ·˜æ±°"]
    D1 --> D2["å›ºå®šå¤§å°"]
```

### 6.2 ä¼˜åŒ–ç­–ç•¥

<Tabs>
  <TabItem value="memory" label="å†…å­˜ä¼˜åŒ–" default>
  ```java
  // 1. åˆç†è®¾ç½®å†…å­˜é…ç½®
  SparkConf conf = new SparkConf()
      .set("spark.executor.memory", "8g")
      .set("spark.storage.memoryFraction", "0.6")
      .set("spark.sql.adaptive.enabled", "true");
  ```
  </TabItem>
  <TabItem value="partition" label="åˆ†åŒºä¼˜åŒ–">
  ```java
  // 2. åˆç†è®¾ç½®åˆ†åŒºæ•°
  JavaRDD<String> data = sc.textFile("input.txt");
  JavaRDD<String> repartitioned = data.repartition(100);
  
  // 3. ä½¿ç”¨coalesceå‡å°‘åˆ†åŒº
  JavaRDD<String> coalesced = data.coalesce(50);
  ```
  </TabItem>
  <TabItem value="cache" label="ç¼“å­˜ç­–ç•¥">
  ```java
  // 4. åˆç†ä½¿ç”¨ç¼“å­˜
  JavaRDD<String> cached = data.cache(); // å†…å­˜ç¼“å­˜
  JavaRDD<String> persisted = data.persist(StorageLevel.MEMORY_AND_DISK());
  
  // 5. å¹¿æ’­å˜é‡
  Broadcast<List<String>> broadcastVar = sc.broadcast(largeList);
  ```
  </TabItem>
</Tabs>

## 7. Sparkéƒ¨ç½²å’Œé…ç½®

### 7.1 éƒ¨ç½²æ¨¡å¼

Sparkæ”¯æŒå¤šç§éƒ¨ç½²æ¨¡å¼ï¼š

#### 7.1.1 Kuberneteséƒ¨ç½²æ¶æ„
```mermaid
graph TB
    A[Kubernetesé›†ç¾¤] --> B[Spark Operator]
    B --> C[Spark Driver Pod]
    C --> D[Spark Executor Pods]
    
    A --> E[æŒä¹…åŒ–å­˜å‚¨]
    A --> F[é…ç½®ç®¡ç†]
    
    B --> B1["ç®¡ç†Sparkåº”ç”¨"]
    B --> B2["èµ„æºè°ƒåº¦"]
    
    C --> C1["åº”ç”¨æ§åˆ¶"]
    C --> C2["ä»»åŠ¡åˆ†å‘"]
    
    D --> D1["è®¡ç®—æ‰§è¡Œ"]
    D --> D2["æ•°æ®ç¼“å­˜"]
    
    E --> E1["HDFS/PVC"]
    F --> F1["ConfigMap/Secret"]
```

#### 7.1.2 äº‘åŸç”Ÿéƒ¨ç½²é…ç½®
```java title="Kuberneteséƒ¨ç½²é…ç½®ç¤ºä¾‹"
public class KubernetesDeployment {
    public void configureKubernetesDeployment(SparkConf conf) {
        // 1. Kubernetesé…ç½®
        conf.set("spark.master", "k8s://https://kubernetes.default.svc");
        conf.set("spark.kubernetes.container.image", "spark:3.4.0");
        conf.set("spark.kubernetes.namespace", "spark-jobs");
        
        // 2. èµ„æºé…ç½®
        conf.set("spark.executor.instances", "5");
        conf.set("spark.executor.memory", "4g");
        conf.set("spark.executor.cores", "2");
        conf.set("spark.driver.memory", "2g");
        conf.set("spark.driver.cores", "1");
        
        // 3. å­˜å‚¨é…ç½®
        conf.set("spark.kubernetes.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName", "spark-local-dir-1");
        conf.set("spark.kubernetes.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path", "/tmp");
        conf.set("spark.kubernetes.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly", "false");
        
        // 4. ç½‘ç»œé…ç½®
        conf.set("spark.kubernetes.driver.serviceAccountName", "spark");
        conf.set("spark.kubernetes.executor.serviceAccountName", "spark");
        
        // 5. å®‰å…¨é…ç½®
        conf.set("spark.kubernetes.authenticate.driver.serviceAccountName", "spark");
        conf.set("spark.kubernetes.authenticate.executor.serviceAccountName", "spark");
        
        System.out.println("Kubernetes deployment configured");
    }
    
    public void configureResourceQuotas() {
        // é…ç½®èµ„æºé…é¢
        String resourceQuota = 
            "apiVersion: v1\n" +
            "kind: ResourceQuota\n" +
            "metadata:\n" +
            "  name: spark-quota\n" +
            "  namespace: spark-jobs\n" +
            "spec:\n" +
            "  hard:\n" +
            "    requests.cpu: \"20\"\n" +
            "    requests.memory: 40Gi\n" +
            "    limits.cpu: \"40\"\n" +
            "    limits.memory: 80Gi\n" +
            "    persistentvolumeclaims: \"10\"";
        
        System.out.println("Resource quota configuration:");
        System.out.println(resourceQuota);
    }
}

// ç›‘æ§å’Œæ—¥å¿—é…ç½®
public class MonitoringConfiguration {
    public void configureMonitoring(SparkConf conf) {
        // 1. æŒ‡æ ‡æ”¶é›†
        conf.set("spark.metrics.conf", "/opt/spark/conf/metrics.properties");
        conf.set("spark.sql.streaming.metricsEnabled", "true");
        
        // 2. æ—¥å¿—é…ç½®
        conf.set("spark.eventLog.enabled", "true");
        conf.set("spark.eventLog.dir", "hdfs://namenode:9000/spark-events");
        conf.set("spark.history.fs.logDirectory", "hdfs://namenode:9000/spark-events");
        
        // 3. æ€§èƒ½ç›‘æ§
        conf.set("spark.sql.adaptive.enabled", "true");
        conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true");
        conf.set("spark.sql.adaptive.skewJoin.enabled", "true");
        
        // 4. åŠ¨æ€èµ„æºåˆ†é…
        conf.set("spark.dynamicAllocation.enabled", "true");
        conf.set("spark.dynamicAllocation.minExecutors", "2");
        conf.set("spark.dynamicAllocation.maxExecutors", "20");
        conf.set("spark.dynamicAllocation.initialExecutors", "5");
        
        System.out.println("Monitoring and logging configured");
    }
}
```

| éƒ¨ç½²æ¨¡å¼ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|----------|------|----------|
| **Localæ¨¡å¼** | å•æœºè¿è¡Œï¼Œç”¨äºå¼€å‘å’Œæµ‹è¯• | æœ¬åœ°å¼€å‘å’Œè°ƒè¯• |
| **Standaloneæ¨¡å¼** | Sparkè‡ªå¸¦çš„é›†ç¾¤ç®¡ç†å™¨ | å°è§„æ¨¡é›†ç¾¤ |
| **YARNæ¨¡å¼** | ä½¿ç”¨Hadoop YARNç®¡ç†èµ„æº | ç”Ÿäº§ç¯å¢ƒï¼Œä¸Hadoopé›†æˆ |
| **Mesosæ¨¡å¼** | ä½¿ç”¨Apache Mesosç®¡ç†èµ„æº | å¤§è§„æ¨¡é›†ç¾¤ï¼Œå¤šæ¡†æ¶æ”¯æŒ |
| **Kubernetesæ¨¡å¼** | ä½¿ç”¨K8sç®¡ç†å®¹å™¨åŒ–éƒ¨ç½² | äº‘åŸç”Ÿç¯å¢ƒ |

### 7.2 é…ç½®ç¤ºä¾‹

<div className="code-with-callout">

```java title="Sparké…ç½®ç¤ºä¾‹"
public class SparkConfiguration {
    public SparkSession createSparkSession() {
        SparkConf conf = new SparkConf()
            .setAppName("MySparkApp")
            .setMaster("yarn")
            .set("spark.executor.memory", "8g")
            .set("spark.executor.cores", "4")
            .set("spark.driver.memory", "4g")
            .set("spark.sql.adaptive.enabled", "true")
            .set("spark.sql.adaptive.coalescePartitions.enabled", "true")
            .set("spark.sql.adaptive.skewJoin.enabled", "true");
        
        return SparkSession.builder()
            .config(conf)
            .enableHiveSupport()
            .getOrCreate();
    }
}
```

:::info é…ç½®å»ºè®®
æ ¹æ®é›†ç¾¤èµ„æºå’Œåº”ç”¨éœ€æ±‚åˆç†é…ç½®Sparkå‚æ•°ï¼Œç‰¹åˆ«æ˜¯å†…å­˜å’ŒCPUé…ç½®ï¼Œå¯¹æ€§èƒ½å½±å“å¾ˆå¤§ã€‚
:::
</div>

## 8. æœ€ä½³å®è·µ

### 8.1 å¼€å‘æœ€ä½³å®è·µ

<div className="card">
<div className="card__body">
<ol>
<li><strong>åˆç†ä½¿ç”¨ç¼“å­˜</strong>ï¼šå¯¹é‡å¤ä½¿ç”¨çš„RDDè¿›è¡Œç¼“å­˜</li>
<li><strong>é¿å…shuffle</strong>ï¼šå‡å°‘ä¸å¿…è¦çš„shuffleæ“ä½œ</li>
<li><strong>ä½¿ç”¨å¹¿æ’­å˜é‡</strong>ï¼šå‡å°‘æ•°æ®ä¼ è¾“å¼€é”€</li>
<li><strong>åˆç†åˆ†åŒº</strong>ï¼šæ ¹æ®æ•°æ®é‡è®¾ç½®åˆé€‚çš„åˆ†åŒºæ•°</li>
<li><strong>ç›‘æ§æ€§èƒ½</strong>ï¼šä½¿ç”¨Spark UIç›‘æ§åº”ç”¨æ€§èƒ½</li>
</ol>
</div>
</div>

### 8.2 å¸¸è§é—®é¢˜è§£å†³

<Tabs>
  <TabItem value="oom" label="å†…å­˜æº¢å‡º" default>
  ```java
  // è§£å†³å†…å­˜æº¢å‡ºé—®é¢˜
  // 1. å¢åŠ æ‰§è¡Œå™¨å†…å­˜
  .set("spark.executor.memory", "16g")
  
  // 2. ä½¿ç”¨ç£ç›˜å­˜å‚¨
  .set("spark.storage.memoryFraction", "0.3")
  
  // 3. å‡å°‘åˆ†åŒºæ•°
  data.repartition(100)
  ```
  </TabItem>
  <TabItem value="slow" label="æ€§èƒ½é—®é¢˜">
  ```java
  // è§£å†³æ€§èƒ½é—®é¢˜
  // 1. å¯ç”¨è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œ
  .set("spark.sql.adaptive.enabled", "true")
  
  // 2. ä½¿ç”¨åŠ¨æ€åˆ†åŒºè£å‰ª
  .set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")
  
  // 3. å¯ç”¨ä»£ç ç”Ÿæˆ
  .set("spark.sql.codegen.wholeStage", "true")
  ```
  </TabItem>
</Tabs>

## 9. æ€»ç»“

Apache Sparkæ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€æ€§èƒ½ä¼˜å¼‚çš„å¤§æ•°æ®å¤„ç†å¼•æ“ï¼Œå®ƒé€šè¿‡å†…å­˜è®¡ç®—ã€ç»Ÿä¸€å¹³å°å’Œä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿï¼Œä¸ºå¤§æ•°æ®å¤„ç†æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚

### å­¦ä¹ å»ºè®®

1. **ç†è§£æ ¸å¿ƒæ¦‚å¿µ**ï¼šæ·±å…¥ç†è§£RDDã€DataFrameã€Datasetç­‰æ ¸å¿ƒæŠ½è±¡
2. **æŒæ¡ç¼–ç¨‹æ¨¡å‹**ï¼šç†Ÿç»ƒä½¿ç”¨è½¬æ¢æ“ä½œå’ŒåŠ¨ä½œæ“ä½œ
3. **å­¦ä¹ é«˜çº§ç‰¹æ€§**ï¼šæŒæ¡Spark SQLã€Streamingã€MLlibç­‰ç»„ä»¶
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå­¦ä¹ å†…å­˜ç®¡ç†ã€åˆ†åŒºç­–ç•¥ç­‰ä¼˜åŒ–æŠ€æœ¯
5. **å®è·µé¡¹ç›®**ï¼šé€šè¿‡å®é™…é¡¹ç›®ç§¯ç´¯ç»éªŒ

Sparkæ˜¯å¤§æ•°æ®æŠ€æœ¯æ ˆä¸­çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼ŒæŒæ¡å®ƒå°†å¤§å¤§æå‡å¤§æ•°æ®å¤„ç†èƒ½åŠ›ã€‚ 