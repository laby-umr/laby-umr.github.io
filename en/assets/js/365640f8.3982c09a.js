"use strict";(self.webpackChunkfullstack_dev_blog=self.webpackChunkfullstack_dev_blog||[]).push([["24812"],{48453:function(e,n,a){a.r(n),a.d(n,{metadata:()=>r,default:()=>h,frontMatter:()=>d,contentTitle:()=>c,toc:()=>u,assets:()=>p});var r=JSON.parse('{"id":"backend/big-data/spark-technology","title":"Spark\u6280\u672F\u8BE6\u89E3","description":"\u6DF1\u5165\u7406\u89E3Apache Spark\u7684\u6838\u5FC3\u6982\u5FF5\u3001\u67B6\u6784\u8BBE\u8BA1\u3001\u7F16\u7A0B\u6A21\u578B\u548C\u6027\u80FD\u4F18\u5316","source":"@site/docs/backend/big-data/spark-technology.mdx","sourceDirName":"backend/big-data","slug":"/backend/big-data/spark-technology","permalink":"/en/docs/backend/big-data/spark-technology","draft":false,"unlisted":false,"editUrl":"https://github.com/MasterLiu93/blog-web/tree/main/docs/backend/big-data/spark-technology.mdx","tags":[],"version":"current","lastUpdatedBy":"Laby","lastUpdatedAt":1755302400000,"sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Spark\u6280\u672F\u8BE6\u89E3","description":"\u6DF1\u5165\u7406\u89E3Apache Spark\u7684\u6838\u5FC3\u6982\u5FF5\u3001\u67B6\u6784\u8BBE\u8BA1\u3001\u7F16\u7A0B\u6A21\u578B\u548C\u6027\u80FD\u4F18\u5316","authors":["Laby"],"last_update":{"date":"2025-08-16T00:00:00.000Z","author":"Laby"}},"sidebar":"tutorialSidebar","previous":{"title":"Hadoop\u751F\u6001\u7CFB\u7EDF\u8BE6\u89E3","permalink":"/en/docs/backend/big-data/hadoop-ecosystem"},"next":{"title":"\u6570\u636E\u4ED3\u5E93\u4E0EETL\u6280\u672F","permalink":"/en/docs/backend/big-data/data-warehouse-etl"}}'),t=a(74848),s=a(28453),i=a(78010),l=a(57250),o=a(19007);let d={sidebar_position:3,title:"Spark\u6280\u672F\u8BE6\u89E3",description:"\u6DF1\u5165\u7406\u89E3Apache Spark\u7684\u6838\u5FC3\u6982\u5FF5\u3001\u67B6\u6784\u8BBE\u8BA1\u3001\u7F16\u7A0B\u6A21\u578B\u548C\u6027\u80FD\u4F18\u5316",authors:["Laby"],last_update:{date:new Date("2025-08-16T00:00:00.000Z"),author:"Laby"}},c="Spark\u6280\u672F\u8BE6\u89E3",p={},u=[{value:"1. Spark\u6838\u5FC3\u6982\u5FF5",id:"1-spark\u6838\u5FC3\u6982\u5FF5",level:2},{value:"1.1 Spark\u67B6\u6784",id:"11-spark\u67B6\u6784",level:3},{value:"1.2 Spark\u6838\u5FC3\u62BD\u8C61",id:"12-spark\u6838\u5FC3\u62BD\u8C61",level:3},{value:"1.2.1 Spark 3.x\u65B0\u7279\u6027",id:"121-spark-3x\u65B0\u7279\u6027",level:4},{value:"1.2.2 RDD\u4F9D\u8D56\u5173\u7CFB\u7BA1\u7406",id:"122-rdd\u4F9D\u8D56\u5173\u7CFB\u7BA1\u7406",level:4},{value:"RDD\u7279\u6027",id:"rdd\u7279\u6027",level:4},{value:"2. Spark\u7F16\u7A0B\u6A21\u578B",id:"2-spark\u7F16\u7A0B\u6A21\u578B",level:2},{value:"2.1 RDD\u64CD\u4F5C\u7C7B\u578B",id:"21-rdd\u64CD\u4F5C\u7C7B\u578B",level:3},{value:"2.2 \u57FA\u672CRDD\u64CD\u4F5C",id:"22-\u57FA\u672Crdd\u64CD\u4F5C",level:3},{value:"3. Spark SQL\u548CDataFrame",id:"3-spark-sql\u548Cdataframe",level:2},{value:"3.1 DataFrame\u6982\u5FF5",id:"31-dataframe\u6982\u5FF5",level:3},{value:"3.1.1 Catalyst\u4F18\u5316\u5668\u5DE5\u4F5C\u539F\u7406",id:"311-catalyst\u4F18\u5316\u5668\u5DE5\u4F5C\u539F\u7406",level:4},{value:"3.1.2 \u81EA\u9002\u5E94\u67E5\u8BE2\u6267\u884C(AQE)",id:"312-\u81EA\u9002\u5E94\u67E5\u8BE2\u6267\u884Caqe",level:4},{value:"3.2 DataFrame\u64CD\u4F5C\u793A\u4F8B",id:"32-dataframe\u64CD\u4F5C\u793A\u4F8B",level:3},{value:"4. Spark Streaming",id:"4-spark-streaming",level:2},{value:"4.1 \u6D41\u5904\u7406\u67B6\u6784",id:"41-\u6D41\u5904\u7406\u67B6\u6784",level:3},{value:"4.1.1 \u7ED3\u6784\u5316\u6D41\u5904\u7406(Structured Streaming)",id:"411-\u7ED3\u6784\u5316\u6D41\u5904\u7406structured-streaming",level:4},{value:"4.1.2 \u7ED3\u6784\u5316\u6D41\u5904\u7406\u793A\u4F8B",id:"412-\u7ED3\u6784\u5316\u6D41\u5904\u7406\u793A\u4F8B",level:4},{value:"4.2 \u6D41\u5904\u7406\u793A\u4F8B",id:"42-\u6D41\u5904\u7406\u793A\u4F8B",level:3},{value:"5. Spark MLlib\u673A\u5668\u5B66\u4E60",id:"5-spark-mllib\u673A\u5668\u5B66\u4E60",level:2},{value:"5.1 MLlib\u7EC4\u4EF6",id:"51-mllib\u7EC4\u4EF6",level:3},{value:"5.1.1 MLlib\u7B97\u6CD5\u5206\u7C7B",id:"511-mllib\u7B97\u6CD5\u5206\u7C7B",level:4},{value:"5.1.2 \u7279\u5F81\u5DE5\u7A0BPipeline\u793A\u4F8B",id:"512-\u7279\u5F81\u5DE5\u7A0Bpipeline\u793A\u4F8B",level:4},{value:"5.2 \u673A\u5668\u5B66\u4E60\u793A\u4F8B",id:"52-\u673A\u5668\u5B66\u4E60\u793A\u4F8B",level:3},{value:"6. Spark\u6027\u80FD\u4F18\u5316",id:"6-spark\u6027\u80FD\u4F18\u5316",level:2},{value:"6.1 \u5185\u5B58\u7BA1\u7406",id:"61-\u5185\u5B58\u7BA1\u7406",level:3},{value:"6.1.1 \u5185\u5B58\u7BA1\u7406\u67B6\u6784",id:"611-\u5185\u5B58\u7BA1\u7406\u67B6\u6784",level:4},{value:"6.1.2 \u5185\u5B58\u8C03\u4F18\u7B56\u7565",id:"612-\u5185\u5B58\u8C03\u4F18\u7B56\u7565",level:4},{value:"6.2 \u4F18\u5316\u7B56\u7565",id:"62-\u4F18\u5316\u7B56\u7565",level:3},{value:"7. Spark\u90E8\u7F72\u548C\u914D\u7F6E",id:"7-spark\u90E8\u7F72\u548C\u914D\u7F6E",level:2},{value:"7.1 \u90E8\u7F72\u6A21\u5F0F",id:"71-\u90E8\u7F72\u6A21\u5F0F",level:3},{value:"7.1.1 Kubernetes\u90E8\u7F72\u67B6\u6784",id:"711-kubernetes\u90E8\u7F72\u67B6\u6784",level:4},{value:"7.1.2 \u4E91\u539F\u751F\u90E8\u7F72\u914D\u7F6E",id:"712-\u4E91\u539F\u751F\u90E8\u7F72\u914D\u7F6E",level:4},{value:"7.2 \u914D\u7F6E\u793A\u4F8B",id:"72-\u914D\u7F6E\u793A\u4F8B",level:3},{value:"8. \u6700\u4F73\u5B9E\u8DF5",id:"8-\u6700\u4F73\u5B9E\u8DF5",level:2},{value:"8.1 \u5F00\u53D1\u6700\u4F73\u5B9E\u8DF5",id:"81-\u5F00\u53D1\u6700\u4F73\u5B9E\u8DF5",level:3},{value:"8.2 \u5E38\u89C1\u95EE\u9898\u89E3\u51B3",id:"82-\u5E38\u89C1\u95EE\u9898\u89E3\u51B3",level:3},{value:"9. \u603B\u7ED3",id:"9-\u603B\u7ED3",level:2},{value:"\u5B66\u4E60\u5EFA\u8BAE",id:"\u5B66\u4E60\u5EFA\u8BAE",level:3}];function m(e){let n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"spark\u6280\u672F\u8BE6\u89E3",children:"Spark\u6280\u672F\u8BE6\u89E3"})}),"\n",(0,t.jsx)(n.p,{children:"Apache Spark\u662F\u4E00\u4E2A\u5FEB\u901F\u3001\u901A\u7528\u7684\u5927\u89C4\u6A21\u6570\u636E\u5904\u7406\u5F15\u64CE\uFF0C\u5B83\u63D0\u4F9B\u4E86\u5185\u5B58\u8BA1\u7B97\u80FD\u529B\uFF0C\u4F7F\u5F97\u5927\u6570\u636E\u5904\u7406\u901F\u5EA6\u6BD4\u4F20\u7EDFMapReduce\u5FEB10-100\u500D\u3002Spark\u652F\u6301\u6279\u5904\u7406\u3001\u4EA4\u4E92\u5F0F\u67E5\u8BE2\u3001\u6D41\u5904\u7406\u548C\u673A\u5668\u5B66\u4E60\u7B49\u591A\u79CD\u8BA1\u7B97\u8303\u5F0F\u3002"}),"\n",(0,t.jsx)(n.admonition,{title:"\u672C\u6587\u5185\u5BB9\u6982\u89C8",type:"info",children:(0,t.jsx)(o.A,{toc:u})}),"\n",(0,t.jsxs)(n.admonition,{title:"\u6838\u5FC3\u4EF7\u503C",type:"tip",children:[(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Apache Spark = \u5185\u5B58\u8BA1\u7B97 + \u7EDF\u4E00\u5E73\u53F0 + \u9AD8\u6027\u80FD + \u6613\u7528\u6027 + \u751F\u6001\u7CFB\u7EDF"})}),(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u{1F680} ",(0,t.jsx)(n.strong,{children:"\u5185\u5B58\u8BA1\u7B97"}),"\uFF1A\u6570\u636E\u5B58\u50A8\u5728\u5185\u5B58\u4E2D\uFF0C\u8BA1\u7B97\u901F\u5EA6\u5927\u5E45\u63D0\u5347"]}),"\n",(0,t.jsxs)(n.li,{children:["\u{1F468}\u200D\u{1F4BB} ",(0,t.jsx)(n.strong,{children:"\u7EDF\u4E00\u5E73\u53F0"}),"\uFF1A\u652F\u6301\u6279\u5904\u7406\u3001\u6D41\u5904\u7406\u3001\u673A\u5668\u5B66\u4E60\u7B49\u591A\u79CD\u8BA1\u7B97\u6A21\u5F0F"]}),"\n",(0,t.jsxs)(n.li,{children:["\u{1F50D} ",(0,t.jsx)(n.strong,{children:"\u9AD8\u6027\u80FD"}),"\uFF1A\u6BD4MapReduce\u5FEB10-100\u500D\uFF0C\u652F\u6301\u590D\u6742\u7684\u8FED\u4EE3\u7B97\u6CD5"]}),"\n",(0,t.jsxs)(n.li,{children:["\u{1F517} ",(0,t.jsx)(n.strong,{children:"\u6613\u7528\u6027"}),"\uFF1A\u63D0\u4F9BJava\u3001Scala\u3001Python\u3001R\u7B49\u591A\u79CDAPI"]}),"\n",(0,t.jsxs)(n.li,{children:["\u{1F4DA} ",(0,t.jsx)(n.strong,{children:"\u751F\u6001\u7CFB\u7EDF"}),"\uFF1ASpark SQL\u3001Spark Streaming\u3001MLlib\u3001GraphX\u7B49\u7EC4\u4EF6"]}),"\n"]})]}),"\n",(0,t.jsx)(n.h2,{id:"1-spark\u6838\u5FC3\u6982\u5FF5",children:"1. Spark\u6838\u5FC3\u6982\u5FF5"}),"\n",(0,t.jsx)(n.h3,{id:"11-spark\u67B6\u6784",children:"1.1 Spark\u67B6\u6784"}),"\n",(0,t.jsx)(n.p,{children:"Spark\u91C7\u7528\u4E3B\u4ECE\u67B6\u6784\uFF0C\u4E3B\u8981\u5305\u542B\u4EE5\u4E0B\u7EC4\u4EF6\uFF1A"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[Spark\u96C6\u7FA4] --\x3e B[Driver Program]\n    A --\x3e C[Cluster Manager]\n    A --\x3e D[Worker Node]\n    \n    B --\x3e B1["\u5E94\u7528\u903B\u8F91"]\n    B --\x3e B2["\u4EFB\u52A1\u8C03\u5EA6"]\n    \n    C --\x3e C1["\u8D44\u6E90\u5206\u914D"]\n    C --\x3e C2["\u4EFB\u52A1\u5206\u53D1"]\n    \n    D --\x3e D1[Executor]\n    D --\x3e D2[Task]\n    \n    D1 --\x3e D3["\u5185\u5B58\u7BA1\u7406"]\n    D1 --\x3e D4["\u4EFB\u52A1\u6267\u884C"]'}),"\n",(0,t.jsx)(n.h3,{id:"12-spark\u6838\u5FC3\u62BD\u8C61",children:"1.2 Spark\u6838\u5FC3\u62BD\u8C61"}),"\n",(0,t.jsxs)("div",{className:"card",children:[(0,t.jsx)("div",{className:"card__header",children:(0,t.jsx)("h4",{children:"Spark\u6838\u5FC3\u62BD\u8C61"})}),(0,t.jsx)("div",{className:"card__body",children:(0,t.jsxs)("ol",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"RDD"}),"\uFF1A\u5F39\u6027\u5206\u5E03\u5F0F\u6570\u636E\u96C6\uFF0C\u4E0D\u53EF\u53D8\u7684\u5206\u5E03\u5F0F\u5BF9\u8C61\u96C6\u5408"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"DataFrame"}),"\uFF1A\u57FA\u4E8ERDD\u7684\u5206\u5E03\u5F0F\u6570\u636E\u8868\uFF0C\u7C7B\u4F3C\u5173\u7CFB\u578B\u6570\u636E\u5E93\u8868"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"Dataset"}),"\uFF1A\u7C7B\u578B\u5B89\u5168\u7684DataFrame\uFF0C\u7ED3\u5408\u4E86RDD\u548CDataFrame\u7684\u4F18\u70B9"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"SparkContext"}),"\uFF1ASpark\u5E94\u7528\u7684\u5165\u53E3\u70B9\uFF0C\u8D1F\u8D23\u4E0E\u96C6\u7FA4\u901A\u4FE1"]})]})})]}),"\n",(0,t.jsx)(n.h4,{id:"121-spark-3x\u65B0\u7279\u6027",children:"1.2.1 Spark 3.x\u65B0\u7279\u6027"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[Spark 3.x\u65B0\u7279\u6027] --\x3e B[\u81EA\u9002\u5E94\u67E5\u8BE2\u6267\u884C]\n    A --\x3e C[\u52A8\u6001\u5206\u533A\u88C1\u526A]\n    A --\x3e D[GPU\u52A0\u901F]\n    A --\x3e E[Kubernetes\u539F\u751F\u652F\u6301]\n    \n    B --\x3e B1["AQE\u4F18\u5316\u5668"]\n    B --\x3e B2["\u52A8\u6001\u5408\u5E76\u5206\u533A"]\n    \n    C --\x3e C1["\u5206\u533A\u88C1\u526A\u4F18\u5316"]\n    C --\x3e C2["\u51CF\u5C11\u6570\u636E\u626B\u63CF"]\n    \n    D --\x3e D1["GPU\u5185\u5B58\u7BA1\u7406"]\n    D --\x3e D2["\u6DF1\u5EA6\u5B66\u4E60\u52A0\u901F"]\n    \n    E --\x3e E1["K8s\u8C03\u5EA6\u5668"]\n    E --\x3e E2["\u5BB9\u5668\u5316\u90E8\u7F72"]'}),"\n",(0,t.jsx)(n.h4,{id:"122-rdd\u4F9D\u8D56\u5173\u7CFB\u7BA1\u7406",children:"1.2.2 RDD\u4F9D\u8D56\u5173\u7CFB\u7BA1\u7406"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="RDD\u4F9D\u8D56\u5173\u7CFB\u793A\u4F8B"',children:'public class RDDDependencyExample {\n    public void demonstrateDependencies(JavaSparkContext sc) {\n        // 1. \u7A84\u4F9D\u8D56 - \u4E00\u5BF9\u4E00\u4F9D\u8D56\n        JavaRDD<String> lines = sc.textFile("input.txt");\n        JavaRDD<Integer> lengths = lines.map(String::length); // \u7A84\u4F9D\u8D56\n        \n        // 2. \u5BBD\u4F9D\u8D56 - Shuffle\u4F9D\u8D56\n        JavaPairRDD<String, Integer> pairs = lines.mapToPair(line -> \n            new Tuple2<>(line.split(" ")[0], 1));\n        JavaPairRDD<String, Integer> counts = pairs.reduceByKey((a, b) -> a + b); // \u5BBD\u4F9D\u8D56\n        \n        // 3. \u68C0\u67E5\u4F9D\u8D56\u7C7B\u578B\n        System.out.println("Lines RDD dependencies: " + lines.dependencies());\n        System.out.println("Lengths RDD dependencies: " + lengths.dependencies());\n        System.out.println("Counts RDD dependencies: " + counts.dependencies());\n        \n        // 4. \u4F18\u5316\u5EFA\u8BAE\n        if (hasWideDependency(counts)) {\n            System.out.println("Warning: Wide dependency detected. Consider repartitioning.");\n        }\n    }\n    \n    private boolean hasWideDependency(JavaPairRDD<String, Integer> rdd) {\n        return rdd.dependencies().stream()\n            .anyMatch(dep -> dep instanceof ShuffleDependency);\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h4,{id:"rdd\u7279\u6027",children:"RDD\u7279\u6027"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="RDD\u7279\u6027\u793A\u4F8B"',children:'public class RDDFeatures {\n    public static void main(String[] args) {\n        // 1. \u5F39\u6027\uFF08Resilient\uFF09\n        System.out.println("RDD\u5177\u6709\u5BB9\u9519\u80FD\u529B\uFF0C\u53EF\u4EE5\u4ECE\u5931\u8D25\u4E2D\u6062\u590D");\n        \n        // 2. \u5206\u5E03\u5F0F\uFF08Distributed\uFF09\n        System.out.println("RDD\u6570\u636E\u5206\u5E03\u5728\u96C6\u7FA4\u7684\u591A\u4E2A\u8282\u70B9\u4E0A");\n        \n        // 3. \u6570\u636E\u96C6\uFF08Dataset\uFF09\n        System.out.println("RDD\u662F\u6570\u636E\u96C6\u5408\uFF0C\u652F\u6301\u591A\u79CD\u6570\u636E\u7C7B\u578B");\n        \n        // 4. \u4E0D\u53EF\u53D8\u6027\uFF08Immutable\uFF09\n        System.out.println("RDD\u4E00\u65E6\u521B\u5EFA\u5C31\u4E0D\u80FD\u4FEE\u6539\uFF0C\u53EA\u80FD\u901A\u8FC7\u8F6C\u6362\u751F\u6210\u65B0\u7684RDD");\n        \n        // 5. \u5EF6\u8FDF\u8BA1\u7B97\uFF08Lazy Evaluation\uFF09\n        System.out.println("RDD\u8F6C\u6362\u64CD\u4F5C\u662F\u5EF6\u8FDF\u7684\uFF0C\u53EA\u6709\u9047\u5230\u52A8\u4F5C\u64CD\u4F5C\u65F6\u624D\u6267\u884C");\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"2-spark\u7F16\u7A0B\u6A21\u578B",children:"2. Spark\u7F16\u7A0B\u6A21\u578B"}),"\n",(0,t.jsx)(n.h3,{id:"21-rdd\u64CD\u4F5C\u7C7B\u578B",children:"2.1 RDD\u64CD\u4F5C\u7C7B\u578B"}),"\n",(0,t.jsx)(n.p,{children:"Spark RDD\u652F\u6301\u4E24\u79CD\u7C7B\u578B\u7684\u64CD\u4F5C\uFF1A"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph LR\n    A[RDD\u64CD\u4F5C] --\x3e B[\u8F6C\u6362\u64CD\u4F5C]\n    A --\x3e C[\u52A8\u4F5C\u64CD\u4F5C]\n    \n    B --\x3e B1["map, filter, flatMap"]\n    B --\x3e B2["groupBy, reduceByKey"]\n    B --\x3e B3["\u8FD4\u56DE\u65B0\u7684RDD"]\n    \n    C --\x3e C1["collect, count, save"]\n    C --\x3e C2["\u89E6\u53D1\u8BA1\u7B97\u6267\u884C"]\n    C --\x3e C3["\u8FD4\u56DE\u7ED3\u679C\u6216\u4FDD\u5B58\u6570\u636E"]'}),"\n",(0,t.jsx)(n.h3,{id:"22-\u57FA\u672Crdd\u64CD\u4F5C",children:"2.2 \u57FA\u672CRDD\u64CD\u4F5C"}),"\n",(0,t.jsxs)(i.A,{children:[(0,t.jsx)(l.A,{value:"transformations",label:"\u8F6C\u6362\u64CD\u4F5C",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="RDD\u8F6C\u6362\u64CD\u4F5C\u793A\u4F8B"',children:'public class RDDTransformations {\n    public void demonstrateTransformations(JavaRDD<String> lines) {\n        // 1. map - \u4E00\u5BF9\u4E00\u8F6C\u6362\n        JavaRDD<Integer> lengths = lines.map(String::length);\n        \n        // 2. filter - \u8FC7\u6EE4\u6570\u636E\n        JavaRDD<String> longLines = lines.filter(line -> line.length() > 100);\n        \n        // 3. flatMap - \u4E00\u5BF9\u591A\u8F6C\u6362\n        JavaRDD<String> words = lines.flatMap(line -> \n            Arrays.asList(line.split(" ")).iterator());\n        \n        // 4. distinct - \u53BB\u91CD\n        JavaRDD<String> uniqueWords = words.distinct();\n        \n        // 5. sample - \u91C7\u6837\n        JavaRDD<String> sampledLines = lines.sample(false, 0.1);\n    }\n}\n'})})}),(0,t.jsx)(l.A,{value:"actions",label:"\u52A8\u4F5C\u64CD\u4F5C",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="RDD\u52A8\u4F5C\u64CD\u4F5C\u793A\u4F8B"',children:'public class RDDActions {\n    public void demonstrateActions(JavaRDD<String> lines) {\n        // 1. collect - \u6536\u96C6\u6240\u6709\u6570\u636E\u5230Driver\n        List<String> allLines = lines.collect();\n        \n        // 2. count - \u8BA1\u7B97\u5143\u7D20\u4E2A\u6570\n        long lineCount = lines.count();\n        \n        // 3. take - \u53D6\u524DN\u4E2A\u5143\u7D20\n        List<String> firstLines = lines.take(10);\n        \n        // 4. reduce - \u5F52\u7EA6\u64CD\u4F5C\n        String longestLine = lines.reduce((a, b) -> \n            a.length() > b.length() ? a : b);\n        \n        // 5. foreach - \u5BF9\u6BCF\u4E2A\u5143\u7D20\u6267\u884C\u64CD\u4F5C\n        lines.foreach(line -> System.out.println("Processing: " + line));\n    }\n}\n'})})}),(0,t.jsx)(l.A,{value:"keyvalue",label:"\u952E\u503C\u5BF9\u64CD\u4F5C",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="\u952E\u503C\u5BF9RDD\u64CD\u4F5C\u793A\u4F8B"',children:"public class KeyValueRDDOperations {\n    public void demonstrateKeyValueOperations(JavaPairRDD<String, Integer> pairs) {\n        // 1. reduceByKey - \u6309\u952E\u5F52\u7EA6\n        JavaPairRDD<String, Integer> sums = pairs.reduceByKey((a, b) -> a + b);\n        \n        // 2. groupByKey - \u6309\u952E\u5206\u7EC4\n        JavaPairRDD<String, Iterable<Integer>> groups = pairs.groupByKey();\n        \n        // 3. sortByKey - \u6309\u952E\u6392\u5E8F\n        JavaPairRDD<String, Integer> sorted = pairs.sortByKey();\n        \n        // 4. join - \u8FDE\u63A5\u64CD\u4F5C\n        JavaPairRDD<String, Tuple2<Integer, String>> joined = \n            pairs.join(otherPairs);\n        \n        // 5. cogroup - \u534F\u540C\u5206\u7EC4\n        JavaPairRDD<String, Tuple2<Iterable<Integer>, Iterable<String>>> cogrouped = \n            pairs.cogroup(otherPairs);\n    }\n}\n"})})})]}),"\n",(0,t.jsx)(n.h2,{id:"3-spark-sql\u548Cdataframe",children:"3. Spark SQL\u548CDataFrame"}),"\n",(0,t.jsx)(n.h3,{id:"31-dataframe\u6982\u5FF5",children:"3.1 DataFrame\u6982\u5FF5"}),"\n",(0,t.jsx)(n.p,{children:"DataFrame\u662FSpark\u4E2D\u5904\u7406\u7ED3\u6784\u5316\u6570\u636E\u7684\u6838\u5FC3\u62BD\u8C61\uFF1A"}),"\n",(0,t.jsx)(n.h4,{id:"311-catalyst\u4F18\u5316\u5668\u5DE5\u4F5C\u539F\u7406",children:"3.1.1 Catalyst\u4F18\u5316\u5668\u5DE5\u4F5C\u539F\u7406"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[SQL\u67E5\u8BE2] --\x3e B[\u89E3\u6790\u9636\u6BB5]\n    B --\x3e C[\u5206\u6790\u9636\u6BB5]\n    C --\x3e D[\u4F18\u5316\u9636\u6BB5]\n    D --\x3e E[\u7269\u7406\u8BA1\u5212]\n    E --\x3e F[\u4EE3\u7801\u751F\u6210]\n    \n    B --\x3e B1["\u8BED\u6CD5\u6811\u6784\u5EFA"]\n    C --\x3e C1["\u8BED\u4E49\u5206\u6790"]\n    C --\x3e C2["\u7C7B\u578B\u68C0\u67E5"]\n    \n    D --\x3e D1["\u5E38\u91CF\u6298\u53E0"]\n    D --\x3e D2["\u8C13\u8BCD\u4E0B\u63A8"]\n    D --\x3e D3["\u5217\u88C1\u526A"]\n    D --\x3e D4["\u5206\u533A\u88C1\u526A"]\n    \n    E --\x3e E1["\u7269\u7406\u7B97\u5B50\u9009\u62E9"]\n    E --\x3e E2["\u5E76\u884C\u5EA6\u8BBE\u7F6E"]\n    \n    F --\x3e F1["Java\u5B57\u8282\u7801\u751F\u6210"]\n    F --\x3e F2["\u6027\u80FD\u4F18\u5316"]'}),"\n",(0,t.jsx)(n.h4,{id:"312-\u81EA\u9002\u5E94\u67E5\u8BE2\u6267\u884Caqe",children:"3.1.2 \u81EA\u9002\u5E94\u67E5\u8BE2\u6267\u884C(AQE)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="AQE\u4F18\u5316\u793A\u4F8B"',children:'public class AQEExample {\n    public void demonstrateAQE(SparkSession spark) {\n        // \u542F\u7528AQE\n        spark.conf().set("spark.sql.adaptive.enabled", "true");\n        spark.conf().set("spark.sql.adaptive.coalescePartitions.enabled", "true");\n        spark.conf().set("spark.sql.adaptive.skewJoin.enabled", "true");\n        spark.conf().set("spark.sql.adaptive.localShuffleReader.enabled", "true");\n        \n        // \u521B\u5EFA\u6D4B\u8BD5\u6570\u636E\n        Dataset<Row> users = spark.createDataFrame(Arrays.asList(\n            RowFactory.create("user1", "Alice", 25, "Engineer"),\n            RowFactory.create("user2", "Bob", 30, "Manager"),\n            RowFactory.create("user3", "Charlie", 35, "Director")\n        ), new StructType()\n            .add("id", DataTypes.StringType)\n            .add("name", DataTypes.StringType)\n            .add("age", DataTypes.IntegerType)\n            .add("job", DataTypes.StringType));\n        \n        Dataset<Row> orders = spark.createDataFrame(Arrays.asList(\n            RowFactory.create("order1", "user1", 100.0),\n            RowFactory.create("order2", "user2", 200.0),\n            RowFactory.create("order3", "user1", 150.0)\n        ), new StructType()\n            .add("orderId", DataTypes.StringType)\n            .add("userId", DataTypes.StringType)\n            .add("amount", DataTypes.DoubleType));\n        \n        // \u590D\u6742\u67E5\u8BE2 - AQE\u4F1A\u81EA\u52A8\u4F18\u5316\n        Dataset<Row> result = users.join(orders, users.col("id").equalTo(orders.col("userId")))\n            .groupBy("job")\n            .agg(functions.avg("amount").as("avg_amount"))\n            .filter(col("avg_amount").gt(100));\n        \n        result.explain(true); // \u663E\u793A\u4F18\u5316\u540E\u7684\u6267\u884C\u8BA1\u5212\n        result.show();\n    }\n}\n'})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[DataFrame] --\x3e B[\u7ED3\u6784\u5316\u6570\u636E]\n    A --\x3e C[Schema\u4FE1\u606F]\n    A --\x3e D[\u4F18\u5316\u6267\u884C]\n    \n    B --\x3e B1["\u884C\u548C\u5217\u7EC4\u7EC7"]\n    B --\x3e B2["\u7C7B\u578B\u5B89\u5168"]\n    \n    C --\x3e C1["\u5217\u540D\u548C\u7C7B\u578B"]\n    C --\x3e C2["\u5143\u6570\u636E\u7BA1\u7406"]\n    \n    D --\x3e D1["Catalyst\u4F18\u5316\u5668"]\n    D --\x3e D2["\u4EE3\u7801\u751F\u6210"]'}),"\n",(0,t.jsx)(n.h3,{id:"32-dataframe\u64CD\u4F5C\u793A\u4F8B",children:"3.2 DataFrame\u64CD\u4F5C\u793A\u4F8B"}),"\n",(0,t.jsxs)("div",{className:"code-with-callout",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="DataFrame\u64CD\u4F5C\u793A\u4F8B"',children:'public class DataFrameOperations {\n    public void demonstrateDataFrameOperations(SparkSession spark) {\n        // 1. \u521B\u5EFADataFrame\n        List<Row> data = Arrays.asList(\n            RowFactory.create("Alice", 25, "Engineer"),\n            RowFactory.create("Bob", 30, "Manager"),\n            RowFactory.create("Charlie", 35, "Director")\n        );\n        \n        StructType schema = new StructType()\n            .add("name", DataTypes.StringType)\n            .add("age", DataTypes.IntegerType)\n            .add("job", DataTypes.StringType);\n        \n        Dataset<Row> df = spark.createDataFrame(data, schema);\n        \n        // 2. \u663E\u793A\u6570\u636E\n        df.show();\n        \n        // 3. \u8FC7\u6EE4\u6570\u636E\n        Dataset<Row> youngPeople = df.filter(col("age").lt(30));\n        \n        // 4. \u9009\u62E9\u5217\n        Dataset<Row> namesAndAges = df.select("name", "age");\n        \n        // 5. \u5206\u7EC4\u805A\u5408\n        Dataset<Row> jobCounts = df.groupBy("job").count();\n        \n        // 6. SQL\u67E5\u8BE2\n        df.createOrReplaceTempView("people");\n        Dataset<Row> sqlResult = spark.sql(\n            "SELECT job, AVG(age) as avg_age FROM people GROUP BY job"\n        );\n    }\n}\n'})}),(0,t.jsx)(n.admonition,{title:"DataFrame\u4F18\u52BF",type:"info",children:(0,t.jsx)(n.p,{children:"DataFrame\u63D0\u4F9B\u4E86\u7C7B\u4F3CSQL\u7684\u67E5\u8BE2\u63A5\u53E3\uFF0C\u652F\u6301\u4F18\u5316\u6267\u884C\uFF0C\u6BD4RDD\u64CD\u4F5C\u6027\u80FD\u66F4\u597D\uFF0C\u7279\u522B\u9002\u5408\u7ED3\u6784\u5316\u6570\u636E\u5904\u7406\u3002"})})]}),"\n",(0,t.jsx)(n.h2,{id:"4-spark-streaming",children:"4. Spark Streaming"}),"\n",(0,t.jsx)(n.h3,{id:"41-\u6D41\u5904\u7406\u67B6\u6784",children:"4.1 \u6D41\u5904\u7406\u67B6\u6784"}),"\n",(0,t.jsx)(n.p,{children:"Spark Streaming\u5C06\u6D41\u5F0F\u8BA1\u7B97\u5206\u89E3\u4E3A\u4E00\u7CFB\u5217\u5C0F\u6279\u91CF\u7684\u6279\u5904\u7406\u4F5C\u4E1A\uFF1A"}),"\n",(0,t.jsx)(n.h4,{id:"411-\u7ED3\u6784\u5316\u6D41\u5904\u7406structured-streaming",children:"4.1.1 \u7ED3\u6784\u5316\u6D41\u5904\u7406(Structured Streaming)"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[\u6570\u636E\u6E90] --\x3e B[\u7ED3\u6784\u5316\u6D41]\n    B --\x3e C[\u67E5\u8BE2\u5904\u7406]\n    C --\x3e D[\u7ED3\u679C\u8F93\u51FA]\n    \n    A --\x3e A1["Kafka"]\n    A --\x3e A2["\u6587\u4EF6\u6D41"]\n    A --\x3e A3["Socket"]\n    \n    B --\x3e B1["\u65E0\u9650DataFrame"]\n    B --\x3e B2["\u4E8B\u4EF6\u65F6\u95F4\u5904\u7406"]\n    \n    C --\x3e C1["\u7A97\u53E3\u805A\u5408"]\n    C --\x3e C2["\u6C34\u5370\u5904\u7406"]\n    C --\x3e C3["\u72B6\u6001\u7BA1\u7406"]\n    \n    D --\x3e D1["\u6587\u4EF6\u8F93\u51FA"]\n    D --\x3e D2["\u6570\u636E\u5E93\u5199\u5165"]\n    D --\x3e D3["\u63A7\u5236\u53F0\u8F93\u51FA"]'}),"\n",(0,t.jsx)(n.h4,{id:"412-\u7ED3\u6784\u5316\u6D41\u5904\u7406\u793A\u4F8B",children:"4.1.2 \u7ED3\u6784\u5316\u6D41\u5904\u7406\u793A\u4F8B"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="\u7ED3\u6784\u5316\u6D41\u5904\u7406\u793A\u4F8B"',children:'public class StructuredStreamingExample {\n    public void buildStructuredStreaming(SparkSession spark) {\n        // 1. \u4ECEKafka\u8BFB\u53D6\u6D41\u6570\u636E\n        Dataset<Row> streamDF = spark\n            .readStream()\n            .format("kafka")\n            .option("kafka.bootstrap.servers", "localhost:9092")\n            .option("subscribe", "user-events")\n            .option("startingOffsets", "latest")\n            .load();\n        \n        // 2. \u89E3\u6790JSON\u6570\u636E\n        Dataset<Row> parsedDF = streamDF\n            .selectExpr("CAST(value AS STRING) as json")\n            .select(functions.from_json(col("json"), getUserEventSchema()).as("data"))\n            .select("data.*");\n        \n        // 3. \u4E8B\u4EF6\u65F6\u95F4\u5904\u7406\u548C\u6C34\u5370\n        Dataset<Row> withWatermark = parsedDF\n            .withWatermark("timestamp", "10 minutes")\n            .groupBy(\n                functions.window(col("timestamp"), "5 minutes"),\n                col("userId")\n            )\n            .agg(\n                functions.count("*").as("event_count"),\n                functions.avg("amount").as("avg_amount")\n            );\n        \n        // 4. \u8F93\u51FA\u5230\u63A7\u5236\u53F0\n        StreamingQuery query = withWatermark\n            .writeStream()\n            .outputMode("append")\n            .format("console")\n            .option("truncate", false)\n            .start();\n        \n        // 5. \u7B49\u5F85\u67E5\u8BE2\u7EC8\u6B62\n        query.awaitTermination();\n    }\n    \n    private StructType getUserEventSchema() {\n        return new StructType()\n            .add("userId", DataTypes.StringType)\n            .add("eventType", DataTypes.StringType)\n            .add("amount", DataTypes.DoubleType)\n            .add("timestamp", DataTypes.TimestampType);\n    }\n}\n\n// \u6709\u72B6\u6001\u6D41\u5904\u7406\u793A\u4F8B\npublic class StatefulStreamingExample {\n    public void buildStatefulStreaming(SparkSession spark) {\n        // 1. \u4ECEKafka\u8BFB\u53D6\u7528\u6237\u884C\u4E3A\u6D41\n        Dataset<Row> userBehaviorStream = spark\n            .readStream()\n            .format("kafka")\n            .option("kafka.bootstrap.servers", "localhost:9092")\n            .option("subscribe", "user-behavior")\n            .load();\n        \n        // 2. \u89E3\u6790\u7528\u6237\u884C\u4E3A\n        Dataset<Row> behaviorDF = userBehaviorStream\n            .selectExpr("CAST(value AS STRING) as json")\n            .select(functions.from_json(col("json"), getBehaviorSchema()).as("data"))\n            .select("data.*");\n        \n        // 3. \u6709\u72B6\u6001\u805A\u5408 - \u7528\u6237\u4F1A\u8BDD\u7EDF\u8BA1\n        Dataset<Row> sessionStats = behaviorDF\n            .withWatermark("timestamp", "1 hour")\n            .groupByKey((MapFunction<Row, String>) row -> row.getAs("userId"), Encoders.STRING())\n            .flatMapGroupsWithState(\n                new UserSessionAggregator(),\n                OutputMode.Append(),\n                Encoders.bean(UserSession.class),\n                Encoders.bean(UserSession.class),\n                GroupStateTimeout.ProcessingTimeTimeout()\n            );\n        \n        // 4. \u8F93\u51FA\u7ED3\u679C\n        StreamingQuery query = sessionStats\n            .writeStream()\n            .outputMode("append")\n            .format("console")\n            .start();\n        \n        query.awaitTermination();\n    }\n}\n\n// \u7528\u6237\u4F1A\u8BDD\u805A\u5408\u5668\nclass UserSessionAggregator implements FlatMapGroupsWithStateFunction<String, Row, UserSession, UserSession> {\n    @Override\n    public Iterator<UserSession> call(String userId, Iterator<Row> events, GroupState<UserSession> state) {\n        List<UserSession> results = new ArrayList<>();\n        \n        // \u83B7\u53D6\u5F53\u524D\u72B6\u6001\n        UserSession currentSession = state.exists() ? state.get() : new UserSession(userId);\n        \n        // \u5904\u7406\u4E8B\u4EF6\n        while (events.hasNext()) {\n            Row event = events.next();\n            currentSession.updateSession(event);\n        }\n        \n        // \u66F4\u65B0\u72B6\u6001\n        state.update(currentSession);\n        \n        // \u5982\u679C\u4F1A\u8BDD\u5B8C\u6210\uFF0C\u8F93\u51FA\u7ED3\u679C\n        if (currentSession.isCompleted()) {\n            results.add(currentSession);\n            state.remove();\n        }\n        \n        return results.iterator();\n    }\n}\n'})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph LR\n    A[\u6570\u636E\u6D41] --\x3e B[\u6570\u636E\u63A5\u6536\u5668]\n    B --\x3e C[\u5FAE\u6279\u5904\u7406]\n    C --\x3e D[RDD\u8F6C\u6362]\n    D --\x3e E[\u7ED3\u679C\u8F93\u51FA]\n    \n    B --\x3e B1["Kafka/Flume\u7B49"]\n    C --\x3e C1["\u65F6\u95F4\u7A97\u53E3"]\n    D --\x3e D1["RDD\u64CD\u4F5C"]\n    E --\x3e E1["\u6587\u4EF6/\u6570\u636E\u5E93"]'}),"\n",(0,t.jsx)(n.h3,{id:"42-\u6D41\u5904\u7406\u793A\u4F8B",children:"4.2 \u6D41\u5904\u7406\u793A\u4F8B"}),"\n",(0,t.jsxs)(i.A,{children:[(0,t.jsx)(l.A,{value:"basic",label:"\u57FA\u7840\u6D41\u5904\u7406",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="\u57FA\u7840\u6D41\u5904\u7406\u793A\u4F8B"',children:'public class BasicStreaming {\n    public void processStream(SparkSession spark) {\n        // \u521B\u5EFAStreamingContext\n        JavaStreamingContext ssc = new JavaStreamingContext(\n            spark.sparkContext(), Durations.seconds(5));\n        \n        // \u521B\u5EFADStream\n        JavaReceiverInputDStream<String> lines = ssc.socketTextStream(\n            "localhost", 9999);\n        \n        // \u5904\u7406\u6570\u636E\u6D41\n        JavaDStream<String> words = lines.flatMap(line -> \n            Arrays.asList(line.split(" ")).iterator());\n        \n        JavaPairDStream<String, Integer> wordCounts = words\n            .mapToPair(word -> new Tuple2<>(word, 1))\n            .reduceByKey((a, b) -> a + b);\n        \n        // \u8F93\u51FA\u7ED3\u679C\n        wordCounts.print();\n        \n        // \u542F\u52A8\u6D41\u5904\u7406\n        ssc.start();\n        ssc.awaitTermination();\n    }\n}\n'})})}),(0,t.jsx)(l.A,{value:"kafka",label:"Kafka\u96C6\u6210",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="Kafka\u6D41\u5904\u7406\u793A\u4F8B"',children:'public class KafkaStreaming {\n    public void processKafkaStream(SparkSession spark) {\n        JavaStreamingContext ssc = new JavaStreamingContext(\n            spark.sparkContext(), Durations.seconds(5));\n        \n        // Kafka\u914D\u7F6E\n        Map<String, Object> kafkaParams = new HashMap<>();\n        kafkaParams.put("bootstrap.servers", "localhost:9092");\n        kafkaParams.put("key.deserializer", StringDeserializer.class);\n        kafkaParams.put("value.deserializer", StringDeserializer.class);\n        kafkaParams.put("group.id", "spark-streaming-group");\n        kafkaParams.put("auto.offset.reset", "latest");\n        \n        // \u521B\u5EFAKafka DStream\n        JavaInputDStream<ConsumerRecord<String, String>> stream = \n            KafkaUtils.createDirectStream(ssc, \n                LocationStrategies.PreferConsistent(),\n                ConsumerStrategies.Subscribe(\n                    Arrays.asList("input-topic"), kafkaParams));\n        \n        // \u5904\u7406\u6D88\u606F\n        JavaDStream<String> lines = stream.map(record -> record.value());\n        \n        // \u8BCD\u9891\u7EDF\u8BA1\n        JavaPairDStream<String, Integer> wordCounts = lines\n            .flatMap(line -> Arrays.asList(line.split(" ")).iterator())\n            .mapToPair(word -> new Tuple2<>(word, 1))\n            .reduceByKey((a, b) -> a + b);\n        \n        wordCounts.print();\n        \n        ssc.start();\n        ssc.awaitTermination();\n    }\n}\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"5-spark-mllib\u673A\u5668\u5B66\u4E60",children:"5. Spark MLlib\u673A\u5668\u5B66\u4E60"}),"\n",(0,t.jsx)(n.h3,{id:"51-mllib\u7EC4\u4EF6",children:"5.1 MLlib\u7EC4\u4EF6"}),"\n",(0,t.jsx)(n.p,{children:"MLlib\u662FSpark\u7684\u673A\u5668\u5B66\u4E60\u5E93\uFF0C\u63D0\u4F9B\u4E86\u4E30\u5BCC\u7684\u7B97\u6CD5\u548C\u5DE5\u5177\uFF1A"}),"\n",(0,t.jsx)(n.h4,{id:"511-mllib\u7B97\u6CD5\u5206\u7C7B",children:"5.1.1 MLlib\u7B97\u6CD5\u5206\u7C7B"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[MLlib\u7B97\u6CD5] --\x3e B[\u5206\u7C7B\u7B97\u6CD5]\n    A --\x3e C[\u56DE\u5F52\u7B97\u6CD5]\n    A --\x3e D[\u805A\u7C7B\u7B97\u6CD5]\n    A --\x3e E[\u63A8\u8350\u7B97\u6CD5]\n    A --\x3e F[\u7279\u5F81\u5DE5\u7A0B]\n    \n    B --\x3e B1["\u903B\u8F91\u56DE\u5F52"]\n    B --\x3e B2["\u51B3\u7B56\u6811"]\n    B --\x3e B3["\u968F\u673A\u68EE\u6797"]\n    B --\x3e B4["SVM"]\n    \n    C --\x3e C1["\u7EBF\u6027\u56DE\u5F52"]\n    C --\x3e C2["\u5CAD\u56DE\u5F52"]\n    C --\x3e C3["Lasso\u56DE\u5F52"]\n    \n    D --\x3e D1["K-means"]\n    D --\x3e D2["LDA"]\n    D --\x3e D3["\u9AD8\u65AF\u6DF7\u5408"]\n    \n    E --\x3e E1["ALS"]\n    E --\x3e E2["\u534F\u540C\u8FC7\u6EE4"]\n    \n    F --\x3e F1["TF-IDF"]\n    F --\x3e F2["Word2Vec"]\n    F --\x3e F3["\u7279\u5F81\u9009\u62E9"]'}),"\n",(0,t.jsx)(n.h4,{id:"512-\u7279\u5F81\u5DE5\u7A0Bpipeline\u793A\u4F8B",children:"5.1.2 \u7279\u5F81\u5DE5\u7A0BPipeline\u793A\u4F8B"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="\u7279\u5F81\u5DE5\u7A0BPipeline\u793A\u4F8B"',children:'public class FeatureEngineeringPipeline {\n    public PipelineModel buildFeaturePipeline(SparkSession spark) {\n        // 1. \u5B57\u7B26\u4E32\u7D22\u5F15\u5316\n        StringIndexer stringIndexer = new StringIndexer()\n            .setInputCol("category")\n            .setOutputCol("categoryIndex")\n            .setHandleInvalid("skip");\n        \n        // 2. \u72EC\u70ED\u7F16\u7801\n        OneHotEncoder oneHotEncoder = new OneHotEncoder()\n            .setInputCol("categoryIndex")\n            .setOutputCol("categoryOneHot");\n        \n        // 3. \u6570\u503C\u7279\u5F81\u6807\u51C6\u5316\n        StandardScaler standardScaler = new StandardScaler()\n            .setInputCol("numericalFeatures")\n            .setOutputCol("scaledFeatures")\n            .setWithStd(true)\n            .setWithMean(true);\n        \n        // 4. \u7279\u5F81\u5411\u91CF\u7EC4\u88C5\n        VectorAssembler assembler = new VectorAssembler()\n            .setInputCols(new String[]{"categoryOneHot", "scaledFeatures", "otherFeatures"})\n            .setOutputCol("features");\n        \n        // 5. \u7279\u5F81\u9009\u62E9\n        ChiSqSelector featureSelector = new ChiSqSelector()\n            .setNumTopFeatures(20)\n            .setFeaturesCol("features")\n            .setLabelCol("label")\n            .setOutputCol("selectedFeatures");\n        \n        // 6. \u6784\u5EFAPipeline\n        Pipeline pipeline = new Pipeline()\n            .setStages(new PipelineStage[]{\n                stringIndexer, oneHotEncoder, standardScaler, assembler, featureSelector\n            });\n        \n        return pipeline;\n    }\n}\n\n// \u5B8C\u6574\u7684\u673A\u5668\u5B66\u4E60Pipeline\u793A\u4F8B\npublic class CompleteMLPipeline {\n    public void buildCompletePipeline(SparkSession spark, Dataset<Row> trainingData) {\n        // 1. \u7279\u5F81\u5DE5\u7A0BPipeline\n        FeatureEngineeringPipeline featurePipeline = new FeatureEngineeringPipeline();\n        PipelineModel featureModel = featurePipeline.buildFeaturePipeline(spark);\n        \n        // 2. \u673A\u5668\u5B66\u4E60\u7B97\u6CD5\n        RandomForestClassifier classifier = new RandomForestClassifier()\n            .setLabelCol("label")\n            .setFeaturesCol("selectedFeatures")\n            .setNumTrees(100)\n            .setMaxDepth(10)\n            .setMaxBins(32)\n            .setSeed(42);\n        \n        // 3. \u6A21\u578B\u8BC4\u4F30\u5668\n        MulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator()\n            .setLabelCol("label")\n            .setPredictionCol("prediction")\n            .setMetricName("accuracy");\n        \n        // 4. \u4EA4\u53C9\u9A8C\u8BC1\n        CrossValidator crossValidator = new CrossValidator()\n            .setEstimator(new Pipeline().setStages(new PipelineStage[]{\n                featureModel, classifier\n            }))\n            .setEvaluator(evaluator)\n            .setNumFolds(5)\n            .setParallelism(2);\n        \n        // 5. \u8BAD\u7EC3\u6A21\u578B\n        CrossValidatorModel cvModel = crossValidator.fit(trainingData);\n        \n        // 6. \u6A21\u578B\u8BC4\u4F30\n        Dataset<Row> predictions = cvModel.transform(trainingData);\n        double accuracy = evaluator.evaluate(predictions);\n        System.out.println("Cross-validation accuracy: " + accuracy);\n        \n        // 7. \u4FDD\u5B58\u6A21\u578B\n        cvModel.save("models/random_forest_model");\n        \n        // 8. \u7279\u5F81\u91CD\u8981\u6027\u5206\u6790\n        PipelineModel bestModel = (PipelineModel) cvModel.bestModel();\n        RandomForestClassificationModel rfModel = (RandomForestClassificationModel) \n            bestModel.stages()[bestModel.stages().length - 1];\n        \n        System.out.println("Feature importances:");\n        double[] importances = rfModel.featureImportances().toArray();\n        for (int i = 0; i < importances.length; i++) {\n            System.out.println("Feature " + i + ": " + importances[i]);\n        }\n    }\n}\n'})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[MLlib] --\x3e B[\u7279\u5F81\u5DE5\u7A0B]\n    A --\x3e C[\u673A\u5668\u5B66\u4E60\u7B97\u6CD5]\n    A --\x3e D[\u6A21\u578B\u8BC4\u4F30]\n    A --\x3e E[\u7BA1\u9053\u7BA1\u7406]\n    \n    B --\x3e B1["\u7279\u5F81\u63D0\u53D6"]\n    B --\x3e B2["\u7279\u5F81\u8F6C\u6362"]\n    B --\x3e B3["\u7279\u5F81\u9009\u62E9"]\n    \n    C --\x3e C1["\u5206\u7C7B\u7B97\u6CD5"]\n    C --\x3e C2["\u56DE\u5F52\u7B97\u6CD5"]\n    C --\x3e C3["\u805A\u7C7B\u7B97\u6CD5"]\n    \n    D --\x3e D1["\u4EA4\u53C9\u9A8C\u8BC1"]\n    D --\x3e D2["\u6A21\u578B\u9009\u62E9"]\n    \n    E --\x3e E1["Pipeline"]\n    E --\x3e E2["\u5DE5\u4F5C\u6D41\u7BA1\u7406"]'}),"\n",(0,t.jsx)(n.h3,{id:"52-\u673A\u5668\u5B66\u4E60\u793A\u4F8B",children:"5.2 \u673A\u5668\u5B66\u4E60\u793A\u4F8B"}),"\n",(0,t.jsxs)("div",{className:"code-with-callout",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="\u673A\u5668\u5B66\u4E60Pipeline\u793A\u4F8B"',children:'public class MLPipelineExample {\n    public void buildMLPipeline(SparkSession spark, Dataset<Row> data) {\n        // 1. \u7279\u5F81\u5DE5\u7A0B\n        StringIndexer indexer = new StringIndexer()\n            .setInputCol("category")\n            .setOutputCol("categoryIndex");\n        \n        VectorAssembler assembler = new VectorAssembler()\n            .setInputCols(new String[]{"feature1", "feature2", "categoryIndex"})\n            .setOutputCol("features");\n        \n        // 2. \u673A\u5668\u5B66\u4E60\u7B97\u6CD5\n        RandomForestClassifier classifier = new RandomForestClassifier()\n            .setLabelCol("label")\n            .setFeaturesCol("features")\n            .setNumTrees(10);\n        \n        // 3. \u6784\u5EFAPipeline\n        Pipeline pipeline = new Pipeline()\n            .setStages(new PipelineStage[]{indexer, assembler, classifier});\n        \n        // 4. \u8BAD\u7EC3\u6A21\u578B\n        PipelineModel model = pipeline.fit(data);\n        \n        // 5. \u9884\u6D4B\n        Dataset<Row> predictions = model.transform(data);\n        predictions.show();\n        \n        // 6. \u6A21\u578B\u8BC4\u4F30\n        MulticlassClassificationEvaluator evaluator = \n            new MulticlassClassificationEvaluator()\n                .setLabelCol("label")\n                .setPredictionCol("prediction")\n                .setMetricName("accuracy");\n        \n        double accuracy = evaluator.evaluate(predictions);\n        System.out.println("Accuracy: " + accuracy);\n    }\n}\n'})}),(0,t.jsx)(n.admonition,{title:"MLlib\u4F18\u52BF",type:"info",children:(0,t.jsx)(n.p,{children:"MLlib\u63D0\u4F9B\u4E86\u5206\u5E03\u5F0F\u673A\u5668\u5B66\u4E60\u7B97\u6CD5\uFF0C\u652F\u6301\u5927\u89C4\u6A21\u6570\u636E\u5904\u7406\uFF0C\u4E0ESpark\u751F\u6001\u7CFB\u7EDF\u65E0\u7F1D\u96C6\u6210\u3002"})})]}),"\n",(0,t.jsx)(n.h2,{id:"6-spark\u6027\u80FD\u4F18\u5316",children:"6. Spark\u6027\u80FD\u4F18\u5316"}),"\n",(0,t.jsx)(n.h3,{id:"61-\u5185\u5B58\u7BA1\u7406",children:"6.1 \u5185\u5B58\u7BA1\u7406"}),"\n",(0,t.jsx)(n.p,{children:"Spark\u5185\u5B58\u7BA1\u7406\u662F\u6027\u80FD\u4F18\u5316\u7684\u5173\u952E\uFF1A"}),"\n",(0,t.jsx)(n.h4,{id:"611-\u5185\u5B58\u7BA1\u7406\u67B6\u6784",children:"6.1.1 \u5185\u5B58\u7BA1\u7406\u67B6\u6784"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[Spark\u5185\u5B58] --\x3e B[\u6267\u884C\u5185\u5B58]\n    A --\x3e C[\u5B58\u50A8\u5185\u5B58]\n    A --\x3e D[\u7528\u6237\u5185\u5B58]\n    A --\x3e E[\u9884\u7559\u5185\u5B58]\n    \n    B --\x3e B1["\u8BA1\u7B97\u548Cshuffle"]\n    B --\x3e B2["\u52A8\u6001\u5206\u914D"]\n    B --\x3e B3["\u53EF\u88AB\u5B58\u50A8\u5185\u5B58\u5360\u7528"]\n    \n    C --\x3e C1["\u7F13\u5B58\u548C\u5E7F\u64AD"]\n    C --\x3e C2["LRU\u6DD8\u6C70\u7B56\u7565"]\n    C --\x3e C3["\u53EF\u88AB\u6267\u884C\u5185\u5B58\u5360\u7528"]\n    \n    D --\x3e D1["\u7528\u6237\u4EE3\u7801\u548C\u6570\u636E\u7ED3\u6784"]\n    D --\x3e D2["\u56FA\u5B9A\u5927\u5C0F"]\n    \n    E --\x3e E1["\u7CFB\u7EDF\u9884\u7559"]\n    E --\x3e E2["\u9632\u6B62OOM"]'}),"\n",(0,t.jsx)(n.h4,{id:"612-\u5185\u5B58\u8C03\u4F18\u7B56\u7565",children:"6.1.2 \u5185\u5B58\u8C03\u4F18\u7B56\u7565"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="\u5185\u5B58\u8C03\u4F18\u793A\u4F8B"',children:'public class SparkMemoryOptimization {\n    public void optimizeMemory(SparkConf conf) {\n        // 1. \u6267\u884C\u5185\u5B58\u914D\u7F6E\n        conf.set("spark.executor.memory", "8g");\n        conf.set("spark.executor.memoryOverhead", "2g"); // \u5806\u5916\u5185\u5B58\n        conf.set("spark.memory.fraction", "0.8"); // \u6267\u884C\u548C\u5B58\u50A8\u5185\u5B58\u5360\u6BD4\n        conf.set("spark.memory.storageFraction", "0.3"); // \u5B58\u50A8\u5185\u5B58\u5360\u6BD4\n        \n        // 2. \u5E8F\u5217\u5316\u914D\u7F6E\n        conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer");\n        conf.set("spark.kryo.registrationRequired", "false");\n        conf.set("spark.kryo.registrator", "com.example.MyKryoRegistrator");\n        \n        // 3. \u538B\u7F29\u914D\u7F6E\n        conf.set("spark.sql.inMemoryColumnarStorage.compressed", "true");\n        conf.set("spark.sql.inMemoryColumnarStorage.batchSize", "10000");\n        \n        // 4. \u5E7F\u64AD\u53D8\u91CF\u914D\u7F6E\n        conf.set("spark.sql.autoBroadcastJoinThreshold", "10485760"); // 10MB\n        \n        // 5. \u52A8\u6001\u5206\u914D\u914D\u7F6E\n        conf.set("spark.dynamicAllocation.enabled", "true");\n        conf.set("spark.dynamicAllocation.minExecutors", "2");\n        conf.set("spark.dynamicAllocation.maxExecutors", "20");\n        conf.set("spark.dynamicAllocation.initialExecutors", "5");\n        \n        System.out.println("Memory optimization configured");\n    }\n    \n    public void optimizeDataStructures(JavaRDD<String> data) {\n        // 1. \u4F7F\u7528\u5E7F\u64AD\u53D8\u91CF\u51CF\u5C11\u6570\u636E\u4F20\u8F93\n        List<String> stopWords = Arrays.asList("the", "a", "an", "and", "or", "but");\n        Broadcast<List<String>> stopWordsBroadcast = data.context().broadcast(stopWords, \n            ClassTag$.MODULE$.apply(List.class));\n        \n        // 2. \u4F7F\u7528\u7D2F\u52A0\u5668\u8FDB\u884C\u8BA1\u6570\n        Accumulator<Integer> totalWords = data.context().accumulator(0, "TotalWords");\n        Accumulator<Integer> filteredWords = data.context().accumulator(0, "FilteredWords");\n        \n        // 3. \u4F18\u5316RDD\u64CD\u4F5C\n        JavaRDD<String> optimizedData = data\n            .mapPartitions(iterator -> {\n                List<String> batch = new ArrayList<>();\n                while (iterator.hasNext()) {\n                    String line = iterator.next();\n                    if (!stopWordsBroadcast.value().contains(line.toLowerCase())) {\n                        batch.add(line);\n                        filteredWords.add(1);\n                    }\n                    totalWords.add(1);\n                }\n                return batch.iterator();\n            })\n            .cache(); // \u7F13\u5B58\u4E2D\u95F4\u7ED3\u679C\n        \n        // 4. \u4F7F\u7528mapPartitions\u51CF\u5C11\u51FD\u6570\u8C03\u7528\u5F00\u9500\n        JavaRDD<String> processedData = optimizedData.mapPartitions(iterator -> {\n            List<String> results = new ArrayList<>();\n            while (iterator.hasNext()) {\n                String word = iterator.next();\n                results.add(word.toUpperCase());\n            }\n            return results.iterator();\n        });\n        \n        System.out.println("Total words: " + totalWords.value());\n        System.out.println("Filtered words: " + filteredWords.value());\n    }\n}\n\n// \u81EA\u5B9A\u4E49Kryo\u5E8F\u5217\u5316\u6CE8\u518C\u5668\npublic class MyKryoRegistrator implements KryoRegistrator {\n    @Override\n    public void registerClasses(Kryo kryo) {\n        // \u6CE8\u518C\u81EA\u5B9A\u4E49\u7C7B\n        kryo.register(User.class);\n        kryo.register(Product.class);\n        kryo.register(Order.class);\n        \n        // \u6CE8\u518C\u96C6\u5408\u7C7B\n        kryo.register(ArrayList.class);\n        kryo.register(HashMap.class);\n        kryo.register(HashSet.class);\n    }\n}\n'})}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[Spark\u5185\u5B58] --\x3e B[\u6267\u884C\u5185\u5B58]\n    A --\x3e C[\u5B58\u50A8\u5185\u5B58]\n    A --\x3e D[\u7528\u6237\u5185\u5B58]\n    \n    B --\x3e B1["\u8BA1\u7B97\u548Cshuffle"]\n    C --\x3e C1["\u7F13\u5B58\u548C\u5E7F\u64AD"]\n    D --\x3e D1["\u7528\u6237\u4EE3\u7801\u548C\u6570\u636E\u7ED3\u6784"]\n    \n    B1 --\x3e B2["\u52A8\u6001\u5206\u914D"]\n    C1 --\x3e C2["LRU\u6DD8\u6C70"]\n    D1 --\x3e D2["\u56FA\u5B9A\u5927\u5C0F"]'}),"\n",(0,t.jsx)(n.h3,{id:"62-\u4F18\u5316\u7B56\u7565",children:"6.2 \u4F18\u5316\u7B56\u7565"}),"\n",(0,t.jsxs)(i.A,{children:[(0,t.jsx)(l.A,{value:"memory",label:"\u5185\u5B58\u4F18\u5316",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:'// 1. \u5408\u7406\u8BBE\u7F6E\u5185\u5B58\u914D\u7F6E\nSparkConf conf = new SparkConf()\n    .set("spark.executor.memory", "8g")\n    .set("spark.storage.memoryFraction", "0.6")\n    .set("spark.sql.adaptive.enabled", "true");\n'})})}),(0,t.jsx)(l.A,{value:"partition",label:"\u5206\u533A\u4F18\u5316",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:'// 2. \u5408\u7406\u8BBE\u7F6E\u5206\u533A\u6570\nJavaRDD<String> data = sc.textFile("input.txt");\nJavaRDD<String> repartitioned = data.repartition(100);\n\n// 3. \u4F7F\u7528coalesce\u51CF\u5C11\u5206\u533A\nJavaRDD<String> coalesced = data.coalesce(50);\n'})})}),(0,t.jsx)(l.A,{value:"cache",label:"\u7F13\u5B58\u7B56\u7565",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:"// 4. \u5408\u7406\u4F7F\u7528\u7F13\u5B58\nJavaRDD<String> cached = data.cache(); // \u5185\u5B58\u7F13\u5B58\nJavaRDD<String> persisted = data.persist(StorageLevel.MEMORY_AND_DISK());\n\n// 5. \u5E7F\u64AD\u53D8\u91CF\nBroadcast<List<String>> broadcastVar = sc.broadcast(largeList);\n"})})})]}),"\n",(0,t.jsx)(n.h2,{id:"7-spark\u90E8\u7F72\u548C\u914D\u7F6E",children:"7. Spark\u90E8\u7F72\u548C\u914D\u7F6E"}),"\n",(0,t.jsx)(n.h3,{id:"71-\u90E8\u7F72\u6A21\u5F0F",children:"7.1 \u90E8\u7F72\u6A21\u5F0F"}),"\n",(0,t.jsx)(n.p,{children:"Spark\u652F\u6301\u591A\u79CD\u90E8\u7F72\u6A21\u5F0F\uFF1A"}),"\n",(0,t.jsx)(n.h4,{id:"711-kubernetes\u90E8\u7F72\u67B6\u6784",children:"7.1.1 Kubernetes\u90E8\u7F72\u67B6\u6784"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\n    A[Kubernetes\u96C6\u7FA4] --\x3e B[Spark Operator]\n    B --\x3e C[Spark Driver Pod]\n    C --\x3e D[Spark Executor Pods]\n    \n    A --\x3e E[\u6301\u4E45\u5316\u5B58\u50A8]\n    A --\x3e F[\u914D\u7F6E\u7BA1\u7406]\n    \n    B --\x3e B1["\u7BA1\u7406Spark\u5E94\u7528"]\n    B --\x3e B2["\u8D44\u6E90\u8C03\u5EA6"]\n    \n    C --\x3e C1["\u5E94\u7528\u63A7\u5236"]\n    C --\x3e C2["\u4EFB\u52A1\u5206\u53D1"]\n    \n    D --\x3e D1["\u8BA1\u7B97\u6267\u884C"]\n    D --\x3e D2["\u6570\u636E\u7F13\u5B58"]\n    \n    E --\x3e E1["HDFS/PVC"]\n    F --\x3e F1["ConfigMap/Secret"]'}),"\n",(0,t.jsx)(n.h4,{id:"712-\u4E91\u539F\u751F\u90E8\u7F72\u914D\u7F6E",children:"7.1.2 \u4E91\u539F\u751F\u90E8\u7F72\u914D\u7F6E"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="Kubernetes\u90E8\u7F72\u914D\u7F6E\u793A\u4F8B"',children:'public class KubernetesDeployment {\n    public void configureKubernetesDeployment(SparkConf conf) {\n        // 1. Kubernetes\u914D\u7F6E\n        conf.set("spark.master", "k8s://https://kubernetes.default.svc");\n        conf.set("spark.kubernetes.container.image", "spark:3.4.0");\n        conf.set("spark.kubernetes.namespace", "spark-jobs");\n        \n        // 2. \u8D44\u6E90\u914D\u7F6E\n        conf.set("spark.executor.instances", "5");\n        conf.set("spark.executor.memory", "4g");\n        conf.set("spark.executor.cores", "2");\n        conf.set("spark.driver.memory", "2g");\n        conf.set("spark.driver.cores", "1");\n        \n        // 3. \u5B58\u50A8\u914D\u7F6E\n        conf.set("spark.kubernetes.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName", "spark-local-dir-1");\n        conf.set("spark.kubernetes.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path", "/tmp");\n        conf.set("spark.kubernetes.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly", "false");\n        \n        // 4. \u7F51\u7EDC\u914D\u7F6E\n        conf.set("spark.kubernetes.driver.serviceAccountName", "spark");\n        conf.set("spark.kubernetes.executor.serviceAccountName", "spark");\n        \n        // 5. \u5B89\u5168\u914D\u7F6E\n        conf.set("spark.kubernetes.authenticate.driver.serviceAccountName", "spark");\n        conf.set("spark.kubernetes.authenticate.executor.serviceAccountName", "spark");\n        \n        System.out.println("Kubernetes deployment configured");\n    }\n    \n    public void configureResourceQuotas() {\n        // \u914D\u7F6E\u8D44\u6E90\u914D\u989D\n        String resourceQuota = \n            "apiVersion: v1\\n" +\n            "kind: ResourceQuota\\n" +\n            "metadata:\\n" +\n            "  name: spark-quota\\n" +\n            "  namespace: spark-jobs\\n" +\n            "spec:\\n" +\n            "  hard:\\n" +\n            "    requests.cpu: \\"20\\"\\n" +\n            "    requests.memory: 40Gi\\n" +\n            "    limits.cpu: \\"40\\"\\n" +\n            "    limits.memory: 80Gi\\n" +\n            "    persistentvolumeclaims: \\"10\\"";\n        \n        System.out.println("Resource quota configuration:");\n        System.out.println(resourceQuota);\n    }\n}\n\n// \u76D1\u63A7\u548C\u65E5\u5FD7\u914D\u7F6E\npublic class MonitoringConfiguration {\n    public void configureMonitoring(SparkConf conf) {\n        // 1. \u6307\u6807\u6536\u96C6\n        conf.set("spark.metrics.conf", "/opt/spark/conf/metrics.properties");\n        conf.set("spark.sql.streaming.metricsEnabled", "true");\n        \n        // 2. \u65E5\u5FD7\u914D\u7F6E\n        conf.set("spark.eventLog.enabled", "true");\n        conf.set("spark.eventLog.dir", "hdfs://namenode:9000/spark-events");\n        conf.set("spark.history.fs.logDirectory", "hdfs://namenode:9000/spark-events");\n        \n        // 3. \u6027\u80FD\u76D1\u63A7\n        conf.set("spark.sql.adaptive.enabled", "true");\n        conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true");\n        conf.set("spark.sql.adaptive.skewJoin.enabled", "true");\n        \n        // 4. \u52A8\u6001\u8D44\u6E90\u5206\u914D\n        conf.set("spark.dynamicAllocation.enabled", "true");\n        conf.set("spark.dynamicAllocation.minExecutors", "2");\n        conf.set("spark.dynamicAllocation.maxExecutors", "20");\n        conf.set("spark.dynamicAllocation.initialExecutors", "5");\n        \n        System.out.println("Monitoring and logging configured");\n    }\n}\n'})}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"\u90E8\u7F72\u6A21\u5F0F"}),(0,t.jsx)(n.th,{children:"\u7279\u70B9"}),(0,t.jsx)(n.th,{children:"\u9002\u7528\u573A\u666F"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Local\u6A21\u5F0F"})}),(0,t.jsx)(n.td,{children:"\u5355\u673A\u8FD0\u884C\uFF0C\u7528\u4E8E\u5F00\u53D1\u548C\u6D4B\u8BD5"}),(0,t.jsx)(n.td,{children:"\u672C\u5730\u5F00\u53D1\u548C\u8C03\u8BD5"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Standalone\u6A21\u5F0F"})}),(0,t.jsx)(n.td,{children:"Spark\u81EA\u5E26\u7684\u96C6\u7FA4\u7BA1\u7406\u5668"}),(0,t.jsx)(n.td,{children:"\u5C0F\u89C4\u6A21\u96C6\u7FA4"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"YARN\u6A21\u5F0F"})}),(0,t.jsx)(n.td,{children:"\u4F7F\u7528Hadoop YARN\u7BA1\u7406\u8D44\u6E90"}),(0,t.jsx)(n.td,{children:"\u751F\u4EA7\u73AF\u5883\uFF0C\u4E0EHadoop\u96C6\u6210"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Mesos\u6A21\u5F0F"})}),(0,t.jsx)(n.td,{children:"\u4F7F\u7528Apache Mesos\u7BA1\u7406\u8D44\u6E90"}),(0,t.jsx)(n.td,{children:"\u5927\u89C4\u6A21\u96C6\u7FA4\uFF0C\u591A\u6846\u67B6\u652F\u6301"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Kubernetes\u6A21\u5F0F"})}),(0,t.jsx)(n.td,{children:"\u4F7F\u7528K8s\u7BA1\u7406\u5BB9\u5668\u5316\u90E8\u7F72"}),(0,t.jsx)(n.td,{children:"\u4E91\u539F\u751F\u73AF\u5883"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"72-\u914D\u7F6E\u793A\u4F8B",children:"7.2 \u914D\u7F6E\u793A\u4F8B"}),"\n",(0,t.jsxs)("div",{className:"code-with-callout",children:[(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",metastring:'title="Spark\u914D\u7F6E\u793A\u4F8B"',children:'public class SparkConfiguration {\n    public SparkSession createSparkSession() {\n        SparkConf conf = new SparkConf()\n            .setAppName("MySparkApp")\n            .setMaster("yarn")\n            .set("spark.executor.memory", "8g")\n            .set("spark.executor.cores", "4")\n            .set("spark.driver.memory", "4g")\n            .set("spark.sql.adaptive.enabled", "true")\n            .set("spark.sql.adaptive.coalescePartitions.enabled", "true")\n            .set("spark.sql.adaptive.skewJoin.enabled", "true");\n        \n        return SparkSession.builder()\n            .config(conf)\n            .enableHiveSupport()\n            .getOrCreate();\n    }\n}\n'})}),(0,t.jsx)(n.admonition,{title:"\u914D\u7F6E\u5EFA\u8BAE",type:"info",children:(0,t.jsx)(n.p,{children:"\u6839\u636E\u96C6\u7FA4\u8D44\u6E90\u548C\u5E94\u7528\u9700\u6C42\u5408\u7406\u914D\u7F6ESpark\u53C2\u6570\uFF0C\u7279\u522B\u662F\u5185\u5B58\u548CCPU\u914D\u7F6E\uFF0C\u5BF9\u6027\u80FD\u5F71\u54CD\u5F88\u5927\u3002"})})]}),"\n",(0,t.jsx)(n.h2,{id:"8-\u6700\u4F73\u5B9E\u8DF5",children:"8. \u6700\u4F73\u5B9E\u8DF5"}),"\n",(0,t.jsx)(n.h3,{id:"81-\u5F00\u53D1\u6700\u4F73\u5B9E\u8DF5",children:"8.1 \u5F00\u53D1\u6700\u4F73\u5B9E\u8DF5"}),"\n",(0,t.jsx)("div",{className:"card",children:(0,t.jsx)("div",{className:"card__body",children:(0,t.jsxs)("ol",{children:[(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"\u5408\u7406\u4F7F\u7528\u7F13\u5B58"}),"\uFF1A\u5BF9\u91CD\u590D\u4F7F\u7528\u7684RDD\u8FDB\u884C\u7F13\u5B58"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"\u907F\u514Dshuffle"}),"\uFF1A\u51CF\u5C11\u4E0D\u5FC5\u8981\u7684shuffle\u64CD\u4F5C"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"\u4F7F\u7528\u5E7F\u64AD\u53D8\u91CF"}),"\uFF1A\u51CF\u5C11\u6570\u636E\u4F20\u8F93\u5F00\u9500"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"\u5408\u7406\u5206\u533A"}),"\uFF1A\u6839\u636E\u6570\u636E\u91CF\u8BBE\u7F6E\u5408\u9002\u7684\u5206\u533A\u6570"]}),(0,t.jsxs)("li",{children:[(0,t.jsx)("strong",{children:"\u76D1\u63A7\u6027\u80FD"}),"\uFF1A\u4F7F\u7528Spark UI\u76D1\u63A7\u5E94\u7528\u6027\u80FD"]})]})})}),"\n",(0,t.jsx)(n.h3,{id:"82-\u5E38\u89C1\u95EE\u9898\u89E3\u51B3",children:"8.2 \u5E38\u89C1\u95EE\u9898\u89E3\u51B3"}),"\n",(0,t.jsxs)(i.A,{children:[(0,t.jsx)(l.A,{value:"oom",label:"\u5185\u5B58\u6EA2\u51FA",default:!0,children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:'// \u89E3\u51B3\u5185\u5B58\u6EA2\u51FA\u95EE\u9898\n// 1. \u589E\u52A0\u6267\u884C\u5668\u5185\u5B58\n.set("spark.executor.memory", "16g")\n\n// 2. \u4F7F\u7528\u78C1\u76D8\u5B58\u50A8\n.set("spark.storage.memoryFraction", "0.3")\n\n// 3. \u51CF\u5C11\u5206\u533A\u6570\ndata.repartition(100)\n'})})}),(0,t.jsx)(l.A,{value:"slow",label:"\u6027\u80FD\u95EE\u9898",children:(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-java",children:'// \u89E3\u51B3\u6027\u80FD\u95EE\u9898\n// 1. \u542F\u7528\u81EA\u9002\u5E94\u67E5\u8BE2\u6267\u884C\n.set("spark.sql.adaptive.enabled", "true")\n\n// 2. \u4F7F\u7528\u52A8\u6001\u5206\u533A\u88C1\u526A\n.set("spark.sql.optimizer.dynamicPartitionPruning.enabled", "true")\n\n// 3. \u542F\u7528\u4EE3\u7801\u751F\u6210\n.set("spark.sql.codegen.wholeStage", "true")\n'})})})]}),"\n",(0,t.jsx)(n.h2,{id:"9-\u603B\u7ED3",children:"9. \u603B\u7ED3"}),"\n",(0,t.jsx)(n.p,{children:"Apache Spark\u662F\u4E00\u4E2A\u529F\u80FD\u5F3A\u5927\u3001\u6027\u80FD\u4F18\u5F02\u7684\u5927\u6570\u636E\u5904\u7406\u5F15\u64CE\uFF0C\u5B83\u901A\u8FC7\u5185\u5B58\u8BA1\u7B97\u3001\u7EDF\u4E00\u5E73\u53F0\u548C\u4E30\u5BCC\u7684\u751F\u6001\u7CFB\u7EDF\uFF0C\u4E3A\u5927\u6570\u636E\u5904\u7406\u63D0\u4F9B\u4E86\u5B8C\u6574\u7684\u89E3\u51B3\u65B9\u6848\u3002"}),"\n",(0,t.jsx)(n.h3,{id:"\u5B66\u4E60\u5EFA\u8BAE",children:"\u5B66\u4E60\u5EFA\u8BAE"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u7406\u89E3\u6838\u5FC3\u6982\u5FF5"}),"\uFF1A\u6DF1\u5165\u7406\u89E3RDD\u3001DataFrame\u3001Dataset\u7B49\u6838\u5FC3\u62BD\u8C61"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u638C\u63E1\u7F16\u7A0B\u6A21\u578B"}),"\uFF1A\u719F\u7EC3\u4F7F\u7528\u8F6C\u6362\u64CD\u4F5C\u548C\u52A8\u4F5C\u64CD\u4F5C"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u5B66\u4E60\u9AD8\u7EA7\u7279\u6027"}),"\uFF1A\u638C\u63E1Spark SQL\u3001Streaming\u3001MLlib\u7B49\u7EC4\u4EF6"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u6027\u80FD\u4F18\u5316"}),"\uFF1A\u5B66\u4E60\u5185\u5B58\u7BA1\u7406\u3001\u5206\u533A\u7B56\u7565\u7B49\u4F18\u5316\u6280\u672F"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"\u5B9E\u8DF5\u9879\u76EE"}),"\uFF1A\u901A\u8FC7\u5B9E\u9645\u9879\u76EE\u79EF\u7D2F\u7ECF\u9A8C"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Spark\u662F\u5927\u6570\u636E\u6280\u672F\u6808\u4E2D\u7684\u91CD\u8981\u7EC4\u6210\u90E8\u5206\uFF0C\u638C\u63E1\u5B83\u5C06\u5927\u5927\u63D0\u5347\u5927\u6570\u636E\u5904\u7406\u80FD\u529B\u3002"})]})}function h(e={}){let{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(m,{...e})}):m(e)}}}]);